# -*- coding: utf-8 -*-
"""×¤×¨×•×™×§×˜ ×’××¨- ××œ××•×’ ×•×××™.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GGaWmwAiu0n8j2iQsnSmNIbNggac8Nvm

pip installs

---
"""

# # × ×¢×©×” ××–×•×¨×™× ×œ××™×§×•× ×¢×œ ×™×“×™ ×§×• ××•×¨×š ×•×§×• ×¨×•×—×‘
# # ×©×”×•×¤×›×ª ××ª ×”×§×• ××•×¨×š ×•×”×§×• ×’×•×‘×” ×œ××–×•×¨×™× h3 ×œ×›×Ÿ × ×©×ª××©  ×‘×¡×¤×¨×™×™×”
!pip install h3

# ×”×ª×§× ×ª ×¡×¤×¨×™×™×ª KeplerGL ×œ×™×¦×™×¨×ª ××¤×•×ª ××™× ×˜×¨××§×˜×™×‘×™×•×ª
!pip install keplergl

# ×”×ª×§× ×ª ×¡×¤×¨×™×™×ª RapidFuzz ×œ× ×™×§×•×™ ×˜×§×¡×˜×™× ×•×”×©×•×•××•×ª ××—×¨×•×–×•×ª (Fuzzy Matching)
!pip install rapidfuzz

#×¢×‘×•×¨ ××•×“×œ catboost
!pip install catboost

"""imports

---


"""

# × ×™×”×•×œ ×•×¢×™×‘×•×“ × ×ª×•× ×™×
import pandas as pd
import numpy as np
import re

# ×•×™×–×•××œ×™×–×¦×™×” ×•× ×™×ª×•×— ×’×¨×¤×™×
import matplotlib.pyplot as plt
import seaborn as sns
import folium
from folium.plugins import HeatMap
from keplergl import KeplerGl
from sklearn.metrics import ConfusionMatrixDisplay

# ×¡×˜×˜×™×¡×˜×™×§×” ×•×‘×“×™×§×•×ª
import scipy.stats as stats
from scipy.stats import chi2_contingency, f_oneway

# ×¢×™×‘×•×“ ×˜×§×¡×˜ (× ×™×§×•×™ ×©××•×ª/××•×ª×’×™× ×•×›×•')
from rapidfuzz import fuzz, process

# ×¢×™×‘×•×“ ××™×§×•× (Geo encoding)
import h3

"""×˜×¢×™× ×ª ×”× ×ª×•× ×™×

---


"""

!file -i /content/Crash_Reporting_-_Drivers_Data.csv

# ×¦×™×™× ×™ ××ª ×”× ×ª×™×‘ ×œ×§×•×‘×¥ ×©×œ×š
file_path = "/content/Crash_Reporting_-_Drivers_Data.csv"

# × ×¡×™ ×œ×˜×¢×•×Ÿ ××ª ×”×§×•×‘×¥ ×¢× ×”×§×™×“×•×“ 'utf-8'
try:
    df = pd.read_csv(file_path, encoding='utf-8')
    print("×˜×¢×™× ×ª ×”×§×•×‘×¥ ×”×¦×œ×™×—×” ×¢× utf-8")
except UnicodeDecodeError:
    print("×©×’×™××ª ×§×™×“×•×“! ×× ×¡×™× ×§×™×“×•×“ ××—×¨...")

    # × ×¡×™ ×§×™×“×•×“ × ×•×¡×£ ×‘××™×“×” ×•-utf-8 ×œ× ×¢×•×‘×“
    try:
        df = pd.read_csv(file_path, encoding='iso-8859-1')
        print("×˜×¢×™× ×ª ×”×§×•×‘×¥ ×”×¦×œ×™×—×” ×¢× iso-8859-1")
    except Exception as e:
        print(f"×©×’×™××”: {e}")

"""# × ×™×ª×•×— ×¨××©×•× ×™ ×©×œ ×”××©×ª× ×™×
(Data understanding report)

---


"""

#×©×™×›×¤×•×œ ×”×“××˜×” ×”××§×•×¨×™×ª ×¢×‘×•×¨ × ×™×ª×•×— × ×ª×•× ×™× ×¨××©×•× ×™
df_analysis = df.copy()

df_analysis.head()

df_analysis.info()

#×”×¢×ª×§ ×©×œ ×”×“××˜×” ×¢×‘×•×¨ ×˜×‘×œ×” ××—×ª ×‘×”××©×š ×”× ×™×ª×•×—
df_original = df.copy()

# ×¡×¤×™×¨×ª ×”×¢×¨×›×™× ×”×™×™×—×•×“×™×™× ×‘×›×œ ×¢××•×“×”
unique_values = df_analysis.nunique()

# ×™×¦×™×¨×ª ×˜×‘×œ×” ××¡×•×“×¨×ª
unique_summary = pd.DataFrame({
    'Unique Values': unique_values
}).sort_values(by='Unique Values', ascending=False)

print(unique_summary)

# ×—×™×©×•×‘ ××¡×¤×¨ ×”×¢×¨×›×™× ×”×—×¡×¨×™× ×‘×›×œ ×¢××•×“×” ×œ×¤× ×™ ×©×™× ×•×™×™×
missing_values = df_analysis.isnull().sum()

# ×—×™×©×•×‘ ××—×•×– ×”×¢×¨×›×™× ×”×—×¡×¨×™× ×‘×›×œ ×¢××•×“×”
missing_percentage = (missing_values / len(df_analysis)) * 100

# ×—×“×© DataFrame ×©×™×œ×•×‘ ×”×ª×•×¦××•×ªÂ ×‘-
missing_summary = pd.DataFrame({
    'Missing Values': missing_values,
    'Percentage Missing': missing_percentage
})

# ×¡×™× ×•×Ÿ ×¢××•×“×•×ª ×¢× ×¢×¨×›×™× ×—×¡×¨×™× ×‘×œ×‘×“
missing_summary = missing_summary[missing_summary['Missing Values'] > 0]

# ×”×¦×’×ª ×”×ª×•×¦××•×ª
print(missing_summary)

#×™×¦×™×¨×ª ×’×¨×£ ×©×œ ×›××•×ª ×”×¢×¨×›×™× ×”×—×¡×¨×™× ×‘××—×•×–×™×
plt.figure(figsize=(10, 6))
sns.barplot(x=missing_summary.index, y='Percentage Missing', data=missing_summary, palette='coolwarm')
plt.title('Percentage of Missing Values by Column', fontsize=16)
plt.xlabel('Columns', fontsize=14)
plt.ylabel('Percentage Missing', fontsize=14)
plt.xticks(rotation=45, ha='right')
plt.show()

# ××• ×¢×¨×›×™× ×¨×™×§×™×, ×¨×•×•×—×™× - ×œ×¢×¨×›×™× ×—×¡×¨×™× "Unknown"/"NA" ×”××¨×ª
df_analysis.replace(["", " ", "Unknown", "NA"], np.nan, inplace=True)

# ×—×™×©×•×‘ ×”×¢×¨×›×™× ×”×—×¡×¨×™× ××—×“×©
missing_values = df_analysis.isnull().sum()
missing_percentage = (missing_values / len(df_analysis)) * 100

missing_summary = pd.DataFrame({
    'Missing Values': missing_values,
    'Percentage Missing': missing_percentage
})

# ×¡×™× ×•×Ÿ ×¢×¨×›×™× ×—×¡×¨×™×
missing_summary = missing_summary[missing_summary['Missing Values'] > 0]
print(missing_summary)

"""×”×¦×’×ª ×›××•×ª ×”×•×¤×¢×•×ª ×©×œ ×¢×¨×›×™× ×™×™×—×•×“×™× ×‘×¢××•×“×•×ª ×©×•× ×•×ª"""

# ×¤×•× ×§×¦×™×” ×œ×¡×¤×™×¨×ª ×¢×¨×›×™× ×™×™×—×•×“×™×™× ×‘×¢××•×“×”
def count_values(df_analysis, column_name, top_n=None):
    counts = df_analysis[column_name].value_counts()
    if top_n:
        counts = counts.head(top_n)
    return counts

#Injury Severity ××¡×¤×¨ ×”×”×•×¤×¢×ª ×©×œ ×”×¢×¨×›×™×  ×”×™×™×—×•×“×™× ×‘×¢××•×“×”
count_values(df_analysis, "Injury Severity")

#Drivers License State ××¡×¤×¨ ×”×”×•×¤×¢×ª ×©×œ ×”×¢×¨×›×™×  ×”×™×™×—×•×“×™× ×‘×¢××•×“×”
count_values(df_analysis, "Drivers License State")

#Vehicle First Impact Location ××¡×¤×¨ ×”×”×•×¤×¢×ª ×©×œ ×”×¢×¨×›×™× ×”×™×™×—×•×“×™× ×‘×¢××•×“×”
count_values(df_analysis, "Vehicle First Impact Location")

#ACRS Report Type ××¡×¤×¨ ×”×”×•×¤×¢×ª ×©×œ ×”×¢×¨×›×™×  ×”×™×™×—×•×“×™× ×‘×¢××•×“×”
count_values(df_analysis, "ACRS Report Type")

#Route Type ××¡×¤×¨ ×”×”×•×¤×¢×ª ×©×œ ×”×¢×¨×›×™×  ×”×™×™×—×•×“×™× ×‘×¢××•×“×”
count_values(df_analysis, "Route Type")

#Road Name ××¡×¤×¨ ×”×”×•×¤×¢×ª ×©×œ ×”×¢×¨×›×™×  ×”×™×™×—×•×“×™× ×‘×¢××•×“×”
count_values(df_analysis, "Road Name")

#Cross-Street Name ××¡×¤×¨ ×”×”×•×¤×¢×ª ×©×œ ×”×¢×¨×›×™×  ×”×™×™×—×•×“×™× ×‘×¢××•×“×”
count_values(df_analysis, "Cross-Street Name")

#Off-Road Description ××¡×¤×¨ ×”×”×•×¤×¢×ª ×©×œ ×”×¢×¨×›×™×  ×”×™×™×—×•×“×™× ×‘×¢××•×“×”
count_values(df_analysis, "Off-Road Description")

#Municipality ××¡×¤×¨ ×”×”×•×¤×¢×ª ×©×œ ×”×¢×¨×›×™×  ×”×™×™×—×•×“×™× ×‘×¢××•×“×”
count_values(df_analysis, "Municipality")

#Related Non-Motorist ××¡×¤×¨ ×”×”×•×¤×¢×ª ×©×œ ×”×¢×¨×›×™×  ×”×™×™×—×•×“×™× ×‘×¢××•×“×”
count_values(df_analysis, "Related Non-Motorist")

#Collision Type ××¡×¤×¨ ×”×”×•×¤×¢×ª ×©×œ ×”×¢×¨×›×™×  ×”×™×™×—×•×“×™× ×‘×¢××•×“×”
count_values(df_analysis, "Collision Type")

#Weather ××¡×¤×¨ ×”×”×•×¤×¢×ª ×©×œ ×”×¢×¨×›×™×  ×”×™×™×—×•×“×™× ×‘×¢××•×“×”
count_values(df_analysis, "Weather")

#Surface Condition ××¡×¤×¨ ×”×”×•×¤×¢×ª ×©×œ ×”×¢×¨×›×™×  ×”×™×™×—×•×“×™× ×‘×¢××•×“×”
count_values(df_analysis, "Surface Condition")

#Light ××¡×¤×¨ ×”×”×•×¤×¢×ª ×©×œ ×”×¢×¨×›×™×  ×”×™×™×—×•×“×™× ×‘×¢××•×“×”
count_values(df_analysis, "Light")

#Traffic Control ××¡×¤×¨ ×”×”×•×¤×¢×ª ×©×œ ×”×¢×¨×›×™×  ×”×™×™×—×•×“×™× ×‘×¢××•×“×”
count_values(df_analysis, "Traffic Control")

#Driver Substance Abuse ××¡×¤×¨ ×”×”×•×¤×¢×ª ×©×œ ×”×¢×¨×›×™×  ×”×™×™×—×•×“×™× ×‘×¢××•×“×”
count_values(df_analysis, "Driver Substance Abuse")

#Driver At Fault ××¡×¤×¨ ×”×”×•×¤×¢×ª ×©×œ ×”×¢×¨×›×™×  ×”×™×™×—×•×“×™× ×‘×¢××•×“×”
count_values(df_analysis, "Driver At Fault")

#Circumstance ××¡×¤×¨ ×”×”×•×¤×¢×ª ×©×œ ×”×¢×¨×›×™×  ×”×™×™×—×•×“×™× ×‘×¢××•×“×”
count_values(df_analysis, "Circumstance")

#Crash Date/Time ××¡×¤×¨ ×”×”×•×¤×¢×ª ×©×œ ×”×¢×¨×›×™×  ×”×™×™×—×•×“×™× ×‘×¢××•×“×”
count_values(df_analysis, "Crash Date/Time")

df_analysis.info()

"""×”××¨×ª ×›×œ ×”×¢××•×“×•×ª ×”×§×˜×’×•×¨×™××œ×™×•×ª ×œ××•×ª×™×•×ª ×§×˜× ×•×ª"""

# ×”××¨×ª ×›×œ ×”×¢×¨×›×™× ×©×œ ×”×¢××•×“×•×ª ×”×§×˜×’×•×¨×™××œ×™×•×ª ×œ××•×ª×™×•×ª ×§×˜× ×•×ª ×›×š ×©×× ×™×© ×›×¤×™×œ×•×™×•×ª ×™×ª××—×“×•

for col in df_analysis.columns:
  if df_analysis[col].dtype == 'object': # categorical columns
    df_analysis[col] = df_analysis[col].str.lower()

df_analysis['Injury Severity'].value_counts()

"""**× ×™×ª×•×— ×¨××©×•× ×™ ×›×•×œ×œ ×’×¨×¤×™×**

---


"""

# ×¤×•× ×§×¦×™×” ×œ×’×¨×£ ×”××¨××” ××ª ×”×ª×¤×œ×’×•×ª ×”×ª××•× ×•×ª ×‘×¢××•×“×•×ª ×”×©×•× ×•×ª
def plot_countplot(df_analysis, column_name, title=None, show_counts=True, top_n=None):
    # ğŸ“ ×”×“×¤×¡×ª ××¡×¤×¨ ×”×¢×¨×›×™× ×”×™×™×—×•×“×™×™× ×œ×¤× ×™ ×”×’×¨×£
    unique_vals = df_analysis[column_name].nunique()
    print(f"ğŸ”¹ ××¡×¤×¨ ×¢×¨×›×™× ×™×™×—×•×“×™×™× ×‘×¢××•×“×” '{column_name}': {unique_vals}")
    print(" ")
    print(" ")

    # ×¡×“×¨ ×”×¢×¨×›×™× ×ª××™×“ ××”×’×“×•×œ ×œ×§×˜×Ÿ (×‘×¨×™×¨×ª ××—×“×œ ×©×œ value_counts)
    order = df_analysis[column_name].value_counts().index
    if top_n:
        order = order[:top_n]

    # ×¦×™×•×¨ ×”×’×¨×£
    plt.figure(figsize=(15, 9))
    sns.countplot(x=column_name, data=df_analysis, order=order, palette='viridis')

    # ×›×•×ª×¨×•×ª ×•×¦×™×¨×™×
    plt.title(title if title else f"{column_name} Distribution", fontsize=16)
    plt.xlabel(column_name, fontsize=14)
    plt.ylabel("Count", fontsize=14)
    plt.xticks(rotation=45, ha='right')

    # ×”×•×¡×¤×ª ××¡×¤×¨×™× ××¢×œ ×”×¢××•×“×•×ª
    if show_counts:
        for p in plt.gca().patches:
            plt.gca().annotate(f'{p.get_height()}',
                               (p.get_x() + p.get_width() / 2., p.get_height()),
                               ha='center', va='center', xytext=(0, 5),
                               textcoords='offset points', fontsize=10)

    plt.tight_layout()
    plt.show()

# ×’×¨×£ ×©×›×™×—×•×™×•×ª ×œ-Injury Severity
plot_countplot(df_analysis, "Injury Severity")

#Weather
#'raining' & 'rain' -××™×—×•×“ ×§×˜×’×•×¨×™×•×ª ×‘×¢×œ×•×ª ××©××¢×•×ª ×“×•××”
df_analysis['Weather'] = df_analysis['Weather'].replace('raining', 'rain')

#Weather ×’×¨×£ ×©×›×™×—×•×™×•×ª ×œ
plot_countplot(df_analysis, "Weather")

#×—×™×©×•×‘ ××—×•×– ×”×ª××•× ×•×ª ×¢×‘×•×¨ ×›×œ ×ª× ××™ ××–×’ ×”××•×•×™×¨, ×©×‘×”×Ÿ ×”×¢×¨×š ×‘×¢××•×“×ª ×—×•××¨×ª ×”×¤×¦×™×¢×” ×©×•× ×” ×Ö¾'×œ×œ× ×¤×¦×™×¢×”Â × ×¨××™×ªÂ ×œ×¢×™×Ÿ'.

weather_summary = df_analysis.groupby('Weather')['Injury Severity'].apply(lambda x: (x != 'no apparent injury').sum() / len(x) * 100)
row_counts = df_analysis["Weather"].value_counts()

#××™×—×•×“ ×›×œ ×”×ª×•×¦××•×ª ×œ×˜×‘×œ×” ××—×ª (DataFrame)
result_df = pd.DataFrame({
    'Percentage Not "No Apparent Injury"': weather_summary,
    'Row Counts': row_counts
})

result_df

#Surface Condition ×’×¨×£ ×©×›×™×—×•×™×•×ª ×œ
plot_countplot(df_analysis, "Surface Condition")

#×—×™×©×•×‘ ××—×•×– ×”×ª××•× ×•×ª ×¢×‘×•×¨ ×›×œ ,××¦×‘ ×¤× ×™ ×”×©×˜×— ×©×‘×”×Ÿ ×”×¢×¨×š ×‘×¢××•×“×ª ×—×•××¨×ª ×”×¤×¦×™×¢×” ×©×•× ×” ×Ö¾'×œ×œ× ×¤×¦×™×¢×”Â × ×¨××™×ªÂ ×œ×¢×™×Ÿ'.

surface_condition_summary = df_analysis.groupby('Surface Condition')['Injury Severity'].apply(lambda x: (x != 'no apparent injury').sum() / len(x) * 100)
row_counts = df_analysis.groupby('Surface Condition')['Injury Severity'].count()

#××™×—×•×“ ×›×œ ×”×ª×•×¦××•×ª ×œ×˜×‘×œ×” ××—×ª (DataFrame)
result_df = pd.DataFrame({
    'Percentage Not "No Apparent Injury"': surface_condition_summary,
    'Row Counts': row_counts
})

result_df

#Collision Type ×’×¨×£ ×©×›×™×—×•×™×•×ª ×œ
plot_countplot(df_analysis, "Collision Type")

#Collision Type ×’×¨×£ ×©×›×™×—×•×™×•×ª ×œ
plot_countplot(df_analysis, "Collision Type")

#Vehicle Damage Extent ×’×¨×£ ×©×›×™×—×•×™×•×ª ×œ
plot_countplot(df_analysis, "Vehicle Damage Extent")

#×—×™×©×•×‘ ××—×•×– ×”×ª××•× ×•×ª ×¢×‘×•×¨ ×›×œ ×”×§×˜×’×•×¨×™×•×ª ×‘×¢××•×“×ª ×”×™×§×£ ×”× ×–×§ ×œ×¨×›×‘, ×©×‘×”×Ÿ ×”×¢×¨×š ×‘×¢××•×“×ª ×—×•××¨×ª ×”×¤×¦×™×¢×” ×©×•× ×” ×Ö¾'×œ×œ× ×¤×¦×™×¢×”Â × ×¨××™×ªÂ ×œ×¢×™×Ÿ'.
vehicle_damage_summary = df_analysis.groupby('Vehicle Damage Extent')['Injury Severity'].apply(lambda x: (x != 'no apparent injury').sum() / len(x) * 100)
row_counts = df_analysis.groupby('Vehicle Damage Extent')['Injury Severity'].count()

#××™×—×•×“ ×›×œ ×”×ª×•×¦××•×ª ×œ×˜×‘×œ×” ××—×ª (DataFrame)
result_df = pd.DataFrame({
    'Percentage Not "No Apparent Injury"': vehicle_damage_summary,
    'Row Counts': row_counts
})

print(result_df)

# ×™×¦×™×¨×ª ×’×¨×£
plt.figure(figsize=(12, 6))
ax = sns.barplot(x=result_df.index, y='Percentage Not "No Apparent Injury"', data=result_df, palette='viridis')
plt.title('Percentage of Non "No Apparent Injury" by Vehicle Damage Extent', fontsize=16)
plt.xlabel('Vehicle Damage Extent', fontsize=14)
plt.ylabel('Percentage', fontsize=14)
plt.xticks(rotation=45, ha='right')

# ×”×•×¡×¤×ª ××—×•×–×™× ××¢×œ ×›×œ ×¢××•×“×” ×‘×’×¨×£
for p in ax.patches:
    ax.annotate(f'{p.get_height():.2f}%', (p.get_x() + p.get_width() / 2., p.get_height()),
                ha='center', va='center', xytext=(0, 5), textcoords='offset points', fontsize=10)

# ×”×•×¡×¤×ª ××¡×¤×¨ ×”×©×•×¨×•×ª ××ª×—×ª ×œ×›×œ ×¢××•×“×” ×‘×’×¨×£
for i, count in enumerate(result_df['Row Counts']):
    ax.text(i, -35, f'Count: {count}', ha='center', va='top', fontsize=8)


plt.tight_layout()
plt.show()

# Speed Limit -×”×™×¡×˜×•×’×¨××” ×œ
plt.figure(figsize=(8, 5))
sns.histplot(df_analysis["Speed Limit"], bins=20, kde=True)
plt.title("Distribution of Speed Limits", fontsize=14)
plt.xlabel("Speed Limit")
plt.ylabel("Count")
plt.show()

# ×’×¨×£ ×œ× ×™×ª×•×— ×§×©×¨ ×‘×™×Ÿ ××’×‘×‘×œ×•×ª ××”×™×¨×•×ª ×××•×¦×¢×ª ×œ×¤×™ ×¨××ª ×¤×¦×™×¢×”
plt.figure(figsize=(10, 6))
sns.boxplot(data=df_analysis, x='Injury Severity', y='Speed Limit', order=['no apparent injury', 'possible injury', 'suspected minor injury', 'suspected serious injury', 'fatal injury'])
plt.title("Speed Limit by Injury Severity")
plt.xlabel("Injury Severity")
plt.ylabel("Speed Limit")
plt.xticks(rotation=45)
plt.show()

df_analysis.isna().sum()

"""

# × ×™×§×•×™ × ×ª×•× ×™×- ×”×›× ×ª × ×ª×•× ×™× ×œ××™×“×•×œ"""

df.replace(["", " ", "Unknown", "NA", "N/A","n/a","na",'UNKNOWN','unknown','Unknown, Unknown',"-","UNK","UNKN","UMK","ZZKNOWN"], np.nan, inplace=True)
# ×—×™×©×•×‘ ×”×¢×¨×›×™× ×”×—×¡×¨×™× ××—×“×©
missing_values = df.isnull().sum()
missing_percentage = (missing_values / len(df)) * 100

missing_summary = pd.DataFrame({
    'Missing Values': missing_values,
    'Percentage Missing': missing_percentage
})

# ×¡×™× ×•×Ÿ ×¢×¨×›×™× ×—×¡×¨×™×
missing_summary = missing_summary[missing_summary['Missing Values'] > 0]
print(missing_summary)

"""Injury Severity ×˜×™×¤×•×œ ×‘×¢××•×“×ª ×”××˜×¨×”"""

# ×”×¦×’×ª ×”×¢×¨×›×™× ×”×™×™×—×•×“×™×™× ×‘×¢××•×“×ª ×”××˜×¨×”
print(" ××¡×¤×¨ ××•×¤×¢×™× ×œ×¤× ×™ ×˜×™×¤×•×œ:", df['Injury Severity'].nunique())
print(df['Injury Severity'].value_counts())

print("-----------")

###××—×¨×™ ×ª×™×§×•×Ÿ###
# ×”×¤×™×›×ª ×›×œ ×”×¢×¨×›×™× ×‘×¢××•×“×ª injury_severity ×œ××•×ª×™×•×ª ×§×˜× ×•×ª (lowercase)
df["Injury Severity"] = df["Injury Severity"].str.strip().str.lower()
print(" ××¡×¤×¨ ××•×¤×¢×™× ×œ××—×¨ ×˜×™×¤×•×œ:", df['Injury Severity'].nunique())
print(df['Injury Severity'].value_counts())

# ××—×™×§×ª ×¨×©×•××•×ª ×©×™×© ×œ×”×Ÿ ×¢×¨×š ×—×¡×¨ ×‘×¢××•×“×ª ××˜×¨×”
before = len(df)
df = df[df['Injury Severity'].notna()]
after = len(df)
print(f" × ××—×§×• {before - after} ×©×•×¨×•×ª ×¢× ×¢×¨×š ×—×¡×¨ ×‘×¢××•×“×ª ×”××˜×¨×”.")

#×©××¦×™×’×” ×”×™×›×Ÿ ×™×©Â ×™×•×ª×¨Â × ×§×•×“×•×ª Latitude ×•Ö¾Longitude ×œ×¤×™ (Heatmap) ×œ×™×¦×•×¨ ××¤×ª ×—×•×

#×œ× ×œ×›×œ×•×œ ×©×•×¨×•×ª ×©×‘×”×Ÿ ×¢×¨×š ×—×•××¨×ª ×”×¤×¦×™×¢×” ×”×•× '×œ×œ× ×¤×¦×™×¢×” × ×¨××™×ªÂ ×œ×¢×™×Ÿ'.
df_filtered = df[df['Injury Severity'] != 'no apparent injury']

#×™×¦×™×¨×ª ××¤×” ×©××•×¨×›×–×ª ×¡×‘×™×‘ ×¢×¨×›×™ ×”×¨×•×—×‘ ×•×”××•×¨×š ×”×××•×¦×¢×™×
map_center = [df_filtered['Latitude'].mean(), df_filtered['Longitude'].mean()]
m = folium.Map(location=map_center, zoom_start=10)

# ×œ×™×¦×•×¨ ×¨×©×™××” ×©×œ ×¨×©×™××•×ª, ×›××©×¨ ×›×œ ×ª×ª-×¨×©×™××” ××™×™×¦×’×ª × ×§×•×“×” ×¢×œ ×”××¤×”
heat_data = [[row['Latitude'], row['Longitude']] for index, row in df_filtered.iterrows()]

# ×”×•×¡×¤×ª ××¤×ª ×”×—×•×
HeatMap(heat_data).add_to(m)

# ×”×¦×’×ª ×”××¤×”
m

#html ×©××™×¨×ª ×”××¤×” ×‘×§×•×‘×¥
m.save("my_folium_map.html")

"""××—×™×§×ª ×¢××•×“×•×ª ×©×œ× ×ª×•×¨××•×ª ×œ×—×™×–×•×™"""

# ×¨×©×™××ª ×¢××•×“×•×ª ×©××•×’×“×¨×•×ª ×›××–×”×™× ×•×™×•×ª×¨ ×80 ××—×•×– ×¢×¨×›×™× ×—×¡×¨×™× â€“ ×œ× ×ª×•×¨××•×ª ×œ× ×™×ª×•×— ××• ×—×™×–×•×™
columns_to_drop = [
    'Report Number',          # ××¡×¤×¨ ×”×“×•×— ×©×œ ×”×ª××•× ×”
    'Local Case Number',      # ××¡×¤×¨ ×ª×™×§ ××§×•××™
    'Person ID',              # ××–×”×” ×™×™×—×•×“×™ ×œ×›×œ ××“× ×‘×“×•×—
    'Vehicle ID',             # ××–×”×” ×™×™×—×•×“×™ ×œ×¨×›×‘ ×‘×“×•×—
    'Location',                # ××™×§×•× ×›×ª×•×‘, ×™×© ×œ× ×• ×›×‘×¨ Latitude + Longitude
    'Off-Road Description',    # ××¢×œ 80 ××—×•×– ×¢×¨×›×™× ×—×¡×¨×™×
    'Municipality',            # ××¢×œ 80 ××—×•×– ×¢×¨×›×™× ×—×¡×¨×™×
    'Related Non-Motorist',    # ××¢×œ 80 ××—×•×– ×¢×¨×›×™× ×—×¡×¨×™×
    'Non-Motorist Substance Abuse',   # ××¢×œ 80 ××—×•×– ×¢×¨×›×™× ×—×¡×¨×™×
    'Circumstance',             # ××¢×œ 80 ××—×•×– ×¢×¨×›×™× ×—×¡×¨×™×
    'Driverless Vehicle',       #×™×© ×¨×§ no
    'Parked Vehicle',          # NO 184,777    ,3022 YES Â Â ,1534Â ×—×¡×¨×™×
    'Vehicle Model',            #×œ× ×¨×œ×•×•× ×˜×™×ª ×œ×—×™×–×•×™- ×’× ×”×¨×‘×” ×¢×¨×›×™× ×™×—×•×“×™×™× 7000 ×‘×¢×¨×š

]

# ×”×¡×¨×” ×©×œ ×”×¢××•×“×•×ª ×”××œ×• ××”-DataFrame
df = df.drop(columns=columns_to_drop)

# ×”×•×“×¢×” ×œ××¢×§×‘
print("âœ… ×”×¢××•×“×•×ª ×”×‘××•×ª × ××—×§×• ××”×˜×‘×œ×”: ", columns_to_drop)

"""Agency Name"""

#####×œ×¤× ×™ ×ª×™×§×•×Ÿ#####

#×œ×¤× ×™ ×˜×™×¤×•×œ ×‘×” Agency Name ×”×¦×’×ª ×”×§×˜×’×•×¨×™×•×ª ×‘×¢××•×“×ª
print(" ××¡×¤×¨ ×¡×•×›× ×•×™×•×ª ×œ×¤× ×™ ×”××™×—×•×“:", df['Agency Name'].nunique())
print(" ×”×ª×¤×œ×’×•×ª ×”×¡×•×›× ×•×™×•×ª ×”×××•×—×“×•×ª:\n", df['Agency Name'].value_counts())

print("----------------")


######××—×¨×™ ×ª×™×§×•×Ÿ#####
#  ××™×œ×•×Ÿ ×”××¨×” ×œ××™×—×•×“ ×¢×¨×›×™× ×‘×¢××•×“×ª Agency Name
# ××™×—×•×“ ×©××•×ª ×¡×•×›× ×•×™×•×ª ×›×ª×•×‘×™× ×‘×¦×•×¨×•×ª ×©×•× ×•×ª (××•×ª×™×•×ª ×’×“×•×œ×•×ª, ×§×™×¦×•×¨×™× ×•×›×•')
agency_mapping = {
    'Montgomery County Police': 'Montgomery',
    'MONTGOMERY': 'Montgomery',

    'Rockville Police Departme': 'Rockville',
    'ROCKVILLE': 'Rockville',

    'Gaithersburg Police Depar': 'Gaithersburg',
    'GAITHERSBURG': 'Gaithersburg',

    'Takoma Park Police Depart': 'Takoma Park',
    'TAKOMA': 'Takoma Park',

    'Maryland-National Capital': 'MCPARK',  # ×”×¡×•×›× ×•×ª ×”×× ×”×œ×ª ××ª ×”×¤××¨×§×™×
}

# ğŸ§¼ ×”××¨×” ×©×œ ×”×¢×¨×›×™× ×‘×˜×‘×œ×” ×œ×¤×™ ×”××™×œ×•×Ÿ
df['Agency Name'] = df['Agency Name'].replace(agency_mapping)

# ğŸ§¾ ×‘×“×™×§×” ×œ××—×¨ ×”××™×—×•×“ â€“ ×›××” ×¢×¨×›×™× ×™×™×—×•×“×™×™× × ×•×ª×¨×•
print(" ××¡×¤×¨ ×¡×•×›× ×•×™×•×ª ×œ××—×¨ ×”××™×—×•×“:", df['Agency Name'].nunique())
print(" ×”×ª×¤×œ×’×•×ª ×”×¡×•×›× ×•×™×•×ª ×”×××•×—×“×•×ª:\n", df['Agency Name'].value_counts())

"""ACRS Report Type"""

#×¢××•×“×ª ACRS Report Type

#ACRS Report Type ×¢××•×“×ª
unique_values_before = df['ACRS Report Type'].nunique()
print(f"\n ××¡×¤×¨ ×§×˜×’×•×¨×™×•×ª  : {unique_values_before}")

print(df['ACRS Report Type'].value_counts())

"""Crash Date/Time"""

# Crash Date/Time ×¡×™×“×•×¨ ×¢××•×“×ª
# ×”××¨×” ×œ×¤×•×¨××˜ datetime
df['Crash Date/Time'] = pd.to_datetime(df['Crash Date/Time'], errors='coerce', infer_datetime_format=True)

# ×›××” ×¢×¨×›×™× ×œ× ×”×¦×œ×—× ×• ×œ×”××™×¨?
invalid_dates = df['Crash Date/Time'].isna().sum()
print(f"â— ××¡×¤×¨ ×ª××¨×™×›×™× ×©×œ× ×”×¦×œ×—× ×• ×œ×”××™×¨ (NaT): {invalid_dates}")

# ×—×™×œ×•×¥ ×××¤×™×™× ×™× ×—×“×©×™× ××”×ª××¨×™×š
df['Crash Hour'] = df['Crash Date/Time'].dt.hour
df['Crash Day'] = df['Crash Date/Time'].dt.dayofweek  # 0=Monday, 6=Sunday
df['Crash Month'] = df['Crash Date/Time'].dt.month

# ×”×¡×¨×” ×©×œ ×”×¢××•×“×” ×”××§×•×¨×™×ª Crash Date/Time
df.drop(columns=['Crash Date/Time'], inplace=True)

# ×‘×“×™×§×”
print("âœ… ×¡×™×™×× ×• ×˜×™×¤×•×œ ×‘×ª××¨×™×š!")
print(df[['Crash Hour', 'Crash Day', 'Crash Month']].head())

"""route type"""

#×¢××•×“×ª route type

#####×œ×¤× ×™ ×ª×™×§×•×Ÿ#####
#×‘×“×™×§×ª ×¢×¨×›×™× ×™×—×•×“×™×™×
print(" ××¡×¤×¨ ××•×¤×¢×™× ×œ×¤× ×™ ×˜×™×¤×•×œ:", df['Route Type'].nunique())
print( df['Route Type'].value_counts())

#####××—×¨×™ ×ª×™×§×•×Ÿ#####
#×˜×™×¤×•×œ×” ×‘×¢××•×“×ª route type
# ××™×œ×•×Ÿ ×œ××™×—×•×“ ×§×˜×’×•×¨×™×•×ª ×‘×¢××•×“×ª Route Type
route_type_mapping = {
    'Maryland (State)': 'Maryland (State) Route',
    'County': 'County Route',
    'Municipality': 'Municipality Route',
    'Government': 'Government Route',
}
print("-------")
#  ×”××¨×” ×©×œ ×”×¢×¨×›×™× ×‘×˜×‘×œ×” ×œ×¤×™ ×”××™×œ×•×Ÿ
df['Route Type'] = df['Route Type'].replace(route_type_mapping)
print(" ××¡×¤×¨ ××•×¤×¢×™× ×œ××—×¨ ×˜×™×¤×•×œ:", df['Route Type'].nunique())
print(df['Route Type'].value_counts())

"""Collision Type"""

# Collision Type ×¢××•×“×ª

#####×œ×¤× ×™ ×ª×™×§×•×Ÿ#####
# ×”×¦×’×ª ××¡×¤×¨ ×”×§×˜×’×•×¨×™×•×ª ×œ×¤× ×™ ×”×ª×™×§×•×Ÿ
unique_values_before = df['Collision Type'].nunique()
print(f"\n ××¡×¤×¨ ×§×˜×’×•×¨×™×•×ª ×œ×¤× ×™ ×ª×™×§×•×Ÿ: {unique_values_before}")

print(df['Collision Type'].value_counts())

#####××—×¨×™ ×ª×™×§×•×Ÿ#####
# ×”××¨×ª ×›×œ ×”×¢×¨×›×™× ×œ××•×ª×™×•×ª ×’×“×•×œ×•×ª ×•×”×¡×¨×ª ×¨×•×•×—×™× ××™×•×ª×¨×™×
df['Collision Type'] = df['Collision Type'].apply(lambda x: x.upper() if pd.notna(x) else x)

# ×ª×™×§×•×Ÿ ×•×”××—×“×ª ×©××•×ª ×“×•××™×
df['Collision Type'] = df['Collision Type'].replace({
    'FRONT TO REAR': 'SAME DIRECTION REAR-END',
    'SAME DIR REAR END': 'SAME DIRECTION REAR-END',
    'SIDESWIPE, SAME DIRECTION': 'SAME DIRECTION SIDESWIPE',
    'SIDESWIPE, OPPOSITE DIRECTION': 'OPPOSITE DIRECTION SIDESWIPE',
    'FRONT TO FRONT': 'HEAD ON',
    'OTHER':'other'
})

#×¢×¨×›×™× ×—×¡×¨×™×
missing_Collision_Type = df['Collision Type'].isna().sum()
print(f" ××¡×¤×¨ ×¢×¨×›×™× ×—×¡×¨×™× ×‘×¢××•×“×ª CollisionType: {missing_Collision_Type}")

# ×”×¦×’×ª ××¡×¤×¨ ×”×§×˜×’×•×¨×™×•×ª ××—×¨×™ ×”×ª×™×§×•×Ÿ
unique_values_after = df['Collision Type'].nunique()
print(f"\n ××¡×¤×¨ ×§×˜×’×•×¨×™×•×ª ××—×¨×™ ×ª×™×§×•×Ÿ : {unique_values_after}")

print(df['Collision Type'].value_counts())

"""Weather"""

# Weather ×¢××•×“×ª

###×œ×¤× ×™ ×ª×™×§×•×Ÿ
# ×”×¦×’×ª ××¡×¤×¨ ×”×§×˜×’×•×¨×™×•×ª
unique_values_before = df['Weather'].nunique()
print(f"\n ××¡×¤×¨ ×§×˜×’×•×¨×™×•×ª ×œ×¤× ×™ ×ª×™×§×•×Ÿ: {unique_values_before}")

print(df['Weather'].value_counts())

#××—×¨×™ ×ª×™×§×•×Ÿ
#×”×¤×™×›×ª ×›×œ ×”×¢×¨×›×™× ×œ××•×ª×™×•×ª ×§×˜× ×•×ª
df['Weather'] = df['Weather'].str.strip().str.lower()

#××™×—×•×“ ×©××•×ª ×“×•××™×
df['Weather'] = df['Weather'].replace('raining', 'rain')

# ×”×¦×’×ª ××¡×¤×¨ ×”×§×˜×’×•×¨×™×•×ª ××—×¨×™ ×”×ª×™×§×•×Ÿ
unique_values_after = df['Weather'].nunique()
print(f"\n ××¡×¤×¨ ×§×˜×’×•×¨×™×•×ª ××—×¨×™ ×ª×™×§×•×Ÿ × ×•×¡×£: {unique_values_after}")

print(df['Weather'].value_counts())

"""Surface Condition"""

# Surface Condition ×¢××•×“×ª

####K×œ×¤× ×™ ×ª×™×§×•×Ÿ####
# ×”×¦×’×ª ××¡×¤×¨ ×”×§×˜×’×•×¨×™×•×ª ×œ×¤× ×™ ×”×ª×™×§×•×Ÿ
unique_values_before = df['Surface Condition'].nunique()
print(f"\n ××¡×¤×¨ ×§×˜×’×•×¨×™×•×ª ×œ×¤× ×™ ×ª×™×§×•×Ÿ: {unique_values_before}")

print(df['Surface Condition'].value_counts())

print("---------")

####××—×¨×™ ×ª×™×§×•×Ÿ####
#×”×¤×™×›×ª ×›×œ ×”×¢×¨×›×™× ×œ××•×ª×™×•×ª ×§×˜× ×•×ª
df['Surface Condition'] = df['Surface Condition'].str.strip().str.lower()

#××™×—×•×“ ×©××•×ª ×“×•××™×
mapping_surface_condition = {
    'ice/frost': 'ice',
    'water (standing, moving)': 'water(standing/moving)'
}

df['Surface Condition'] = df['Surface Condition'].replace(mapping_surface_condition)

# ×”×¦×’×ª ××¡×¤×¨ ×”×§×˜×’×•×¨×™×•×ª ××—×¨×™ ×”×ª×™×§×•×Ÿ
unique_values_after = df['Surface Condition'].nunique()
print(f"\n ××¡×¤×¨ ×§×˜×’×•×¨×™×•×ª ××—×¨×™ ×ª×™×§×•×Ÿ : {unique_values_after}")

print(df['Surface Condition'].value_counts())

"""Light"""

# Light ×¢××•×“×ª

#####×œ×¤× ×™ ×ª×™×§×•×Ÿ####
#×¢×¨×›×™× ×—×¡×¨×™×
missing_Light= df['Light'].isna().sum()
print(f" ××¡×¤×¨ ×¢×¨×›×™× ×—×¡×¨×™× ×‘×¢××•×“×ª Light: {missing_Light}")

# ×”×¦×’×ª ××¡×¤×¨ ×”×§×˜×’×•×¨×™×•×ª ×œ×¤× ×™ ×”×ª×™×§×•×Ÿ
print(df['Light'].nunique())
print(df['Light'].value_counts())

#####××—×¨×™ ×ª×™×§×•×Ÿ####
#×”×¤×™×›×ª ×›×œ ×”×¢×¨×›×™× ×œ××•×ª×™×•×ª ×§×˜× ×•×ª
df['Light'] = df['Light'].str.strip().str.lower()

#××™×—×•×“ ×©××•×ª ×“×•××™×
df['Light'] = df['Light'].replace({
    'dark lights on': 'dark - lighted',
    'dark no lights': 'dark - not lighted',
    'dark -- unknown lighting': 'dark - unknown lighting'
})
# ×”×¦×’×ª ××¡×¤×¨ ×”×§×˜×’×•×¨×™×•×ª ××—×¨×™ ×”×ª×™×§×•×Ÿ
unique_values_after = df['Light'].nunique()
print(f"\n ××¡×¤×¨ ×§×˜×’×•×¨×™×•×ª ××—×¨×™ ×ª×™×§×•×Ÿ: {unique_values_after}")

print(df['Light'].value_counts())

"""Traffic Control"""

# Traffic Control ×¢××•×“×ª

####×œ×¤× ×™ ×ª×™×§×•×Ÿ####
#×¢×¨×›×™× ×—×¡×¨×™×
missing_Traffic_Control= df['Traffic Control'].isna().sum()
print(f" ××¡×¤×¨ ×¢×¨×›×™× ×—×¡×¨×™× ×‘×¢××•×“×ª Traffic Control: {missing_Traffic_Control}")

# ×”×¦×’×ª ××¡×¤×¨ ×”×§×˜×’×•×¨×™×•×ª ×œ×¤× ×™ ×”×ª×™×§×•×Ÿ
print(df['Traffic Control'].nunique())
print(df['Traffic Control'].value_counts())

####××—×¨×™ ×ª×™×§×•×Ÿ####
#×”×¤×™×›×ª ×›×œ ×”×¢×¨×›×™× ×œ××•×ª×™×•×ª ×§×˜× ×•×ª
df['Traffic Control'] = df['Traffic Control'].str.strip().str.lower()

# #××™×—×•×“ ×©××•×ª ×“×•××™×
df['Traffic Control'] = df['Traffic Control'].replace({
    # ×¨××–×•×¨×™×
    'traffic control signal': 'traffic signal',
    'flashing traffic control signal': 'flashing traffic signal',
    # ×©×œ×˜×™× ××—×¨×™× (××–×”×¨×”, ×”×•×œ×›×™ ×¨×’×œ, ×¢×§×•××•×ª, ×‘×™×ª ×¡×¤×¨)
    'other warning sign': 'warning sign',
    'school zone sign device': 'school zone sign',
    'school zone': 'school zone sign',
    # ×”×•×œ×›×™ ×¨×’×œ/×× ×©×™×
    'pedestrian crossing': 'pedestrian crossing sign',
    'person': 'person (including flagger, law enforcement, crossing guard, etc.',

    'other signal' : 'other'
})

# ×”×¦×’×ª ××¡×¤×¨ ×”×§×˜×’×•×¨×™×•×ª ××—×¨×™ ×”×ª×™×§×•×Ÿ
unique_values_after = df['Traffic Control'].nunique()
print(f"\n ××¡×¤×¨ ×§×˜×’×•×¨×™×•×ª ××—×¨×™ ×ª×™×§×•×Ÿ : {unique_values_after}")

print(df['Traffic Control'].value_counts())

"""Driver Substance Abuse"""

# Driver Substance Abuse ×¢××•×“×ª

####×œ×¤× ×™ ×ª×™×§×•×Ÿ####
#×¢×¨×›×™× ×—×¡×¨×™×
missing_Driver_Substance_Abuse= df['Driver Substance Abuse'].isna().sum()
print(f" ××¡×¤×¨ ×¢×¨×›×™× ×—×¡×¨×™× ×‘×¢××•×“×ª Driver Substance Abuse: {missing_Driver_Substance_Abuse}")

# ×”×¦×’×ª ××¡×¤×¨ ×”×§×˜×’×•×¨×™×•×ª ×œ×¤× ×™ ×”×ª×™×§×•×Ÿ
print(df['Driver Substance Abuse'].nunique())
print(df['Driver Substance Abuse'].value_counts())

####××—×¨×™ × ×™×§×•×™####
# ×”××¨×” ×œ××•×ª×™×•×ª ×§×˜× ×•×ª ×•×”×¡×¨×ª ×¨×•×•×—×™×
df['Driver Substance Abuse'] = df['Driver Substance Abuse'].str.strip().str.lower()

# ××™×¤×•×™ ×¢×¨×›×™× ×¢× × ×™×¡×•×— ×œ× ×‘×¨×•×¨ ×œ× ×™×¡×•×—×™× ××“×•×™×§×™× ×™×•×ª×¨
mapping_substance_abuse = {
    'unknown, not suspect of drug use': 'alcohol unknown, drug not suspected',
    'unknown, suspect of drug use': 'alcohol unknown, drug suspected',
    'suspect of alcohol use, unknown': 'alcohol suspected, drug unknown',
    'not suspect of alcohol use, unknown': 'alcohol not suspected, drug unknown'
}

# ×”×—×œ×ª ×”××™×¤×•×™
df['Driver Substance Abuse'] = df['Driver Substance Abuse'].replace(mapping_substance_abuse)

#×”×¦×’×ª ×¡×™×›×•×
print(f" ××¡×¤×¨ ×¢×¨×›×™× ×—×¡×¨×™×: {df['Driver Substance Abuse'].isna().sum()}")
df['Driver Substance Abuse'].value_counts()

"""Driver At Fault"""

# Driver At Fault ×¢××•×“×ª

#×¢×¨×›×™× ×—×¡×¨×™×
missing_Driver_At_Fault= df['Driver At Fault'].isna().sum()
print(f"ğŸ“Œ ××¡×¤×¨ ×¢×¨×›×™× ×—×¡×¨×™× ×‘×¢××•×“×ª Driver At Fault: {missing_Driver_At_Fault}")

# ×”×¦×’×ª ××¡×¤×¨ ×”×§×˜×’×•×¨×™×•×ª ×œ×¤× ×™ ×”×ª×™×§×•×Ÿ
print(df['Driver At Fault'].nunique())
df['Driver At Fault'].value_counts()

"""Driver Distracted By"""

# Driver Distracted By ×¢××•×“×ª

#×¢×¨×›×™× ×—×¡×¨×™×
missing_Driver_Distracted_By= df['Driver Distracted By'].isna().sum()
print(f" ××¡×¤×¨ ×¢×¨×›×™× ×—×¡×¨×™× ×‘×¢××•×“×ª Driver Distracted By: {missing_Driver_Distracted_By}")

##×œ×¤× ×™ ×ª×™×§×•×Ÿ
# ×”×¦×’×ª ××¡×¤×¨ ×”×§×˜×’×•×¨×™×•×ª ×œ×¤× ×™ ×”×ª×™×§×•×Ÿ
print(df['Driver Distracted By'].nunique())
print(df['Driver Distracted By'].value_counts())

##××—×¨×™ ×ª×™×§×•×Ÿ
#×”×¤×™×›×ª ×›×œ ×”×¢×¨×›×™× ×œ××•×ª×™×•×ª ×§×˜× ×•×ª
df['Driver Distracted By'] = df['Driver Distracted By'].str.strip().str.lower()

# ×”×¦×’×ª ××¡×¤×¨ ×”×§×˜×’×•×¨×™×•×ª ××—×¨×™ ×”×ª×™×§×•×Ÿ
unique_values_after = df['Driver Distracted By'].nunique()
print(f"\n ××¡×¤×¨ ×§×˜×’×•×¨×™×•×ª ××—×¨×™ ×ª×™×§×•×Ÿ : {unique_values_after}")

print(df['Driver Distracted By'].value_counts())

"""Drivers License State"""

#Drivers License State ×¢××•×“×ª

####×œ×¤× ×™ ×ª×™×§×•×Ÿ####

#×¢×¨×›×™× ×—×¡×¨×™×
missing_Drivers_License_State= df['Drivers License State'].isna().sum()
print(f" ××¡×¤×¨ ×¢×¨×›×™× ×—×¡×¨×™× ×‘×¢××•×“×ª Drivers License State: {missing_Drivers_License_State}")

# ×”×¦×’×ª ××¡×¤×¨ ×”×§×˜×’×•×¨×™×•×ª ×œ×¤× ×™ ×”×ª×™×§×•×Ÿ
print(df['Drivers License State'].nunique())
print(df['Drivers License State'].value_counts())

print("\n :××—×¨×™ ×˜×™×¤×•×œ ×‘×¢××•×“×ª ×©× ××“×™× ×”")
####××—×¨×™ ×ª×™×§×•×Ÿ####
# ××™×œ×•×Ÿ ×§×•×“ â ×©× ××“×™× ×” ××œ×
state_mapping = {
    'AB': 'Alberta',
    'AK': 'Alaska',
    'AL': 'Alabama',
    'AR': 'Arkansas',
    'AS': 'American Samoa',
    'AZ': 'Arizona',
    'BC': 'British Columbia',
    'CA': 'California',
    'CO': 'Colorado',
    'CT': 'Connecticut',
    'DC': 'District of Columbia',
    'DE': 'Delaware',
    'FL': 'Florida',
    'FM': 'Federated States of Micronesia',
    'GA': 'Georgia',
    'GU': 'Guam',
    'HI': 'Hawaii',
    'IA': 'Iowa',
    'ID': 'Idaho',
    'IL': 'Illinois',
    'IN': 'Indiana',
    'IT': 'Italy',
    'KS': 'Kansas',
    'KY': 'Kentucky',
    'LA': 'Louisiana',
    'MA': 'Massachusetts',
    'MB': 'Manitoba',
    'MD': 'Maryland',
    'ME': 'Maine',
    'MH': 'Marshall Islands',
    'MI': 'Michigan',
    'MN': 'Minnesota',
    'MO': 'Missouri',
    'MP': 'Northern Mariana Islands',
    'MS': 'Mississippi',
    'MT': 'Montana',  #
    'MX-BCN': 'Baja California',
    'MX-MEX': 'Estado de MÃ©xico',
    'MX-ROO': 'Quintana Roo',
    'NB': 'New Brunswick',
    'NC': 'North Carolina',
    'ND': 'North Dakota',
    'NE': 'Nebraska',
    'NF': 'Norfolk',  #
    'NH': 'New Hampshire',
    'NJ': 'New Jersey',
    'NL': 'Netherlands',  #
    'NM': 'New Mexico',
    'NS': 'Nova Scotia',   #
    'NT': 'Northwest Territories',
    'NV': 'Nevada',
    'NY': 'New York',
    'OH': 'Ohio',
    'OK': 'Oklahoma',
    'ON': 'Ontario',  #
    'OR': 'Oregon',
    'PA': 'Pennsylvania',
    'PR': 'Puerto Rico',
    'QC': 'Quebec',
    'RI': 'Rhode Island',
    'SC': 'South Carolina',
    'SD': 'South Dakota',
    'SK': 'Saskatchewan',
    'TN': 'Tennessee',
    'TX': 'Texas',
    'UM': 'U.S. Minor Outlying Islands',
    'US': 'United States',
    'UT': 'Utah',
    'VA': 'Virginia',
    'VI': 'U.S. Virgin Islands',
    'VT': 'Vermont',
    'WA': 'Washington',
    'WI': 'Wisconsin',
    'WV': 'West Virginia',
    'WY': 'Wyoming',
    'YT': 'Yukon'
}

print(df['Drivers License State'].nunique())

# ×¢×“×›×•×Ÿ ×™×©×™×¨ ×©×œ ×”×¢××•×“×” ×”××§×•×¨×™×ª
def map_state(code):
    if code == 'XX':
        return np.nan
    elif code in state_mapping:
        return state_mapping[code]
    else:
        return code

df['Drivers License State'] = df['Drivers License State'].apply(map_state)
missing_Drivers_License_State= df['Drivers License State'].isna().sum()
print(missing_Drivers_License_State)

# drivers license state ×§×•×“ ×œ××™×—×•×“ ×§×˜×’×•×¨×™×•×ª × ×“×™×¨×•×ª (×¢×“ 50 ××•×¤×¢×™× ×™×™×—×•×“×™×)  ×‘×¢××•×“×ª
# ×¡×•×¤×¨×™× ××ª ×›××•×ª ×”×”×•×¤×¢×•×ª ×©×œ ×›×œ ××“×™× ×”
state_counts = df['Drivers License State'].value_counts()

# ×‘×•×—×¨×™× ××“×™× ×•×ª ×¢× ×œ×¤×—×•×ª 50 ××§×¨×™×
threshold = 50
common_states = state_counts[state_counts >= threshold].index

# ××—×œ×™×¤×™× ×¢×¨×›×™× × ×“×™×¨×™× ×‘Ö¾"Other" ×™×©×™×¨×•×ª ×‘×¢××•×“×” ×”××§×•×¨×™×ª
df['Drivers License State'] = df['Drivers License State'].apply(
    lambda x: x if x in common_states else 'other'
)

"""Vehicle Damage Extent"""

#Vehicle Damage Extent ×¢××•×“×ª

#×¢×¨×›×™× ×—×¡×¨×™×
missing_Vehicle_Damage_Extent= df['Vehicle Damage Extent'].isna().sum()
print(f" ××¡×¤×¨ ×¢×¨×›×™× ×—×¡×¨×™× ×‘×¢××•×“×ª Vehicle Damage Extent: {missing_Vehicle_Damage_Extent}")

# ×”×¦×’×ª ××¡×¤×¨ ×”×§×˜×’×•×¨×™×•×ª ×œ×¤× ×™ ×”×ª×™×§×•×Ÿ
print(f"\n ××¡×¤×¨ ×§×˜×’×•×¨×™×•×ª ×œ×¤× ×™ ×ª×™×§×•×Ÿ:")
print(df['Vehicle Damage Extent'].nunique())
print(df['Vehicle Damage Extent'].value_counts())

#××—×¨×™ ×ª×™×§×•×Ÿ
#×”×¤×™×›×ª ×›×œ ×”×¢×¨×›×™× ×œ××•×ª×™×•×ª ×§×˜× ×•×ª
df['Vehicle Damage Extent'] = df['Vehicle Damage Extent'].str.strip().str.lower()

# ×”×¦×’×ª ××¡×¤×¨ ×”×§×˜×’×•×¨×™×•×ª ××—×¨×™ ×”×ª×™×§×•×Ÿ
unique_values_after = df['Vehicle Damage Extent'].nunique()
print(f"\n ××¡×¤×¨ ×§×˜×’×•×¨×™×•×ª ××—×¨×™ ×ª×™×§×•×Ÿ × ×•×¡×£: {unique_values_after}")

print(df['Vehicle Damage Extent'].value_counts())

"""Vehicle First Impact Location"""

#Vehicle First Impact Location ×¢××•×“×ª

####×œ×¤× ×™ ×ª×™×§×•×Ÿ####
#×¢×¨×›×™× ×—×¡×¨×™×
missing_Vehicle_First_Impact_Location= df['Vehicle First Impact Location'].isna().sum()
print(f" ××¡×¤×¨ ×¢×¨×›×™× ×—×¡×¨×™× ×‘×¢××•×“×ª Vehicle First Impact Location: {missing_Vehicle_First_Impact_Location}")

# ×”×¦×’×ª ××¡×¤×¨ ×”×§×˜×’×•×¨×™×•×ª ×œ×¤× ×™ ×”×ª×™×§×•×Ÿ
print(df['Vehicle First Impact Location'].nunique())
print(df['Vehicle First Impact Location'].value_counts())

####××—×¨×™ ×ª×™×§×•×Ÿ####
#××™×—×•×“ ×¢×¨×›×™× ×“×•××™×
mapping = {
    'Twelve O Clock': 'TWELVE OCLOCK',
    'Six O Clock': 'SIX OCLOCK',
    'One O Clock': 'ONE OCLOCK',
    'Two O Clock': 'TWO OCLOCK',
    'Three O Clock': 'THREE OCLOCK',
    'Four O Clock': 'FOUR OCLOCK',
    'Five O Clock': 'FIVE OCLOCK',
    'Six O Clock': 'SIX OCLOCK',
    'Seven O Clock': 'SEVEN OCLOCK',
    'Eight O Clock': 'EIGHT OCLOCK',
    'Nine O Clock': 'NINE OCLOCK',
    'Ten O Clock': 'TEN OCLOCK',
    'Eleven O Clock': 'ELEVEN OCLOCK',
    'Non-Collision': 'NON-COLLISION',
    'UNDERSIDE': 'Underside'
}
df['Vehicle First Impact Location'] = df['Vehicle First Impact Location'].replace(mapping)

# ×”×¦×’×ª ××¡×¤×¨ ×”×§×˜×’×•×¨×™×•×ª ××—×¨×™ ×”×ª×™×§×•×Ÿ
unique_values_after = df['Vehicle First Impact Location'].nunique()
print(f"\n ××¡×¤×¨ ×§×˜×’×•×¨×™×•×ª ××—×¨×™ ×ª×™×§×•×Ÿ : {unique_values_after}")

print(df['Vehicle First Impact Location'].value_counts())

"""Vehicle Body Type"""

#Vehicle Body Type ×¢××•×“×ª

####×œ×¤× ×™ ×ª×™×§×•×Ÿ####
#×¢×¨×›×™× ×—×¡×¨×™×
missing_Vehicle_Body_Type= df['Vehicle Body Type'].isna().sum()
print(f"ğŸ“Œ ××¡×¤×¨ ×¢×¨×›×™× ×—×¡×¨×™× ×‘×¢××•×“×ª Vehicle Body Type: {missing_Vehicle_Body_Type}")

# ×”×¦×’×ª ××¡×¤×¨ ×”×§×˜×’×•×¨×™×•×ª ×œ×¤× ×™ ×”×ª×™×§×•×Ÿ
print(df['Vehicle Body Type'].nunique())
print(df['Vehicle Body Type'].value_counts())

####××—×¨×™ ×ª×™×§×•×Ÿ####
#×”×¤×™×›×ª ×”×¢×¨×›×™× ×œ××•×ª×™×•×ª ×§×˜× ×•×ª
df['Vehicle Body Type'] = df['Vehicle Body Type'].str.strip().str.lower()

#××™×—×•×“ ×¢×¨×›×™× ×“×•××™×
vehicle_type_mapping = {
    '(sport) utility vehicle': 'sport utility vehicle',
    'bus - transit': 'transit bus',
    'bus - school': 'school bus',
    'bus - other type': 'other bus',
    'bus - cross country': 'cross country bus',
    #'all-terrain vehicle/all-terrain cycle (atv/atc)': 'all terrain vehicle (atv)',
    #'recreational off-highway vehicles (rov)': 'recreational vehicle',
}

df['Vehicle Body Type'] = df['Vehicle Body Type'].replace(vehicle_type_mapping)

# ×”×¦×’×ª ××¡×¤×¨ ×”×§×˜×’×•×¨×™×•×ª ××—×¨×™ ×”×ª×™×§×•×Ÿ
unique_values_after = df['Vehicle Body Type'].nunique()
print(f"\n ××¡×¤×¨ ×§×˜×’×•×¨×™×•×ª ××—×¨×™ ×ª×™×§×•×Ÿ : {unique_values_after}")

print(df['Vehicle Body Type'].value_counts())

# ×§×•×“ ×œ××™×—×•×“ ×§×˜×’×•×¨×™×•×ª × ×“×™×¨×•×ª ×‘×¢××•×“×ª Vehicle Body Type

# ×¡×•×¤×¨×™× ××ª ×›××•×ª ×”×”×•×¤×¢×•×ª ×©×œ ×›×œ ×¡×•×’ ×¨×›×‘
body_type_counts = df['Vehicle Body Type'].value_counts()

# ×‘×•×—×¨×™× ×¡×•×’×™ ×¨×›×‘ ×©××•×¤×™×¢×™× ×œ×¤×—×•×ª 20 ×¤×¢××™×
threshold = 20
common_body_types = body_type_counts[body_type_counts >= threshold].index

# ××—×œ×™×¤×™× ×§×˜×’×•×¨×™×•×ª × ×“×™×¨×•×ª ×‘Ö¾"other" ×™×©×™×¨×•×ª ×‘×¢××•×“×” ×”××§×•×¨×™×ª
df['Vehicle Body Type'] = df['Vehicle Body Type'].apply(
    lambda x: x if x in common_body_types else 'other'
)

"""Vehicle Movement"""

#Vehicle Movement ×¢××•×“×ª

####×œ×¤× ×™ ×ª×™×§×•×Ÿ####
#×¢×¨×›×™× ×—×¡×¨×™×
missing_Vehicle_Movement= df['Vehicle Movement'].isna().sum()
print(f" ××¡×¤×¨ ×¢×¨×›×™× ×—×¡×¨×™× ×‘×¢××•×“×ª Vehicle Movement: {missing_Vehicle_Movement}")

# ×”×¦×’×ª ××¡×¤×¨ ×”×§×˜×’×•×¨×™×•×ª ×œ×¤× ×™ ×”×ª×™×§×•×Ÿ
print(df['Vehicle Movement'].nunique())
print(df['Vehicle Movement'].value_counts())

####××—×¨×™ ×ª×™×§×•×Ÿ####
#×”×¤×™×›×ª ×”×¢×¨×›×™× ×œ××•×ª×™×•×ª ×§×˜× ×•×ª
df['Vehicle Movement'] = df['Vehicle Movement'].str.strip().str.lower()

#××™×—×•×“ ×¢×¨×›×™× ×“×•××™×
mapping = {
    'making u turn': 'making u-turn',
    'parked': 'parking',
    'turning right': 'making right turn',
    'turning left': 'making left turn',
    'overtaking/passing': 'passing'
}
df['Vehicle Movement'] = df['Vehicle Movement'].replace(mapping)

# ×”×¦×’×ª ××¡×¤×¨ ×”×§×˜×’×•×¨×™×•×ª ××—×¨×™ ×”×ª×™×§×•×Ÿ
unique_values_after = df['Vehicle Movement'].nunique()
print(f"\n ××¡×¤×¨ ×§×˜×’×•×¨×™×•×ª ××—×¨×™ ×ª×™×§×•×Ÿ : {unique_values_after}")

print(df['Vehicle Movement'].value_counts())

"""vehicle_year"""

#'vehicle_year'

# ×”××¨×” ×‘×˜×•×—×” ×œ××¡×¤×¨×™× (×œ××§×¨×” ×©×™×© ×ª×•×•×™× ×—×¨×™×’×™×)
df['Vehicle Year'] = pd.to_numeric(df['Vehicle Year'], errors='coerce')

# NaN- ×”×¤×™×›×ª ×¢×¨×›×™× ×œ× ×ª×§×™× ×™× (××—×•×¥ ×œ×˜×•×•×— 1980â€“2025) ×œ
df.loc[(df['Vehicle Year'] < 1980) | (df['Vehicle Year'] > 2025), 'Vehicle Year'] = pd.NA
#×œ×¤× ×™ ×©×¢×©×™× ×• ××ª ×”×˜×•×•×— ×œ× ×”×™×• ×¢×¨×›×™× ×—×¡×¨×™× ×‘×¢××•×“×” ×–××ª

print(f"\n ××¡×¤×¨ ×¢×¨×›×™× ×—×¡×¨×™×: {df['Vehicle Year'].isna().sum()}")

"""Vehicle Make"""

#×˜×™×¤×•×œ ×‘×¢××•×“×ª vehicle make

####×œ×¤× ×™ ×˜×™×¤×•×œ####
#×‘fuzzy ×‘×“×™×§×” ×©×œ ×”×¢×¨×›×™× ×”× ×¤×•×¦×™× ××ª×•×š ×›×œ ×”×¢×¨×›×™× ×”×™×™×—×•×“×™× ×‘×¢××•×“×ª ×”×™×™×¦×¨×Ÿ ×›×“×™ ×œ×˜×¤×œ ×™×“× ×™×ª ×‘×¢×¨×›×™× ×œ×¤× ×™ ×©× ×©×ª××©
# ×”××¨×” ×œ××•×ª×™×•×ª ×’×“×•×œ×•×ª ×•×”×¡×¨×ª ×¨×•×•×—×™×
df["Vehicle Make"] = df["Vehicle Make"].str.upper().str.strip()

# ×¡×¤×™×¨×ª ×¢×¨×›×™× ×™×™×—×•×“×™×™× ×•×”×¦×’×ª 40 ×”×¨××©×•× ×™×
top_40 = df["Vehicle Make"].value_counts(dropna=False).head(40)

# ×”×“×¤×¡×”
print("ğŸ“Š 40 ×”×¢×¨×›×™× ×”× ×¤×•×¦×™× ×‘×™×•×ª×¨ ×‘×¢××•×“×” Vehicle Make:")
print(top_40)
print("\n--------")

####××—×¨×™ ×˜×™×¤×•×œ####

# ×©×œ×‘ 1ï¸âƒ£: × ×™×§×•×™ ×¨××©×•× ×™ ×©×œ ×¢××•×“×ª Vehicle Make
# ×”××¨×” ×œ××•×ª×™×•×ª ×’×“×•×œ×•×ª ×•×”×¡×¨×ª ×¨×•×•×—×™×
df["Vehicle Make"] = df["Vehicle Make"].str.upper().str.strip()

# âœ¨ ×˜×™×¤×•×œ ×™×“× ×™ ×œ×¤× ×™ fuzzy â€“ ×¢×¨×›×™× ×©×›×™×—×™× ×¢× ×§×™×¦×•×¨×™×
df["Vehicle Make"] = df["Vehicle Make"].replace({
    "TOYT": "TOYOTA",
    "TOTA" : "TOYOTA",
    "TOY": "TOYOTA",
    "TOYO": "TOYOTA",
    "TOYOA": "TOYOTA",
    "TOYOT": "TOYOTA",
    "TOYOYA" : "TOYOTA",
    "TOYOTO" : "TOYOTA",
    "TOYOTS" :"TOYOTA",
    "TOYOVAL" : "TOYOTA",
    "TOYOYTA" : "TOYOTA",
    "TOYTA" : "TOYOTA",
    "TOYTOA" : "TOYOTA",
    "TOYTOTA" :"TOYOTA",
    "HOND": "HONDA",
    "CHEV": "CHEVROLET",
    "CHEVY": "CHEVROLET",
    "NISS": "NISSAN",
    "HYUN": "HYUNDAI",
    "MERZ": "MERCEDES-BENZ",
    "MERC": "MERCEDES-BENZ",
    "MERCEDES": "MERCEDES-BENZ",
    "VOLK": "VOLKSWAGEN",
    "VOLKS" : "VOLKSWAGEN",
    "VOLKS WAGON" : "VOLKSWAGEN",
    "VOLKSWAGON": "VOLKSWAGEN",
    "VOLV" : "VOLVO",
    "VW" : "VOLKSWAGEN",
    "ACUR": "ACURA",
    "SUBA": "SUBARU",
    "DODG": "DODGE",
    "MAZD": "MAZDA",
    "GILL": "GILLIG",
    "THOMAS": "THOMAS BUILT BUSES",
    "LINC" : "LINCOLN",
    "MITS" : "MITSUBISHI",
    "MITZ" : "MITSUBISHI",
    "NFLY" : "NEW FLYER",
    "PIERCE": "PIERCE MANUFACTURING",
    "PONT" : "PONTIAC",
    "PORS" : "PORSCHE",
    "SUBA" : "SUBARU",
    "SUB" : "SUBARU",
    "SUBU" : "SUBARU",
    "SUBURU" : "SUBARU",
    "SUZI" : "SUZUKI",
    "SUZ" : "SUZUKI",
    "SUSUKI" : "SUZUKI",
    "SUZU" : "SUZUKI",
    "SUZIKI": "SUZUKI",
    "TELSA" : "TESLA",
    "THMNS" : "THOMAS BUILT BUSES",
    "THOM" : "THOMAS BUILT BUSES",
    "US" : "USPS",
    "US POSTAL" : "USPS",
    "YAMA" : "YAMAHA"
})

# ×©×œ×‘ 2ï¸âƒ£: ×”×’×“×¨×ª ×¨×©×™××ª ××•×ª×’×™× ×ª×§× ×™×™× (××•×ª×’×™× ×™×“×•×¢×™× ×©×œ ×¨×›×‘×™×)
known_brands = [
    'TOYOTA', 'HONDA', 'FORD', 'CHEVROLET', 'NISSAN', 'HYUNDAI', 'KIA',
    'MAZDA', 'MERCEDES-BENZ', 'BMW', 'VOLKSWAGEN', 'JEEP', 'ACURA', 'DODGE',
    'SUBARU', 'LEXUS', 'INFINITI', 'CHRYSLER', 'AUDI', 'BUICK', 'GMC',
    'THOMAS BUILT BUSES', 'GILLIG', 'FREIGHTLINER', 'BLUE BIRD', 'TESLA',
    'ISUZU', 'HARLEY DAVIDSON', 'PETERBILT', 'MITSUBISHI', 'MINI','CADILLAC',
    'VOLVO', 'NEW FLYER','JAGUAR','LINCOLN','MACK','ORION','PIERCE MANUFACTURING','PONTIAC',
    'PORSCHE','PTRB','RAM','RANGE ROVER','SATURN','SCION', 'SPARTAN','SUZUKI','TBU','TRANZIT',
    'TRIUMPH','TSMR','USPS','YAMAHA'
]

# ×©×œ×‘ 3ï¸âƒ£: fuzzy matching â€“ ×”×ª×××ª ×¢×¨×›×™× ×™×“× ×™×™× ×œ×¨×©×™××ª ×”××•×ª×’×™× ×”×ª×§× ×™×™×
# ×× ×”×“××™×•×Ÿ ×‘×™×Ÿ ×¢×¨×š ×‘×¢××•×“×” ×œ×‘×™×Ÿ ××•×ª×’ ×ª×§× ×™ ×”×•× >= 80%, × ×—×œ×™×£ ×œ××•×ª×’.
# ×× ×œ× â€“ × ×›× ×™×¡ ×œ×§×˜×’×•×¨×™×” "other"
mapping_dict = {}
for make in df["Vehicle Make"].dropna().unique():
    match, score, _ = process.extractOne(make, known_brands, scorer=fuzz.token_sort_ratio)
    if score >= 80:
        mapping_dict[make] = match
    else:
        mapping_dict[make] = "OTHER"

# ×©×œ×‘ 4ï¸âƒ£: ×”×—×œ×ª ×”×”××¨×” ×¢×œ ×›×œ ×”×¢×¨×›×™× ×‘×¢××•×“×” â€“ ×›×•×œ×œ NaN (×™×”×¤×•×š ×œ"OTHER")
df["Vehicle Make"] = df["Vehicle Make"].apply(lambda x: mapping_dict.get(x, "OTHER"))

# ×©×œ×‘ 5ï¸âƒ£: ×”×¦×’×ª ×¡×™×›×•×
print("âœ… ××¡×¤×¨ ×¢×¨×›×™× ×™×™×—×•×“×™×™× ×œ××—×¨ × ×™×§×•×™:", df["Vehicle Make"].nunique(dropna=True))
print("ğŸ“„ ××¡×¤×¨ ×¨×©×•××•×ª:", df.shape[0])
print("âŒ ××¡×¤×¨ ×¢×¨×›×™× ×©×”×¤×›×• ×œ-other:", (df["Vehicle Make"] == "OTHER").sum())
print("\nğŸ” ×”×¢×¨×›×™× ×œ××—×¨ ×”× ×™×§×•×™:")
print(df["Vehicle Make"].value_counts())

"""h3_index

 \Latitude, Longitude × ×•×¦×¨ ×‘×××¦×¢×•×ª ×”×¢××•×“×•×ª

"""

# # × ×¢×©×” ××–×•×¨×™× ×œ××™×§×•× ×¢×œ ×™×“×™ ×§×• ××•×¨×š ×•×§×• ×¨×•×—×‘
# # ×©×”×•×¤×›×ª ××ª ×”×§×• ××•×¨×š ×•×”×§×• ×’×•×‘×” ×œ××–×•×¨×™× h3 ×œ×›×Ÿ × ×©×ª××©  ×‘×¡×¤×¨×™×™×”

# H3 ×œ×”×’×“×™×¨ ×¤×•× ×§×¦×™×” ×©×××™×¨×” ×§×•×•×™ ×¨×•×—×‘/××•×¨×šÂ ×œ××™× ×“×§×¡
def lat_lng_to_h3(row, resolution=7):
    return h3.latlng_to_cell(row['Latitude'], row['Longitude'], resolution)

#×œ××‘× ×” ×”× ×ª×•× ×™× ×‘×¨×–×•×œ×•×¦×™×” 6 H3 ××•×¡×™×¤×”
# × ×™×ª×Ÿ ×œ×©× ×•×ª ××ª ×”×¨×–×•×œ×•×¦×™×” (0â€“15) ×‘×”×ª×× ×œ×¦×•×¨×š
# ×¨×–×•×œ×•×¦×™×” ×’×‘×•×”×” ×™×•×ª×¨ = ××©×•×©×™×Â ×§×˜× ×™×Â ×™×•×ª×¨
resolution = 6
df['h3_index'] = df.apply(lat_lng_to_h3, resolution=resolution, axis=1)

#×œ×©× ×•×ª ××ª ×”×¦×‘×¢×™× ×©×œ ×”××©×•×©×™× ×‘×§×¤×œ×¨ ×œ×¤×™ ×›××•×ª ×”×ª××•× ×•×ª
df_agg = df.groupby('h3_index')['Latitude'].count().reset_index(name="Number Of Accidents")

#×œ×›××” ××©×•×©×™× ×”×©×˜×— (×”×ª××•× ×•×ª) ××ª×—×œ×§×•×ª
df.h3_index.nunique()

## ×”×§×•×“ ×©××™×™×¦×¨ ××ª ×”×§×•×‘×¥  ×©×œ ×”×§×¤×œ×¨

from keplergl import KeplerGl
# Create a Kepler.gl map
config = {
    "mapState": {
        "latitude": 39.0458,     # Maryland center ××™×¤×” ×”××¤×” ×ª×¤×ª×— ×‘×ª×•×¨ ×”×ª×—×œ×”
        "longitude": -76.6413,
        "zoom": 8,
        "bearing": 0,
        "pitch": 0
    }
}

map_1 = KeplerGl(height=600, config=config)

# Add data to the map
map_1.add_data(data=df_agg, name="H3 Points")

# Display the map
map_1

map_1.save_to_html(file_name='kepler_map.html')

# Display HTML map in Colab
from google.colab import files
files.download('kepler_map.html')  # DownloadÂ theÂ HTMLÂ file

# ×©×”×•×¤×›×ª ××ª ×”×§×• ××•×¨×š ×•×’×•×‘×” ×œ××–×•×¨×™× ×•×œ×›×Ÿ ××™×Ÿ ×©×™××•×© ×‘×©××¨ ×¢××•×“×•×ª ×”××™×§×•××™× h3 ×¢××•×“×•×ª ××œ×• ×”×•×¡×¨×• ×œ××—×¨ ×©×™××•×© ×‘×¡×¤×¨×™×™×ª
columns_to_drop = [
    'Road Name',
    'Cross-Street Name',
    'Latitude',
    'Longitude'
]

df = df.drop(columns=columns_to_drop)

# ×”×•×“×¢×” ×œ××¢×§×‘
print(" ×”×¢××•×“×•×ª ×”×‘××•×ª × ××—×§×• ××”×˜×‘×œ×”: ", columns_to_drop)

"""×˜×™×¤×•×œ ×‘×¢×¨×›×™× ×—×¡×¨×™×"""

# (×—×¦×™×•×Ÿ/××—×¨) ×˜×™×¤×•×œ ×‘×¢×¨×›×™× ×—×¡×¨×™×
for col in df.columns:
    if df[col].dtype == 'object' or str(df[col].dtype) == 'category':
        # most_common = df[col].mode()[0]  # ×©×›×™×—
        # df[col].fillna(most_common, inplace=True)
        df[col] = df[col].fillna('other') # ××—×¨
    else:
        mean_value = df[col].median()  # ×—×¦×™×•×Ÿ
        df[col].fillna(mean_value, inplace=True)

#CSV ×©××™×¨×ª ×”×“××˜×” ×œ××—×¨ ×”× ×™×§×•×™ ×‘×§×•×‘×¥
df.to_csv("processed_data.csv")

"""# ×’×¨×¤×™× ×•×˜×‘×œ××•×ª ×œ× ×™×ª×•×— ×”× ×ª×•× ×™×"""

# ×˜×‘×œ×” ×œ×¨××•×ª ××ª ×”×›××•×ª ×”×¢×¨×›×™× ×”×™×™×—×•×“×™× ×œ×¤× ×™ ×•××—×¨×™ ×›×œ ×”× ×™×§×•×™×™×

#×¢×©×™× ×• ×§×•×“ ×œ××¢×œ×” ×œ×”×¢×ª×§×ª ×”× ×ª×•× ×™× ×œ×¤× ×™ ×›×œ ×©×™× ×•×™
unique_before = df_original.nunique()
unique_after = df.nunique()

# ×™×¦×™×¨×ª ×˜×‘×œ×” ××—×ª ×©××©×•×•×”
unique_comparison = pd.DataFrame({
    'Unique Values (Before)': unique_before,
    'Unique Values (After)': unique_after
}).sort_values('Unique Values (Before)', ascending=False)

display(unique_comparison)

#× ×™×ª×•×— ×¢××•×“×ª ×”××˜×¨×”
# ×™×¦×™×¨×ª ×˜×‘×œ×ª ×©×›×™×—×•×™×•×ª
target_counts = df['Injury Severity'].value_counts()

# ×¡×™×“×•×¨ ×œ×¤×™ ×¡×“×¨ ×”×™×¨×¨×›×™ (××•×¤×¦×™×•× ×œ×™)
order = ['no apparent injury', 'possible injury', 'suspected minor injury', 'suspected serious injury', 'fatal injury']

# ×”×’×¨×£
plt.figure(figsize=(10, 6))
sns.set_style("whitegrid")
sns.barplot(x=target_counts.index, y=target_counts.values, palette='Blues_d', order=order)

# ×›×•×ª×¨×•×ª ×•×ª×™×•×’×™×
plt.title('Distribution of Injury Severity', fontsize=18, pad=20)
plt.xlabel('Injury Severity', fontsize=14)
plt.ylabel('Count', fontsize=14)
plt.xticks(rotation=45, ha='right', fontsize=12)
plt.yticks(fontsize=12)

# ×”×¦×’×ª ××¡×¤×¨×™× ××¢×œ ×›×œ ×¢××•×“×”
for i, v in enumerate(target_counts.loc[order]):
    plt.text(i, v + 2000, f'{v:,}', ha='center', va='bottom', fontsize=12)

plt.tight_layout()
plt.show()

# ×¡×˜×˜×¡×™×˜×§×•×ª ×ª×™××•×¨×™×•×ª ×©×œ ××©×ª× ×™× ××¡×¤×¨×™×™×
# ×‘×—×™×¨×ª ××©×ª× ×™× ××¡×¤×¨×™×™×
numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns

# ×™×¦×™×¨×ª ×˜×‘×œ×ª ×¡×˜×˜×™×¡×˜×™×§×”
df_numerical_stats = df[numerical_cols].describe().T

# ×¢×™×¦×•×‘
styled_stats = df_numerical_stats.style\
    .background_gradient(cmap='Blues', subset=['mean', 'std', 'min', '25%', '50%', '75%', 'max'])\
    .format("{:.2f}")\
    .set_caption("×¡×˜×˜×™×¡×˜×™×§×•×ª ×ª×™××•×¨×™×•×ª ×¢×‘×•×¨ ××©×ª× ×™× ××¡×¤×¨×™×™×")\
    .set_table_styles(
        [{'selector': 'th', 'props': [('font-size', '14pt'), ('text-align', 'center')]},
         {'selector': 'td', 'props': [('font-size', '12pt'), ('text-align', 'center')]}]
    )\
    .set_properties(**{'border': '1px solid black', 'padding': '6px'})

styled_stats

#×’×¨×£ ×œ×©×¢×” ×‘×™×•×
# ×˜×‘×œ×ª ×©×›×™×—×•×™×•×ª ×œ×¤×™ ×©×¢×”
hour_counts = df['Crash Hour'].value_counts().sort_index()

# ×”×“×¤×¡×ª ×˜×‘×œ×”
print("×˜×‘×œ×ª ×©×›×™×—×•×™×•×ª ×œ×¤×™ ×©×¢×”:")
print(hour_counts)

plt.figure(figsize=(12,6))
sns.set_style("whitegrid")
ax = sns.barplot(x=hour_counts.index, y=hour_counts.values, color="#4F81BD")
plt.title('Distribution of Crashes by Hour of Day', fontsize=18, pad=20)
plt.xlabel('Hour of Day', fontsize=14)
plt.ylabel('Number of Crashes', fontsize=14)
plt.xticks(range(0,24), fontsize=12)
plt.yticks(fontsize=12)
plt.grid(axis='y', linestyle='--', alpha=0.7)

# ×”×•×¡×¤×ª ××¡×¤×¨×™× ××¢×œ ×›×œ ×¢××•×“×”
for container in ax.containers:
    ax.bar_label(container, fmt='%d', fontsize=10, label_type='edge', padding=2)

plt.tight_layout()
plt.show()

#×’×¨×£ ×œ×™×•× ×‘×©×‘×•×¢
# ×˜×‘×œ×ª ×©×›×™×—×•×™×•×ª ×œ×¤×™ ×™×•×
day_counts = df['Crash Day'].value_counts().sort_index()

# ××™×¤×•×™ ××¡×¤×¨ -> ×©× ×™×•×
day_names = {
    0: 'Monday', 1: 'Tuesday', 2: 'Wednesday', 3: 'Thursday',
    4: 'Friday', 5: 'Saturday', 6: 'Sunday'
}
day_counts.index = day_counts.index.map(day_names)

# ×”×“×¤×¡×ª ×˜×‘×œ×”
print("×˜×‘×œ×ª ×©×›×™×—×•×™×•×ª ×œ×¤×™ ×™×•×:")
print(day_counts)

plt.figure(figsize=(10,6))
sns.set_style("whitegrid")
ax = sns.barplot(x=day_counts.index, y=day_counts.values, color="#4F81BD")
plt.title('Distribution of Crashes by Day of Week', fontsize=18, pad=20)
plt.xlabel('Day of Week', fontsize=14)
plt.ylabel('Number of Crashes', fontsize=14)
plt.xticks(rotation=45, fontsize=12)
plt.yticks(fontsize=12)
plt.grid(axis='y', linestyle='--', alpha=0.7)

# ×”×•×¡×¤×ª ××¡×¤×¨×™× ××¢×œ ×›×œ ×¢××•×“×”
for container in ax.containers:
    ax.bar_label(container, fmt='%d', fontsize=10, label_type='edge', padding=2)

plt.tight_layout()
plt.show()

#×’×¨×£ ×œ×—×•×“×© ×‘×©× ×”
# ×˜×‘×œ×ª ×©×›×™×—×•×™×•×ª ×œ×¤×™ ×—×•×“×©
month_counts = df['Crash Month'].value_counts().sort_index()

# ××™×¤×•×™ ××¡×¤×¨ -> ×©× ×—×•×“×©
month_names = {1: 'January', 2: 'February', 3: 'March', 4: 'April',
               5: 'May', 6: 'June', 7: 'July', 8: 'August',
               9: 'September', 10: 'October', 11: 'November', 12: 'December'}
month_counts.index = month_counts.index.map(month_names)

# ×”×“×¤×¡×ª ×˜×‘×œ×”
print("×˜×‘×œ×ª ×©×›×™×—×•×™×•×ª ×œ×¤×™ ×—×•×“×©:")
print(month_counts)

plt.figure(figsize=(12,6))
sns.set_style("whitegrid")
ax = sns.barplot(x=month_counts.index, y=month_counts.values, color="#4F81BD")
plt.title('Distribution of Crashes by Month', fontsize=18, pad=20)
plt.xlabel('Month', fontsize=14)
plt.ylabel('Number of Crashes', fontsize=14)
plt.xticks(rotation=45, fontsize=12)
plt.yticks(fontsize=12)
plt.grid(axis='y', linestyle='--', alpha=0.7)

# ×”×•×¡×¤×ª ××¡×¤×¨×™× ××¢×œ ×›×œ ×¢××•×“×”
for container in ax.containers:
    ax.bar_label(container, fmt='%d', fontsize=10, label_type='edge', padding=2)

plt.tight_layout()
plt.show()

#×¤×•× ×¨×¦×™×” ×œ×”×¨×¦×ª ×’×¨×¤×™× ×œ×¢××•×“×•×ª ×”×©×•× ×•×ª
def plot_categorical_distribution(df, column, top_n=None, palette="pastel", rotation=45):

    # ×˜×‘×œ×ª ×©×›×™×—×•×™×•×ª
    value_counts = df[column].value_counts(dropna=False)

    # ×× ×”××©×ª××© ×”×’×“×™×¨ top_n
    if top_n is not None:
        value_counts = value_counts.head(top_n)

    # ×”×“×¤×¡×ª ×”×˜×‘×œ×”
    print(f"\n×˜×‘×œ×ª ×©×›×™×—×•×™×•×ª ×¢×‘×•×¨ {column}:")
    print(value_counts)

    # ×§×‘×™×¢×ª ×’×•×“×œ ×’×¨×£ ××•×˜×•××˜×™ ×œ×¤×™ ××¡×¤×¨ ×§×˜×’×•×¨×™×•×ª
    num_categories = len(value_counts)
    width = max(12, num_categories * 0.6)   # ×œ×¤×—×•×ª 12, ××—×¨×ª ××ª×¨×—×‘ ×‘×™×—×¡ ×œ×§×˜×’×•×¨×™×•×ª
    height = 6 if num_categories < 15 else 8

    # ×’×¨×£
    plt.figure(figsize=(width, height))
    sns.set_style("whitegrid")
    ax = sns.barplot(
        x=value_counts.index.astype(str),
        y=value_counts.values,
    )

    plt.title(f"Distribution of {column}", fontsize=18, pad=20)
    plt.xlabel(column, fontsize=14)
    plt.ylabel("Count", fontsize=14)
    plt.xticks(rotation=rotation, ha='right', fontsize=12)
    plt.yticks(fontsize=12)
    plt.grid(axis='y', linestyle='--', alpha=0.7)

    # ×”×•×¡×¤×ª ××¡×¤×¨×™× ××¢×œ ×”×¢××•×“×•×ª
    for container in ax.containers:
        ax.bar_label(container, fmt='%d', fontsize=10, label_type='edge', padding=2)

    plt.tight_layout()
    plt.show()

plot_categorical_distribution(df, "Agency Name")

plot_categorical_distribution(df, "ACRS Report Type")

plot_categorical_distribution(df, "Collision Type")

plot_categorical_distribution(df, "Weather")

plot_categorical_distribution(df, "Surface Condition")

plot_categorical_distribution(df, "Light")

plot_categorical_distribution(df, "Traffic Control")

#driver at fault
import matplotlib.pyplot as plt

plt.rcParams['font.family'] = 'DejaVu Sans'

# ×˜×‘×œ×ª ×©×›×™×—×•×™×•×ª
fault_counts = df['Driver At Fault'].value_counts()
print(fault_counts)

# ×”×›× ×ª ×ª×•×•×™×•×ª ××©×•×œ×‘×•×ª (Label + ××—×•×–×™×)
labels = [f'{label} ({percent:.1f}%)' for label, percent in zip(fault_counts.index, 100 * fault_counts / fault_counts.sum())]

# ×¦×‘×¢×™× × ××™×
colors = ['#66B2FF', '#FF9933','#99CC66']  # ×›×—×•×œ ×•×›×ª×•× ×¢×“×™×Ÿ

# ×’×¨×£ ×¤××™
plt.figure(figsize=(8, 8))
plt.pie(fault_counts,
        labels=labels,
        autopct=None,
        startangle=90,
        counterclock=False,
        colors=colors)
plt.title('Distribution of Driver Fault in Accidents')
plt.show()

plot_categorical_distribution(df, "Driver Distracted By")

plot_categorical_distribution(df, "Drivers License State")

plot_categorical_distribution(df, "Vehicle Damage Extent")

plot_categorical_distribution(df, "Vehicle First Impact Location")

plot_categorical_distribution(df, "Vehicle Body Type")

plot_categorical_distribution(df, "Vehicle Movement")

plot_categorical_distribution(df, "Vehicle Going Dir")

plot_categorical_distribution(df, "Route Type")

plot_categorical_distribution(df, "Vehicle Make")

""" **× ×™×ª×•×— ×©×œ ×”×¢××•×“×•×ª ×¢× ×¢××•×“×ª ×”××˜×¨×”**"""

# ×¤×•× ×§×¦×™×” ×›×œ×œ×™×ª ×œ×—×™×©×•×‘ ×”×¤×™×¦×¨×™× ×”×©×•× ×™× ××•×œ ×¢××•×“×ª ×”××˜×¨×”
def analyze_categorical_relationship(feature_column, target_column):
    # ×˜×‘×œ×ª ×©×›×™×—×•×™×•×ª
    crosstab = pd.crosstab(df[feature_column], df[target_column], normalize='index')* 100
    display(crosstab)  # ××¦×™×’ ××ª ×”×˜×‘×œ×”

    # ×’×¨×£
    crosstab.plot(kind='bar', stacked=True, figsize=(10,6))
    plt.legend(title='Injury Severity', bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.title(f'Distribution of {target_column} by {feature_column}')
    plt.ylabel('Percentage')
    plt.xlabel(feature_column)
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

    # ×—×™-×‘×¨×™×‘×•×¢
    contingency_table = pd.crosstab(df[feature_column], df[target_column])
    chi2, p, dof, expected = chi2_contingency(contingency_table)

    # ×—×™×©×•×‘ CramÃ©r's V
    n = contingency_table.sum().sum()
    min_dim = min(contingency_table.shape) - 1
    cramers_v = np.sqrt(chi2 / (n * min_dim))

    # ×”×“×¤×¡×ª ×ª×•×¦××•×ª
    print(f"Chi-Square Statistic: {chi2:.4f}")
    print(f"P-Value: {p:.4f}")
    print(f"CramÃ©r's V: {cramers_v:.4f}")

# ×‘×“×™×§×ª ×§×©×¨ ×‘×™×Ÿ ×¢××•×“×ª ×”××˜×¨×” ×œ×¢××•×“×ª ACRS Report Type
analyze_categorical_relationship('ACRS Report Type', 'Injury Severity')

# ×‘×“×™×§×ª ×§×©×¨ ×‘×™×Ÿ ×¢××•×“×ª ×”××˜×¨×” ×œ×¢××•×“×ª Vehicle Damage Extent
analyze_categorical_relationship('Vehicle Damage Extent', 'Injury Severity')

#×¤×•× ×§×¦×™×” ×œ×”×¨×¦×ª ×’×¨×¤×™× ×œ× ×™×ª×•×— ×‘×™×Ÿ ××©×ª× ×™ ×”×–××Ÿ ×œ××©×ª× ×” ×”××˜×¨×”
def analyze_time_columns_vs_target(df):
    # 3 ××©×ª× ×™ ×”×–××Ÿ
    time_columns = ['Crash Hour', 'Crash Day', 'Crash Month']
    target_column = 'Injury Severity'

    for col in time_columns:
        print(f"\nğŸ“Š × ×™×ª×•×— {col} ××•×œ {target_column}")
        print('-'*50)

        # Boxplot
        plt.figure(figsize=(10,6))
        sns.boxplot(x=target_column, y=col, data=df)
        plt.title(f'{col} by {target_column}')
        plt.ylabel(col)
        plt.xlabel(target_column)
        plt.xticks(rotation=45)
        plt.show()

        # ANOVA
        groups = [df[df[target_column] == g][col] for g in df[target_column].unique()]
        f_stat, p_val = f_oneway(*groups)

        print(f"ANOVA F-Statistic: {f_stat:.4f}")
        print(f"P-Value: {p_val:.4f}")

        if p_val < 0.05:
            print("âœ… ×™×© ×”×‘×“×œ ××•×‘×”×§ ×¡×˜×˜×™×¡×˜×™×ª ×‘×™×Ÿ ×”×§×‘×•×¦×•×ª (p < 0.05)")
        else:
            print("âš ï¸ ××™×Ÿ ×”×‘×“×œ ××•×‘×”×§ ×¡×˜×˜×™×¡×˜×™×ª ×‘×™×Ÿ ×”×§×‘×•×¦×•×ª (p >= 0.05)")

# ×”×¨×¦×ª ×”×¤×•× ×§×¦×™×”
analyze_time_columns_vs_target(df)

# ×˜×‘×œ×ª ×©×›×™×—×•×™×•×ª: ×©×¢×” ××•×œ ×—×•××¨×ª ×¤×¦×™×¢×”
hour_injury_crosstab = pd.crosstab(df['Crash Hour'], df['Injury Severity'], normalize='index') * 100

plt.figure(figsize=(14,8))
sns.heatmap(hour_injury_crosstab, annot=True, fmt='.1f', cmap='coolwarm')
plt.title('Heatmap of Injury Severity by Crash Hour')
plt.ylabel('Crash Hour')
plt.xlabel('Injury Severity')
plt.show()

day_heatmap = pd.crosstab(df['Crash Day'], df['Injury Severity'])

plt.figure(figsize=(12, 6))
sns.heatmap(day_heatmap, annot=True, fmt='d', cmap='YlGnBu')
plt.title('Number of Crashes by Day of Week and Injury Severity')
plt.xlabel('Injury Severity')
plt.ylabel('Day of Week')
plt.tight_layout()
plt.show()

# ×˜×‘×œ×ª ×©×›×™×—×•×™×•×ª
day_vs_severity = pd.crosstab(df['Crash Day'], df['Injury Severity'])

# ×’×¨×£ ×‘×¨×™× ××¨×•×‘×“
df_day_grouped = day_vs_severity.reset_index().melt(id_vars='Crash Day',
                                                    var_name='Injury Severity',
                                                    value_name='Count')
plt.figure(figsize=(10, 6))
sns.barplot(data=df_day_grouped, x='Crash Day', y='Count', hue='Injury Severity')
plt.title("Injury Severity by Day of Week")
plt.ylabel("Number of Crashes")
plt.xlabel("Day of Week")
plt.grid(axis='y')
plt.tight_layout()
plt.show()

# ×‘×“×™×§×ª ×§×©×¨ ×‘×™×Ÿ ×¢××•×“×ª ×”××˜×¨×” ×œ×¢××•×“×ª Speed Limit

# Boxplot: ×§×©×¨ ×‘×™×Ÿ ×¨××ª ×”×¤×¦×™×¢×” ×œ××”×™×¨×•×ª
plt.figure(figsize=(10, 6))
sns.boxplot(data=df, x='Injury Severity', y='Speed Limit')
plt.title('Speed Limit by Injury Severity')
plt.xlabel('Injury Severity')
plt.ylabel('Speed Limit')
plt.grid(True)
plt.show()

# ×¡×˜×˜×™×¡×˜×™×§×•×ª ×ª×™××•×¨×™×•×ª ×œ×¤×™ ×§×˜×’×•×¨×™×™×ª ×”×¤×¦×™×¢×”
summary_stats = df.groupby('Injury Severity')['Speed Limit'].describe()
display(summary_stats)

# ×‘×“×™×§×ª ×§×©×¨ ×‘×™×Ÿ ×¢××•×“×ª ×”××˜×¨×” ×œ×¢××•×“×ª Speed Limit

# ğŸ§¹ × ×™×¤×•×™ ×¢×¨×›×™× ×—×¡×¨×™×
df_speed = df[['Injury Severity', 'Speed Limit']].dropna()

# ğŸªœ ×”×¤×™×›×ª Speed Limit ×œ×§×˜×’×•×¨×™×” (×œ×“×•×’××”: bins ×©×œ ××”×™×¨×•×ª)
bins = [0, 30, 50, 70, 100, np.inf]
labels = ['0-30', '31-50', '51-70', '71+', '100+']
df_speed['Speed Category'] = pd.cut(df_speed['Speed Limit'], bins=bins, labels=labels)

# ğŸ“Š ×˜×‘×œ×ª ×©×›×™×—×•×™×•×ª
contingency_table = pd.crosstab(df_speed['Speed Category'], df_speed['Injury Severity'])

# ğŸ”¥ heatmap
plt.figure(figsize=(10, 6))
sns.heatmap(contingency_table, annot=True, fmt='d', cmap='YlOrRd')
plt.title('Heatmap: Speed Category vs Injury Severity')
plt.xlabel('Injury Severity')
plt.ylabel('Speed Category')
plt.show()

# ××‘×—×Ÿ ×—×™-×‘×¨×™×‘×•×¢
chi2, p, dof, expected = stats.chi2_contingency(contingency_table)
n = contingency_table.sum().sum()
phi2 = chi2 / n
r, k = contingency_table.shape
cramers_v = np.sqrt(phi2 / min(k - 1, r - 1))


# ×”×“×¤×¡×” ×‘×¡×’× ×•×Ÿ ×©×‘×™×§×©×ª
print(f"Chi-Square Statistic: {chi2:.3f}")
print(f"P-Value: {p:.5e}")
print(f"CramÃ©r's V: {cramers_v:.6f}")

# ×˜×‘×œ×ª ×”×¦×œ×‘×” ×¢× ××—×•×–×™× ×œ×›×œ ×©×•×¨×ª Route Type
route_vs_injury = pd.crosstab(df['Route Type'], df['Injury Severity'], normalize='index') * 100

plt.figure(figsize=(14, 10))
sns.heatmap(route_vs_injury, annot=True, fmt=".2f", cmap="Blues")
plt.title("Distribution (%) of Injury Severity by Route Type")
plt.ylabel("Route Type")
plt.xlabel("Injury Severity")
plt.tight_layout()
plt.show()

# ×—×™×©×•×‘ ××‘×—×Ÿ ×—×™ ×‘×¨×™×‘×•×¢
chi2_stat, p_val, dof, expected = chi2_contingency(contingency_table)

# ×—×™×©×•×‘ CramÃ©r's V
n = contingency_table.sum().sum()
min_dim = min(contingency_table.shape) - 1
cramers_v = np.sqrt(chi2_stat / (n * min_dim))

# ×”×“×¤×¡×ª ×ª×•×¦××•×ª ×”××‘×—×Ÿ
print("Chi-Square Statistic:", chi2_stat)
print("P-Value:", p_val)
print("CramÃ©r's V:", cramers_v)

#Driver Substance Abuse ×”×§×©×¨ ×‘×™×Ÿ ×¢××•×“×ª ×”××˜×¨×” ×œ

def analyze_driver_substance_abuse(df, target_col='Injury Severity'):
    feature_col = 'Driver Substance Abuse'
    data = df[[feature_col, target_col]].dropna()

    # ×˜×‘×œ×ª ×©×›×™×—×•×™×•×ª
    contingency = pd.crosstab(data[feature_col], data[target_col])

    # Heatmap
    plt.figure(figsize=(10, 6))
    sns.heatmap(contingency, annot=True, fmt='d', cmap='YlGnBu', linewidths=0.5, linecolor='gray')
    plt.title('Driver Substance Abuse vs Injury Severity')
    plt.xlabel('Injury Severity')
    plt.ylabel('Driver Substance Abuse')
    plt.tight_layout()
    plt.show()

    #×¢× ××—×•×–×™× Heatmap
    # ×—×™×©×•×‘ ××—×•×–×™× ×œ×¤×™ ×©×•×¨×•×ª
    row_percent = contingency.div(contingency.sum(axis=1), axis=0) * 100

    # ×¦×™×•×¨ heatmap
    plt.figure(figsize=(14, 8))
    sns.heatmap(row_percent, annot=True, fmt=".1f", cmap="YlGnBu", linewidths=0.5, linecolor='gray', cbar_kws={'label': 'Percentage (%)'})

    plt.title(f"{feature_col} vs {target_col} (Row %)", fontsize=16)
    plt.xlabel(target_col, fontsize=12)
    plt.ylabel(feature_col, fontsize=12)
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

    # ××‘×—×Ÿ ×—×™-×‘×¨×™×‘×•×¢
    chi2, p, dof, expected = chi2_contingency(contingency)

    # CramÃ©r's V
    n = contingency.sum().sum()
    phi2 = chi2 / n
    r, k = contingency.shape
    cramers_v = np.sqrt(phi2 / min(k - 1, r - 1))

    # ×ª×¦×•×’×ª ×ª×•×¦××•×ª ×‘×œ×‘×“
    print("\nğŸ§ª ×ª×•×¦××•×ª ×¡×˜×˜×™×¡×˜×™×•×ª:")
    print(f"Chi-Square Statistic: {chi2:.3f}")
    print(f"P-Value: {p:.6e}")
    print(f"CramÃ©r's V: {cramers_v:.6f}")
analyze_driver_substance_abuse(df)

#Vehicle Body Type ×”×§×©×¨ ×‘×™×Ÿ ×¢××•×“×ª ×”××˜×¨×” ×œ

def analyze_vehicle_body_type(df, target_col='Injury Severity'):
    feature_col = 'Vehicle Body Type'
    data = df[[feature_col, target_col]].dropna()

    # ×¦××¦×•× ×œ×¨×›×‘×™× ×¢× ×œ×¤×—×•×ª 100 ××•×¤×¢×™× (×›×“×™ ×œ× ×œ×”×¢××™×¡)
    top_types = data[feature_col].value_counts()[data[feature_col].value_counts() >= 100].index
    data = data[data[feature_col].isin(top_types)]

    # ×˜×‘×œ×ª ×©×›×™×—×•×™×•×ª
    contingency = pd.crosstab(data[feature_col], data[target_col])

    # Heatmap
    plt.figure(figsize=(12, 7))
    sns.heatmap(contingency, annot=True, fmt='d', cmap='Oranges', linewidths=0.5, linecolor='gray')
    plt.title('Vehicle Body Type vs Injury Severity')
    plt.xlabel('Injury Severity')
    plt.ylabel('Vehicle Body Type')
    plt.tight_layout()
    plt.show()

    # ×¡×˜×˜×™×¡×˜×™×§×•×ª
    chi2, p, dof, expected = chi2_contingency(contingency)
    n = contingency.sum().sum()
    phi2 = chi2 / n
    r, k = contingency.shape
    cramers_v = np.sqrt(phi2 / min(k - 1, r - 1))

    print(f"\nğŸ§ª ×ª×•×¦××•×ª ×¡×˜×˜×™×¡×˜×™×•×ª:")
    print(f"Chi-Square Statistic: {chi2:.3f}")
    print(f"P-Value: {p:.6e}")
    print(f"CramÃ©r's V: {cramers_v:.6f}")

analyze_vehicle_body_type(df)

# ×¡×¤×™×¨×” ×›×•×œ×œ×ª ×©×œ ×¢×¨×›×™× ×—×¡×¨×™× ×‘×›×œ ×”×¢××•×“×•×ª
missing_summary = df.isna().sum()

# ××¦×™×’ ×¨×§ ×¢××•×“×•×ª ×©×‘×”×Ÿ ×™×© ×œ×¤×—×•×ª ×¢×¨×š ×—×¡×¨ ××—×“
missing_summary = missing_summary[missing_summary > 0].sort_values(ascending=False)

print("ğŸ“Š ×¢××•×“×•×ª ×¢× ×¢×¨×›×™× ×—×¡×¨×™×:")
print(missing_summary)

# ××—×•×–×™× â€“ ×œ×¨××•×ª ×™×—×¡×™×ª ×œ×’×•×“×œ ×”×“××˜×”
missing_percent = (missing_summary / len(df)) * 100
print("\nğŸ“‰ ××—×•×–×™ ×—×¡×¨×™×:")
print(missing_percent)

# ×”×¦×’×ª ×©××•×ª ×›×œ ×”×¢××•×“×•×ª
print(df.columns.tolist())

"""#CatBoost - ×”××•×“×œ ×”× ×‘×—×¨
**×—×™×–×•×™ ×‘×™× ××¨×™ - ×™×© ×¤×¦×™×¢×”/××™×Ÿ ×¤×¦×™×¢×”**
"""

from catboost import CatBoostClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, f1_score
from imblearn.over_sampling import SMOTENC  # ×’×™×¨×¡×” ×©×œ SMOTE ×©×ª×•××›×ª ×‘×¢××•×“×•×ª ×§×˜×’×•×¨×™××œ×™×•×ª
from sklearn.metrics import confusion_matrix
from sklearn.preprocessing import LabelEncoder
from collections import Counter #×¡×•×¤×¨ ×›××” ××•×¤×¢×™× ×™×© ×‘×›×œ ×§×˜×’×•×¨×™×” ×‘×¢××•×“×ª ×”××˜×¨×” ×•××—×–×™×¨ ×›××™×œ×•×Ÿ(×¢×‘×•×¨ ×—×™×©×•×‘ ××©×§×œ×™×)
from dataclasses import dataclass
from typing import Dict, Any, List, Tuple
from catboost import CatBoostClassifier, Pool

# ===============================
# 1. ×™×™×‘×•× ×¡×¤×¨×™×•×ª
# ===============================
import pandas as pd
import numpy as np
from catboost import CatBoostClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, f1_score
from imblearn.over_sampling import SMOTENC  # ×’×™×¨×¡×” ×©×œ SMOTE ×©×ª×•××›×ª ×‘×¢××•×“×•×ª ×§×˜×’×•×¨×™××œ×™×•×ª

# ===============================
# 2. ×™×¦×™×¨×ª ×¢××•×“×ª ××˜×¨×” ×‘×™× ××¨×™×ª
# ===============================
# ×¡×™× ×•×Ÿ ×©×•×¨×•×ª ×©×—×¡×¨×” ×‘×”×Ÿ ×—×•××¨×ª ×¤×¦×™×¢×”
df = df[df['Injury Severity'].notna()].copy()

# ×”××¨×” ×©×œ ×¢××•×“×ª ×”×¤×¦×™×¢×” ×œ×¢×¨×š ×‘×™× ××¨×™: 0 = ××™×Ÿ ×¤×¦×™×¢×”, 1 = ×™×© ×¤×¦×™×¢×”
df['Injury Binary'] = df['Injury Severity'].apply(
    lambda x: 0 if x.strip().lower() == 'no apparent injury' else 1
)

# ===============================
# 3. ×”×’×“×¨×ª X ×•-y
# ===============================
# ×”×¤×¨×“×ª ××©×ª× ×™× ××¡×‘×™×¨×™× (X) ×•×¢××•×“×ª ×”××˜×¨×” (y)
X = df.drop(columns=['Injury Severity', 'Injury Binary'])
y = df['Injury Binary']

# ×–×™×”×•×™ ×¢××•×“×•×ª ×§×˜×’×•×¨×™××œ×™×•×ª (object) ×•×”×’×“×¨×ª×Ÿ ×›Ö¾category â€“ × ×•×— ×œ×¢×‘×•×“×” ×¢× SMOTENC ×•Ö¾CatBoost
cat_cols = X.select_dtypes(include='object').columns
for col in cat_cols:
    X[col] = X[col].astype('category')

# ===============================
# 4. ×¤×™×¦×•×œ ×œÖ¾Train ×•Ö¾Test
# ===============================
# ×¤×™×¦×•×œ ×”× ×ª×•× ×™× ×œ×¡×˜ ××™××•×Ÿ ×•×¡×˜ ×‘×“×™×§×” ×¢× stratify ×œ×¤×™ y ×œ×©××™×¨×” ×¢×œ ×¤×¨×•×¤×•×¨×¦×™×•×ª
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.3,
    random_state=42,
    stratify=y
)

# ===============================
# 5. ××™×–×•×Ÿ ×”× ×ª×•× ×™× ×‘×××¦×¢×•×ª SMOTENC
# ===============================
# ×•×•×™×“×•× ×©×¢××•×“×•×ª ×§×˜×’×•×¨×™××œ×™×•×ª ××•×’×“×¨×•×ª ×›Ö¾category ×’× ×‘Ö¾X_train
for col in cat_cols:
    X_train[col] = X_train[col].astype('category')

# ×™×¦×™×¨×ª ×¨×©×™××ª ××™× ×“×§×¡×™× ×©×œ ×”×¢××•×“×•×ª ×”×§×˜×’×•×¨×™××œ×™×•×ª ×¢×‘×•×¨ SMOTENC
cat_indices = [X_train.columns.get_loc(c) for c in cat_cols]

# ×™×¦×™×¨×ª ××•×¤×¢ ×©×œ SMOTENC ×¢× ×”×¢××•×“×•×ª ×”×§×˜×’×•×¨×™××œ×™×•×ª
smote_nc = SMOTENC(categorical_features=cat_indices, random_state=42)

# SMOTENC ×“×•×¨×© numpy â€“ ×”××¨×” ×¤× ×™××™×ª ××ª×‘×¦×¢×ª ××•×˜×•××˜×™×ª
X_train_resampled, y_train_resampled = smote_nc.fit_resample(X_train, y_train)

# ×”×—×–×¨×ª ×”× ×ª×•× ×™× ×œ×ª×•×š DataFrame (×›××• ×”××§×•×¨)
X_train_resampled = pd.DataFrame(X_train_resampled, columns=X_train.columns)

# ===============================
# 6. ××™××•×Ÿ ××•×“×œ CatBoost
# ===============================
# ×™×¦×™×¨×ª ××•×“×œ CatBoost ×¢× ×¤×¨××˜×¨×™× ××•×ª×××™×
model = CatBoostClassifier(
    iterations=500,
    learning_rate=0.05,
    depth=6,
    loss_function='Logloss',
    random_seed=42,
    verbose=0  # ×œ× ××¦×™×’ ×ª×•×¦××•×ª ×‘×›×œ ××™×˜×¨×¦×™×”
)

# ××™××•×Ÿ ×”××•×“×œ â€“ ×”×¢×‘×¨×ª ×©××•×ª ×”×¢××•×“×•×ª ×”×§×˜×’×•×¨×™××œ×™×•×ª (×•×œ× ××™× ×“×§×¡×™×)
model.fit(
    X_train_resampled,
    y_train_resampled,
    cat_features=list(cat_cols)
)

from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# ===============================
# 7. ×—×™×–×•×™ ×•×”×¢×¨×›×ª ×‘×™×¦×•×¢×™×
# ===============================
y_probs = model.predict_proba(X_test)[:, 1]


# ×¡×£ ×”×—×œ×˜×” ×©×œ 0.25 â€“ ×¨×’×™×©×•×ª ×’×‘×•×”×” ×™×•×ª×¨
threshold = 0.25
y_pred_thresh = (y_probs >= threshold).astype(int)

# ×“×•"×— ×¡×™×•×•×’
print("\nğŸ“Š ×ª×•×¦××•×ª CatBoost - ×¤×¦×™×¢×” / ××™×Ÿ ×¤×¦×™×¢×” (×¡×£ = 0.25):")
print(classification_report(y_test, y_pred_thresh, digits=2))

# Macro F1
macro_f1 = f1_score(y_test, y_pred_thresh, average='macro')
print(f"\nğŸ¯ Macro F1 Score (threshold = 0.25): {macro_f1:.4f}")

# ===============================
# ××˜×¨×™×¦×ª ×‘×œ×‘×•×œ
# ===============================
cm = confusion_matrix(y_test, y_pred_thresh)
labels = ['No Injury', 'Injury']

print("\nğŸ“Œ ××˜×¨×™×¦×ª ×‘×œ×‘×•×œ:")
print(cm)

# ×’×¨×£ Heatmap
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
            xticklabels=labels, yticklabels=labels)
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix - CatBoost (Threshold=0.25)")
plt.tight_layout()
plt.show()

"""**×‘×“×™×§×” ×× ×™×© ×§×©×¨ ×‘×™×Ÿ ×”×¡×ª×‘×¨×•×™×•×ª ×—×™×–×•×™ ×”××•×“×œ ×œ×¨××ª ×—×•××¨×ª ×”×¤×¦×™×¢×”**"""

### ×¨×¦×™× ×• ×œ×‘×“×•×§ ×œ×¤×™ ×”×¡×ª×‘×¨×•×™×•×ª ×‘××•×“×œ ×”× ×‘×—×¨(×”×‘×™× ××¨×™) ×× ×”×”×¡×ª×‘×¨×•×ª ×§×¨×•×‘×” ×™×•×ª×¨ ×œ 1 ××– ×”×¡×™×›×•×™ ×©×‘×××ª ×—×–×™× ×• × ×›×•×Ÿ ×©×™×© ×¤×¦×™×¢×” ×™×•×ª×¨ ×’×‘×•×” ×××©×¨ ×§×¦×ª ×¤×—×•×ª ×§×¨×•×‘ ×œ1 ×•×›×›×” ××¤×©×¨ ×’× ×œ×¢×–×•×¨ ×œ×ª×¢×“×£ ×¢×œ ×™×“×™ ×©× ×¡×¤×§ ×§×•×“× ×œ××œ×” ×©×‘×˜×•×— ×™×© ×œ×”× ×¤×¦×™×¢×”
results_bin_cat=df[['Injury Severity']].merge(y_test.to_frame(name='y_test_binary'),left_index=True,right_index=True)
results_bin_cat["y_prob"]=y_probs
results_bin_cat["y_pred"]=y_pred_thresh

# boxplot for each probability and injuery severity
results_bin_cat

plt.figure(figsize=(8, 5))
sns.boxplot(x='Injury Severity', y='y_prob', data=results_bin_cat, showfliers=False, boxprops=dict(facecolor='#4682B4'), medianprops=dict(color='red', linewidth=3))
plt.xticks(rotation=45, ha='right')
plt.xlabel('Injury Severity')
plt.ylabel('Predicted Probability of Injury')
plt.title('Predicted Probability by Injury Severity')
plt.tight_layout()
plt.show()

results_bin_cat

# × × ×™×— ×©×™×© ×œ×š DataFrame ×¢× ×ª×—×–×™×•×ª ×‘×©× results_bin_cat
# ×”×•× ×¦×¨×™×š ×œ×›×œ×•×œ ×œ×¤×—×•×ª ××ª ×”×¢××•×“×•×ª:
# 'Injury Severity' - ×—×•××¨×ª ×”×¤×¦×™×¢×” ×”××§×•×¨×™×ª
# 'y_prob' - ×”×”×¡×ª×‘×¨×•×ª ×©×—×–×ª×” ×”××¢×¨×›×ª ×œ×§×˜×’×•×¨×™×™×ª "×™×© ×¤×¦×™×¢×”"

# ×—×™×©×•×‘ ×××•×¦×¢, ×—×¦×™×•×Ÿ ×•×¡×˜×™×™×ª ×ª×§×Ÿ ×œ×›×œ ×§×˜×’×•×¨×™×”
summary = results_bin_cat.groupby('Injury Severity')['y_prob'].agg(
    mean='mean',
    median='median',
    std='std'
).reset_index()

print(summary)

#save the model - locally
import pickle
with open("catboost_model.pkl", "wb") as f:
    pickle.dump(model, f)

"""**× ×™×¡×™×•×Ÿ ×œ×©× ×•×ª ××ª ×”×¡×£ ×©×œ ×”××•×“×œ**"""

#× ×™×¡×™×•×Ÿ ×œ×©× ×•×ª ××ª ×”×¡×£ ×œ××¡×¤×¨×™× ××—×¨×™×

# ===============================
# 7. ×—×™×–×•×™ ×•×”×¢×¨×›×ª ×‘×™×¦×•×¢×™×
# ===============================
# ×—×™×–×•×™ ×”×¡×ª×‘×¨×•×™×•×ª ×œ××—×œ×§×” "×™×© ×¤×¦×™×¢×”"
y_probs = model.predict_proba(X_test)[:, 1]

# ×¡×£ ×”×—×œ×˜×” ×©×œ 0.3 â€“ ×××¤×©×¨ ×¨×’×™×©×•×ª ×’×‘×•×”×” ×™×•×ª×¨ ×œ××™×ª×•×¨ ×¤×¦×™×¢×•×ª
threshold = 0.3
y_pred_thresh = (y_probs >= threshold).astype(int)

# ×”×“×¤×¡×ª ×“×•"×— ×¡×™×•×•×’
print("\nğŸ“Š ×ª×•×¦××•×ª CatBoost - ×¤×¦×™×¢×” / ××™×Ÿ ×¤×¦×™×¢×” (×¡×£ = 0.3):")
print(classification_report(y_test, y_pred_thresh, digits=2))

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# ×™×¦×™×¨×ª ××˜×¨×™×¦×ª ×‘×œ×‘×•×œ
cm = confusion_matrix(y_test, y_pred_thresh)

# ×”×’×“×¨×ª ×ª×•×•×™×•×ª ×”×§×˜×’×•×¨×™×•×ª
labels = ['No Injury (0)', 'Injury (1)']

# ×¦×™×•×¨ ××˜×¨×™×¦×”
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix - Injury Prediction (Threshold = 0.3)')
plt.tight_layout()
plt.show()

"""# ×”×¨×¦×ª ××•×“×œ×™× × ×•×¡×¤×™×

---

CatBoost
---
"""

from catboost import CatBoostClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, f1_score
from imblearn.over_sampling import SMOTENC  # ×’×™×¨×¡×” ×©×œ SMOTE ×©×ª×•××›×ª ×‘×¢××•×“×•×ª ×§×˜×’×•×¨×™××œ×™×•×ª
from sklearn.metrics import confusion_matrix
from sklearn.preprocessing import LabelEncoder
from collections import Counter #×¡×•×¤×¨ ×›××” ××•×¤×¢×™× ×™×© ×‘×›×œ ×§×˜×’×•×¨×™×” ×‘×¢××•×“×ª ×”××˜×¨×” ×•××—×–×™×¨ ×›××™×œ×•×Ÿ(×¢×‘×•×¨ ×—×™×©×•×‘ ××©×§×œ×™×)
from dataclasses import dataclass
from typing import Dict, Any, List, Tuple
from catboost import CatBoostClassifier, Pool

""">**××•×“×œ 5 ×§×˜×’×¨×•×™×•×ª**"""

# ========= ×©×œ×‘ 1: × ×™×§×•×™ ×•×ª×™×•×’ =========
df = df[df['Injury Severity'].notna()].copy()
df['Injury Severity'] = df['Injury Severity'].str.lower().str.strip()

X = df.drop(columns=['Injury Severity'])
y_raw = df['Injury Severity']

# ×§×™×“×•×“ ××˜×¨×•×ª
le = LabelEncoder()
y = le.fit_transform(y_raw)

# ========= ×©×œ×‘ 2: ×¤×™×¦×•×œ =========
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, stratify=y, random_state=42
)

# ========= ×©×œ×‘ 3: ×”×’×“×¨×ª ×¢××•×“×•×ª ×§×˜×’×•×¨×™××œ×™×•×ª =========
cat_features = X.select_dtypes(include='object').columns.tolist()
for col in cat_features:
    X_train[col] = X_train[col].astype('category')
    X_test[col] = X_test[col].astype('category')

# ========= ×©×œ×‘ 4: ×—×™×©×•×‘ class_weights =========
# ××—×•×©×‘×™× ×œ×¤×™ ×©×›×™×—×•×ª ×”×¤×•×›×” â€“ ×›×›×œ ×©×§×˜×’×•×¨×™×” × ×“×™×¨×” ×™×•×ª×¨, ×”×™× ×ª×§×‘×œ ××©×§×œ ×’×‘×•×” ×™×•×ª×¨
from collections import Counter

class_counts = Counter(y_train)
total = sum(class_counts.values())
class_weights = {
    k: total / (len(class_counts) * v)
    for k, v in class_counts.items()
}
print("Class Weights:", class_weights)

# ========= ×©×œ×‘ 5: ××™××•×Ÿ CatBoost =========
model = CatBoostClassifier(
    iterations=300,
    learning_rate=0.05,
    depth=6,
    loss_function='MultiClass',
    eval_metric='TotalF1',  # ×©×™×¤×•×¨ Recall ×›×œ×œ×™
    class_weights=class_weights,
    cat_features=cat_features,
    random_state=42,
    verbose=100
)

train_pool = Pool(X_train, y_train, cat_features=cat_features)
test_pool = Pool(X_test, y_test, cat_features=cat_features)

model.fit(train_pool)

# ========= ×©×œ×‘ 6: ×—×™×–×•×™ =========
y_pred = model.predict(test_pool).flatten()
target_names = le.inverse_transform(np.unique(y))

print("\nğŸ“Š ×“×•\"×— ×¡×™×•×•×’ â€“ TEST (5 ×§×˜×’×•×¨×™×•×ª):")
print(classification_report(y_test, y_pred, target_names=target_names))

# ========= ×©×œ×‘ 7: ××˜×¨×™×¦×ª ×‘×œ×‘×•×œ =========
plt.figure(figsize=(8,6))
sns.heatmap(confusion_matrix(y_test, y_pred),
            annot=True, fmt='d', cmap='Blues',
            xticklabels=target_names, yticklabels=target_names)
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("CatBoost â€“ Confusion Matrix (5-Class Prediction)")
plt.tight_layout()
plt.show()

"""> **××•×“×œ: 3 ×§×˜×’×•×¨×™×•×ª - ××™×Ÿ ×¤×¦×™×¢×”/×¤×¦×™×¢×” ×§×œ×”/×¤×¦×™×¢×” ×—××•×¨×”**

"""

# ===============================
# 1. ××™×¤×•×™ ×œÖ¾3 ×§×˜×’×•×¨×™×•×ª
# ===============================
injury_mapping = {
    'no apparent injury': 'no injury',
    'possible injury': 'minor injury',
    'suspected minor injury': 'minor injury',
    'suspected serious injury': 'severe injury',
    'fatal injury': 'severe injury'
}

df = df[df['Injury Severity'].isin(injury_mapping.keys())].copy()
df['Injury Severity'] = df['Injury Severity'].map(injury_mapping)

# ===============================
# 2. ×”×›× ×ª × ×ª×•× ×™×
# ===============================
X = df.drop(columns=['Injury Severity'])
y = df['Injury Severity']

# ×–×™×”×•×™ ×¢××•×“×•×ª ×§×˜×’×•×¨×™××œ×™×•×ª
cat_cols = X.select_dtypes(include='object').columns
for col in cat_cols:
    X[col] = X[col].astype('category')

# ×§×™×“×•×“ y
le = LabelEncoder()
y_encoded = le.fit_transform(y)

# ×¤×™×¦×•×œ ×œ-Train/Test
X_train, X_test, y_train, y_test = train_test_split(
    X, y_encoded, test_size=0.3, random_state=42, stratify=y_encoded
)

# ===============================
# 3. ×—×™×©×•×‘ class_weights
# ===============================
counter = Counter(y_train)
total = sum(counter.values())
class_weights = [total / counter[i] for i in range(len(counter))]

print("ğŸ¯ class_weights:", class_weights)

# ===============================
# 4. ××™××•×Ÿ CatBoost ×¢× ××©×§×œ×™×
# ===============================
cat_features_indices = [X.columns.get_loc(col) for col in cat_cols]

model = CatBoostClassifier(
    iterations=500,
    learning_rate=0.05,
    depth=6,
    loss_function='MultiClass',
    class_weights=class_weights,
    random_seed=42,
    verbose=0
)

model.fit(X_train, y_train, cat_features=cat_features_indices)

# ===============================
# 5. ×—×™×–×•×™ ×•×”×¢×¨×›×ª ×‘×™×¦×•×¢×™×
# ===============================
y_pred = model.predict(X_test).flatten()

print("\nğŸ“Š ×ª×•×¦××•×ª CatBoost (×¢× class_weights):")
print(classification_report(y_test, y_pred, target_names=le.classes_))

macro_f1 = f1_score(y_test, y_pred, average='macro')
print(f"\nğŸ¯ Macro F1: {macro_f1:.4f}")

"""> **××•×“×œ ×“×•-×©×œ×‘×™**

×©×œ×‘ 1: ×™×© ×¤×¦×™×¢×”/××™×Ÿ ×¤×¦×™×¢×”
×©×œ×‘ 2: ×¤×¦×™×¢×” ×§×˜×œ× ×™×ª/×§×œ×”/×§×©×”

"""

# ---------------- ×”×’×“×¨×•×ª ----------------

@dataclass
class CatConfig:
    random_state: int = 42
    test_size: float = 0.30
    threshold: float = 0.25
    use_smotenc: bool = True
    cat_params_stage1: Dict[str, Any] = None
    cat_params_stage2: Dict[str, Any] = None
    smotenc_params: Dict[str, Any] = None

    def __post_init__(self):
        if self.cat_params_stage1 is None:
            self.cat_params_stage1 = dict(
                iterations=500,
                learning_rate=0.05,
                depth=6,
                loss_function='Logloss',  # ğŸ”¹ Binary classification
                random_seed=self.random_state,
                verbose=0
            )
        if self.cat_params_stage2 is None:
            self.cat_params_stage2 = dict(
                iterations=500,
                learning_rate=0.05,
                depth=6,
                loss_function='MultiClass',  # ğŸ”¹ Multiclass classification
                random_seed=self.random_state,
                verbose=0
            )
        if self.smotenc_params is None:
            self.smotenc_params = dict(random_state=self.random_state)


# ---------------- ×¤×•× ×§×¦×™×•×ª ×¢×–×¨ ----------------

def print_cm(y_true, y_pred, title):
    cm = confusion_matrix(y_true, y_pred)
    print(f"\nğŸ“Š {title}")
    print(classification_report(y_true, y_pred, digits=2))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.xlabel("Predicted"); plt.ylabel("True"); plt.title(title)
    plt.show()

def split_xy(X, y, cfg):
    return train_test_split(X, y, test_size=cfg.test_size,
                            random_state=cfg.random_state, stratify=y)

def apply_smotenc(X_train, y_train, cat_cols, cfg):
    for c in cat_cols:
        X_train[c] = X_train[c].astype('category')
    cat_indices = [X_train.columns.get_loc(c) for c in cat_cols]
    smote = SMOTENC(categorical_features=cat_indices, **cfg.smotenc_params)
    X_res, y_res = smote.fit_resample(X_train, y_train)
    return pd.DataFrame(X_res, columns=X_train.columns), y_res


# ---------------- ×©×œ×‘ 1: ×™×© ×¤×¦×™×¢×” / ××™×Ÿ ×¤×¦×™×¢×” ----------------

def preprocess_stage1(df):
    df = df[df['Injury Severity'].notna()].copy()
    df['Injury Binary'] = df['Injury Severity'].str.lower().str.strip().apply(
        lambda x: 0 if x == 'no apparent injury' else 1
    )
    return df

def build_xy_binary(df):
    X = df.drop(columns=['Injury Severity', 'Injury Binary'])
    y = df['Injury Binary']
    cat_cols = X.select_dtypes(include='object').columns.tolist()
    for c in cat_cols:
        X[c] = X[c].astype('category')
    return X, y, cat_cols

def train_stage1(df, cfg):
    df = preprocess_stage1(df)
    X, y, cat_cols = build_xy_binary(df)
    X_train, X_test, y_train, y_test = split_xy(X, y, cfg)

    if cfg.use_smotenc:
        X_train, y_train = apply_smotenc(X_train.copy(), y_train.copy(), cat_cols, cfg)

    model = CatBoostClassifier(**cfg.cat_params_stage1)
    model.fit(Pool(X_train, y_train, cat_features=cat_cols))

    y_prob = model.predict_proba(Pool(X_test, cat_features=cat_cols))[:, 1]
    y_pred = (y_prob >= cfg.threshold).astype(int)

    print_cm(y_test, y_pred, "Stage 1: Injury / No Injury")

    return model, cat_cols


# ---------------- ×©×œ×‘ 2: ×§×œ×” / ×—××•×¨×” / ×§×˜×œ× ×™×ª ----------------

injury_mapping = {
    'possible injury': 'light injury',
    'suspected minor injury': 'light injury',
    'suspected serious injury': 'serious injury',
    'fatal injury': 'fatal injury',
    'no apparent injury': None
}

def prepare_stage2_data(df):
    df = df[df['Injury Severity'].notna()].copy()
    df['Injury Mapped'] = df['Injury Severity'].str.lower().str.strip().map(injury_mapping)
    df = df[df['Injury Mapped'].notna()].copy()

    X = df.drop(columns=['Injury Severity', 'Injury Binary', 'Injury Mapped'], errors='ignore')
    y = df['Injury Mapped']
    cat_cols = X.select_dtypes(include='object').columns.tolist()
    for c in cat_cols:
        X[c] = X[c].astype('category')

    le = LabelEncoder()
    y_enc = le.fit_transform(y)

    return X, y_enc, cat_cols, le

def train_stage2(df, cfg):
    X, y, cat_cols, le = prepare_stage2_data(df)
    X_train, X_test, y_train, y_test = split_xy(X, y, cfg)

    if cfg.use_smotenc:
        X_train, y_train = apply_smotenc(X_train.copy(), pd.Series(y_train), cat_cols, cfg)

    model = CatBoostClassifier(**cfg.cat_params_stage2)
    model.fit(Pool(X_train, y_train, cat_features=cat_cols))

    y_pred = model.predict(Pool(X_test, cat_features=cat_cols)).astype(int)
    y_test_lbl = le.inverse_transform(y_test)
    y_pred_lbl = le.inverse_transform(y_pred)

    print_cm(y_test_lbl, y_pred_lbl, "Stage 2: Injury Type (Light / Serious / Fatal)")

    return model, le


# ---------------- Pipeline ××œ× ----------------

def run_catboost_2stage_pipeline(df, cfg):
    print("ğŸ”¹ Stage 1: Injury / No Injury")
    model1, cat_cols1 = train_stage1(df.copy(), cfg)

    print("\nğŸ”¹ Stage 2: Injury Type (Only on Injured cases)")
    df2 = df[df['Injury Severity'].str.lower().str.strip() != 'no apparent injury'].copy()
    model2, label_encoder = train_stage2(df2, cfg)

    return model1, model2, label_encoder



cfg = CatConfig()
model1, model2, le = run_catboost_2stage_pipeline(df, cfg)

"""LightGBM
---


"""

import lightgbm as lgb
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix
from imblearn.over_sampling import SMOTE
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.utils.class_weight import compute_class_weight
from sklearn.model_selection import GridSearchCV

from dataclasses import dataclass
from typing import Dict, Any, Optional, Tuple, List
from sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve
from imblearn.over_sampling import SMOTE

""">**××•×“×œ ×™×© ×¤×¦×™×¢×”/××™×Ÿ ×¤×¦×™×¢×”**

"""

# âœ¨ ×™×¦×™×¨×ª ×¢××•×“×ª ×¤×¦×™×¢×” / ××™×Ÿ ×¤×¦×™×¢×”
df = df.dropna(subset=['Injury Severity'])
df['Injury Binary'] = df['Injury Severity'].apply(lambda x: 0 if x == 'no apparent injury' else 1)

# ğŸ”¹ ×©×œ×‘ ×¨××©×•×Ÿ â€“ ×¤×¦×™×¢×” / ××™×Ÿ ×¤×¦×™×¢×”
X = df.drop(['Injury Severity', 'Injury Binary'], axis=1)
y = df['Injury Binary']
X = pd.get_dummies(X, drop_first=True)
X.columns = X.columns.str.replace('[^A-Za-z0-9_]+', '_', regex=True)
X = X.drop(columns=X.select_dtypes(include=['datetime64']).columns)
X = X.astype(float)

X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)
X_train1, y_train1 = SMOTE(random_state=42).fit_resample(X_train1, y_train1)

model1 = lgb.LGBMClassifier(n_estimators=500, max_depth=10, learning_rate=0.03, class_weight='balanced', random_state=42)
model1.fit(X_train1, y_train1)

probs1 = model1.predict_proba(X_test1)[:,1]
thresh1 = 0.2473  # ×”×•×¨×“× ×• ××ª ×”×¡×£ ×›×“×™ ×œ×©×¤×¨ Recall ×œ×¤×¦×™×¢×•×ª
y_pred1 = (probs1 >= thresh1).astype(int)
# 0.5
print("\n×—×™×–×•×™  â€“ ×¤×¦×™×¢×” / ××™×Ÿ ×¤×¦×™×¢×”")
print(classification_report(y_test1, y_pred1))
sns.heatmap(confusion_matrix(y_test1, y_pred1), annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix - Stage 1")
plt.show()

"""× ×™×¡×™×•×Ÿ ×œ××¦×™××ª ×”×¡×£ ×”××•×¤×˜×™××œ×™(×œ× ×”×¦×œ×™×—)"""

# from sklearn.metrics import precision_recall_curve

# # ×—×™×©×•×‘ Precision ×•-Recall ×¢×‘×•×¨ ×›×œ ×¡×£ ××¤×©×¨×™
# precision, recall, thresholds = precision_recall_curve(y_test1, probs1)

# # ×—×™×©×•×‘ ×”-F1 ×œ×›×œ ×¡×£ ×›×“×™ ×œ××¦×•× ××ª ×”××•×¤×˜×™××œ×™ (×× ×¨×•×¦×™×)
# f1_scores = 2 * (precision * recall) / (precision + recall)
# best_idx = np.argmax(f1_scores)
# best_threshold = thresholds[best_idx]
# best_f1 = f1_scores[best_idx]

# # ×¦×™×•×¨ ×”×’×¨×£
# plt.figure(figsize=(8,6))
# plt.plot(thresholds, precision[:-1], label="Precision", color='royalblue')
# plt.plot(thresholds, recall[:-1], label="Recall", color='lightseagreen')
# plt.plot(thresholds, f1_scores[:-1], label="F1 Score", color='gray', linestyle='--')

# # # ×¡×™××•×Ÿ ×”× ×§×•×“×” ×©×œ threshold=0.3
# # plt.axvline(x=0.3, color='red', linestyle='--')

# # ×¡×™××•×Ÿ ×”× ×§×•×“×” ×”×›×™ ×˜×•×‘×” ×œ×¤×™ F1
# plt.axvline(x=best_threshold, color='green', linestyle='--', label=f'Best F1 (th={best_threshold:.2f})')

# plt.xlabel("Threshold")
# plt.ylabel("Score")
# plt.title("Precision / Recall / F1 by Threshold")
# plt.legend()
# plt.grid()
# plt.show()

# #× ×™×¡×™×•×Ÿ ×œ×©× ×•×ª ×›×œ ×¤×¢× ××ª ×”×¡×£ thresh1 ×œ××¡×¤×¨ ××—×¨
# #× ×™×¡×™× ×• 0.5,0.38

# probs1 = model1.predict_proba(X_test1)[:,1]
# thresh1 = 0.38  # ×©×™× ×™× ×• ××ª ×”×¡×£ ×›×“×™ ×œ×©×¤×¨ Recall ×œ×¤×¦×™×¢×•×ª
# y_pred1 = (probs1 >= thresh1).astype(int)

# print("\n×©×œ×‘ 1 â€“ ×¤×¦×™×¢×” / ××™×Ÿ ×¤×¦×™×¢×”")
# print(classification_report(y_test1, y_pred1))
# sns.heatmap(confusion_matrix(y_test1, y_pred1), annot=True, fmt='d', cmap='Blues')
# plt.title("Confusion Matrix - Stage 1")
# plt.show()

""">**××•×“×œ 5 ×§×˜×’×¨×•×™×•×ª**"""

# âœ… ×©×œ×‘ 1: × ×™×§×•×™ ×¢××•×“×ª ×”××˜×¨×”
df = df.dropna(subset=['Injury Severity'])
df['Injury Severity'] = df['Injury Severity'].str.lower().str.strip()

# âœ… ×©×œ×‘ 2: ×§×™×“×•×“ ×”××˜×¨×” ×œÖ¾Label
le = LabelEncoder()
df['Injury_Label'] = le.fit_transform(df['Injury Severity'])  # × ×©××¨ ×”×¡×“×¨ ×”××§×•×¨×™ ×©×œ 5 ×”×§×˜×’×•×¨×™×•×ª

# âœ… ×©×œ×‘ 3: ×”×›× ×ª ×¤×™×¦'×¨×™×
X = df.drop(columns=['Injury Severity', 'Injury_Label'])
y = df['Injury_Label']

# ×§×™×“×•×“ ×§×˜×’×•×¨×™×•×ª
X = pd.get_dummies(X, drop_first=True)
X.columns = X.columns.str.replace('[^A-Za-z0-9_]+', '_', regex=True)
X = X.drop(columns=X.select_dtypes(include=['datetime64']).columns)
X = X.astype(float)

# âœ… ×—×œ×•×§×” ×œÖ¾train/test + SMOTE ××•×ª×× ×œ×¨×™×‘×•×™ ×§×˜×’×•×¨×™×•×ª
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.3, random_state=42)
smote = SMOTE(random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

# âœ… ×©×œ×‘ 4: ××™××•×Ÿ ××•×“×œ LightGBM ×œ×¨×™×‘×•×™ ×§×˜×’×•×¨×™×•×ª
model = lgb.LGBMClassifier(
    objective='multiclass',
    num_class=5,
    n_estimators=300,
    max_depth=6,
    learning_rate=0.05,
    class_weight='balanced',
    random_state=42
)

model.fit(X_train_smote, y_train_smote)

# âœ… ×©×œ×‘ 5: ×ª×—×–×™×•×ª ×•×”×¢×¨×›×ª ×‘×™×¦×•×¢×™×
y_pred = model.predict(X_test)

# ×©×—×–×•×¨ ×©××•×ª ×”×§×˜×’×•×¨×™×•×ª ×œ×“×•"×—
label_names = le.classes_
print("ğŸ” ×‘×™×¦×•×¢×™× ×¢×œ ×¡×˜ ×”×‘×“×™×§×” â€“ ×—×™×–×•×™ ×—××© ×§×˜×’×•×¨×™×•×ª:")
print(classification_report(y_test, y_pred, target_names=label_names))

# âœ… ××˜×¨×™×¦×ª ×‘×œ×‘×•×œ
plt.figure(figsize=(8, 6))
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues',
            xticklabels=label_names, yticklabels=label_names)
plt.title("Confusion Matrix â€“ Multiclass Injury Severity")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.tight_layout()
plt.show()

"""> **××•×“×œ 3 ×§×˜×’×•×¨×™×•×ª**"""

#×¢× ×¡×£ ×œ×¢×¨×š severe injury

# âœ¨ ×§×™×‘×•×¥ ×œ-3 ×§×˜×’×•×¨×™×•×ª
def map_injury(severity):
    severity = severity.strip().lower()
    if severity == 'no apparent injury':
        return 'no injury'
    elif severity in ['possible injury', 'suspected minor injury']:
        return 'minor injury'
    else:
        return 'severe injury'

df['Injury Group'] = df['Injury Severity'].apply(map_injury)

# ğŸ§ª ×§×œ×˜ ×•×¤×œ×˜
X = df.drop(columns=['Injury Severity', 'Injury Group'])
y = df['Injury Group']

# ×”××¨×ª ××©×ª× ×™× ×§×˜×’×•×¨×™××œ×™×™×
X = pd.get_dummies(X, drop_first=True)
X.columns = X.columns.str.replace('[^A-Za-z0-9_]+', '_', regex=True)
X = X.drop(columns=X.select_dtypes(include=['datetime64']).columns)
X = X.astype(float)

# ×¤×™×¦×•×œ
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.3, random_state=42)

# SMOTE ×¨×§ ×× ×™×© ×—×•×¡×¨ ××™×–×•×Ÿ ×’×“×•×œ
X_train, y_train = SMOTE(random_state=42).fit_resample(X_train, y_train)

# ××©×§×œ×™× ×œ×§×˜×’×•×¨×™×•×ª
weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)
class_weight_dict = dict(zip(np.unique(y_train), weights))

# ××•×“×œ LightGBM
model = lgb.LGBMClassifier(
    n_estimators=700,
    max_depth=12,
    learning_rate=0.02,
    num_leaves=40,
    class_weight=class_weight_dict,
    random_state=42
)
model.fit(X_train, y_train)

# ğŸ”¹ ×©×™××•×© ×‘-threshold ×¨×§ ×œ×§×˜×’×•×¨×™×” severe injury
threshold = 0.3  # ××¤×©×¨ ×œ×©×—×§ ×¢× ×”×¢×¨×š (0.25, 0.35 ×•×›×•')

probs = model.predict_proba(X_test)
severe_index = list(model.classes_).index('severe injury')
probs_severe = probs[:, severe_index]

# ×ª×—×–×™×ª ×¨×’×™×œ×” ×©×œ ×”××•×“×œ
y_pred_default = model.predict(X_test)

# ×× ×”×”×¡×ª×‘×¨×•×ª ×œ-severe injury >= threshold â†’ × × ×‘× severe injury
y_pred = np.where(probs_severe >= threshold, 'severe injury', y_pred_default)

# ğŸ“ˆ ×ª×•×¦××•×ª ×¢× threshold ××•×ª×× ×œ-severe
print(f"\nğŸ“ˆ ×“×•\"×— ×¡×™×•×•×’ â€“ ×¢× threshold={threshold} ×œ-severe injury:")
print(classification_report(y_test, y_pred))

plt.figure(figsize=(8,6))
sns.heatmap(confusion_matrix(y_test, y_pred, labels=model.classes_),
            annot=True, fmt='d', cmap='Blues',
            xticklabels=model.classes_,
            yticklabels=model.classes_)
plt.title(f"Confusion Matrix â€“ threshold={threshold} for severe injury")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.show()

"""
> **××•×“×œ ×“×• ×©×œ×‘×™**


*   ×©×œ×‘ 1: ×¤×¦×™×¢×”/××™×Ÿ ×¤×¦×™×¢×”
*   ×©×œ×‘ 2: ×¤×¦×™×¢×” ×§×œ×”/×§×©×”-×§×˜×œ× ×™×ª
"""

# âœ¨ ×™×¦×™×¨×ª ×¢××•×“×ª ×¤×¦×™×¢×” / ××™×Ÿ ×¤×¦×™×¢×”
df = df.dropna(subset=['Injury Severity'])
df['Injury Binary'] = df['Injury Severity'].apply(lambda x: 0 if x == 'no apparent injury' else 1)

# ğŸ”¹ ×©×œ×‘ ×¨××©×•×Ÿ â€“ ×¤×¦×™×¢×” / ××™×Ÿ ×¤×¦×™×¢×”
X = df.drop(['Injury Severity', 'Injury Binary'], axis=1)
y = df['Injury Binary']
X = pd.get_dummies(X, drop_first=True)
X.columns = X.columns.str.replace('[^A-Za-z0-9_]+', '_', regex=True)
X = X.drop(columns=X.select_dtypes(include=['datetime64']).columns)
X = X.astype(float)

X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)
X_train1, y_train1 = SMOTE(random_state=42).fit_resample(X_train1, y_train1)

model1 = lgb.LGBMClassifier(n_estimators=500, max_depth=10, learning_rate=0.03, class_weight='balanced', random_state=42)
model1.fit(X_train1, y_train1)

probs1 = model1.predict_proba(X_test1)[:,1]
thresh1 = 0.3  # ×”×•×¨×“× ×• ××ª ×”×¡×£ ×›×“×™ ×œ×©×¤×¨ Recall ×œ×¤×¦×™×¢×•×ª
y_pred1 = (probs1 >= thresh1).astype(int)

print("\n×©×œ×‘ 1 â€“ ×¤×¦×™×¢×” / ××™×Ÿ ×¤×¦×™×¢×”")
print(classification_report(y_test1, y_pred1))
sns.heatmap(confusion_matrix(y_test1, y_pred1), annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix - Stage 1")
plt.show()

# ğŸ”¸ ×©×œ×‘ ×©× ×™ â€“ ×§×œ×” (0) ××•×œ ×§×©×”/×§×˜×œ× ×™×ª (1)
df_stage2 = df[df['Injury Binary'] == 1].copy()

# ×™×¦×™×¨×ª ×¢××•×“×ª ×™×¢×“ ×—×“×©×”
df_stage2['Severe Binary'] = df_stage2['Injury Severity'].apply(
    lambda x: 1 if x in ['fatal injury', 'suspected serious injury'] else 0
)

# ×”×’×“×¨×ª X ×•-y
X2 = df_stage2.drop(['Injury Severity', 'Injury Binary', 'Severe Binary'], axis=1)
y2 = df_stage2['Severe Binary']

# ×”×›× ×” â€“ ×‘×“×™×•×§ ×›××• ×‘×©×œ×‘ ×”×¨××©×•×Ÿ
X2 = pd.get_dummies(X2, drop_first=True)
X2.columns = X2.columns.str.replace('[^A-Za-z0-9_]+', '_', regex=True)
X2 = X2.drop(columns=X2.select_dtypes(include=['datetime64']).columns)
X2 = X2.astype(float)

# ×¤×™×¦×•×œ + SMOTE
X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, test_size=0.3, stratify=y2, random_state=42)

# ×—×™×–×•×§ ×§×˜×’×•×¨×™×™×ª ×”××™×¢×•×˜ (×¤×¦×™×¢×” ×§×©×”)
strategy = {1: y_train2.value_counts()[1]*3}  # ××¤×©×¨ ×œ×›×•×•× ×Ÿ ××ª ×”××›×¤×œ×”
X_train2, y_train2 = SMOTE(sampling_strategy=strategy, random_state=42).fit_resample(X_train2, y_train2)

# ××•×“×œ
model2 = lgb.LGBMClassifier(n_estimators=500, max_depth=10, learning_rate=0.03,
                            class_weight='balanced', random_state=42)
model2.fit(X_train2, y_train2)

# ×—×™×–×•×™
probs2 = model2.predict_proba(X_test2)[:,1]
thresh2 = 0.3
y_pred2 = (probs2 >= thresh2).astype(int)

# ×ª×¦×•×’×ª ×‘×™×¦×•×¢×™×
print("\n×©×œ×‘ 2 â€“ ×¤×¦×™×¢×” ×§×œ×” (0) ××•×œ ×§×©×”/×§×˜×œ× ×™×ª (1)")
print(classification_report(y_test2, y_pred2))
sns.heatmap(confusion_matrix(y_test2, y_pred2), annot=True, fmt='d', cmap='Oranges')
plt.title("Confusion Matrix - Stage 2: Minor vs Severe")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.show()

"""

> **××•×“×œ ×“×• ×©×œ×‘×™**

*   ×©×œ×‘ 1: ×™×© ×¤×¦×™×¢×”/××™×Ÿ ×¤×¦×™×¢×”
*   ×©×œ×‘ ×©× ×™: 3 ×§×˜×’×•×¨×™×•×ª- ×¤×¦×™×¢×” ×§×œ×”/×§×©×”/×§×˜×œ× ×™×ª



---

"""

# ğŸ§  ×”×•×¨×“×ª ×©×•×¨×•×ª ×œ×œ× ×¢×¨×š ×‘×¢××•×“×ª ×”××˜×¨×”
df = df.dropna(subset=['Injury Severity'])
df['Injury Severity'] = df['Injury Severity'].str.lower().str.strip()

# âœ¨ ×™×¦×™×¨×ª ×¢××•×“×ª ×¤×¦×™×¢×” / ××™×Ÿ ×¤×¦×™×¢×”
df['Injury Binary'] = df['Injury Severity'].apply(lambda x: 0 if x == 'no apparent injury' else 1)

# ğŸŸ¡ ×”××¨×” ×œ×§×˜×’×•×¨×™×•×ª
for col in df.select_dtypes(include='object').columns:
    df[col] = df[col].astype('category')

# ğŸ”¹ ×©×œ×‘ ×¨××©×•×Ÿ â€“ ×—×™×–×•×™ ×¤×¦×™×¢×” / ××™×Ÿ ×¤×¦×™×¢×”
X1 = df.drop(columns=['Injury Severity', 'Injury Binary'])
y1 = df['Injury Binary']

X_train1, X_test1, y_train1, y_test1 = train_test_split(X1, y1, test_size=0.3, stratify=y1, random_state=42)

lgb_model1 = lgb.LGBMClassifier(
    n_estimators=300, max_depth=7, learning_rate=0.05,
    class_weight='balanced', random_state=42
)
lgb_model1.fit(X_train1, y_train1)

y_probs1 = lgb_model1.predict_proba(X_test1)[:, 1]
threshold = 0.3
y_pred1 = (y_probs1 >= threshold).astype(int)

print("\nğŸ“ˆ ×“×•\"×— ×¡×™×•×•×’ - ×©×œ×‘ ×¨××©×•×Ÿ (×¤×¦×™×¢×” / ××™×Ÿ ×¤×¦×™×¢×”):")
print(classification_report(y_test1, y_pred1))

plt.figure(figsize=(8,6))
sns.heatmap(confusion_matrix(y_test1, y_pred1), annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix - Stage 1 (Injury/No Injury)')
plt.show()

# ğŸ”¸ ×©×œ×‘ ×©× ×™ â€“ ×—×™×–×•×™ ×¡×•×’ ×”×¤×¦×™×¢×”
injury_mapping = {
    'possible injury': 'light injury',
    'suspected minor injury': 'light injury',
    'suspected serious injury': 'serious injury',
    'fatal injury': 'fatal injury'
}
df_stage2 = df[df['Injury Severity'].isin(injury_mapping.keys())].copy()
df_stage2['Injury Mapped'] = df_stage2['Injury Severity'].map(injury_mapping)

X2 = df_stage2.drop(columns=['Injury Severity', 'Injury Binary', 'Injury Mapped'])
y2 = df_stage2['Injury Mapped'].astype('category')

X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, test_size=0.3, stratify=y2, random_state=42)

# âš–ï¸ ×—×™×©×•×‘ ××©×§×œ×™× ×œ×§×˜×’×•×¨×™×•×ª
class_weights = compute_class_weight(
    class_weight='balanced',
    classes=np.unique(y_train2),
    y=y_train2
)
class_weight_dict = dict(zip(np.unique(y_train2), class_weights))

# ğŸš€ ××•×“×œ ×©×œ×‘ ×©× ×™
lgb_model2 = lgb.LGBMClassifier(
    n_estimators=300, max_depth=7, learning_rate=0.05,
    class_weight=class_weight_dict, random_state=42
)
lgb_model2.fit(X_train2, y_train2)

y_pred2 = lgb_model2.predict(X_test2)

print("\nğŸ“ˆ ×“×•\"×— ×¡×™×•×•×’ - ×©×œ×‘ ×©× ×™ (3 ×§×˜×’×•×¨×™×•×ª):")
print(classification_report(y_test2, y_pred2))

plt.figure(figsize=(8,6))
sns.heatmap(confusion_matrix(y_test2, y_pred2, labels=lgb_model2.classes_),
            annot=True, fmt='d', cmap='Blues',
            xticklabels=lgb_model2.classes_, yticklabels=lgb_model2.classes_)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title("Confusion Matrix - Stage 2 (Injury Type)")
plt.show()

"""
> **××•×“×œ ×ª×œ×ª ×©×œ×‘×™**


*   ××•×“×œ 1: ×™×© ×¤×¦×™×¢×”/××™×Ÿ ×¤×¦×™×¢×”
*   ××•×“×œ ×©× ×™: ×¤×¦×™×¢×” ×§×˜×œ× ×™×ª/×œ× ×§×˜×œ× ×™×ª
*   ××•×“×œ 3: ×¤×¦×™×¢×” ×—××•×¨×”/ ×§×œ×”

---

"""

# âœ… ×©×™×¤×•×¨ ×›×•×œ×œ ×œ××•×“×œ LightGBM ×‘×©×œ×•×©×” ×©×œ×‘×™×: ×¤×¦×™×¢×” / ×§×˜×œ× ×™×ª / ×¨××ª ×—×•××¨×”

# âœ¨ ×™×¦×™×¨×ª ×¢××•×“×ª ×¤×¦×™×¢×” / ××™×Ÿ ×¤×¦×™×¢×”
df = df.dropna(subset=['Injury Severity'])
df['Injury Binary'] = df['Injury Severity'].apply(lambda x: 0 if x == 'no apparent injury' else 1)

# ğŸ”¹ ×©×œ×‘ ×¨××©×•×Ÿ â€“ ×¤×¦×™×¢×” / ××™×Ÿ ×¤×¦×™×¢×”
X = df.drop(['Injury Severity', 'Injury Binary'], axis=1)
y = df['Injury Binary']
X = pd.get_dummies(X, drop_first=True)
X.columns = X.columns.str.replace('[^A-Za-z0-9_]+', '_', regex=True)
X = X.drop(columns=X.select_dtypes(include=['datetime64']).columns)
X = X.astype(float)

X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)
X_train1, y_train1 = SMOTE(random_state=42).fit_resample(X_train1, y_train1)

model1 = lgb.LGBMClassifier(n_estimators=500, max_depth=10, learning_rate=0.03, class_weight='balanced', random_state=42)
model1.fit(X_train1, y_train1)

probs1 = model1.predict_proba(X_test1)[:,1]
thresh1 = 0.3  # ×”×•×¨×“× ×• ××ª ×”×¡×£ ×›×“×™ ×œ×©×¤×¨ Recall ×œ×¤×¦×™×¢×•×ª
y_pred1 = (probs1 >= thresh1).astype(int)
# 0.5
print("\n×©×œ×‘ 1 â€“ ×¤×¦×™×¢×” / ××™×Ÿ ×¤×¦×™×¢×”")
print(classification_report(y_test1, y_pred1))
sns.heatmap(confusion_matrix(y_test1, y_pred1), annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix - Stage 1")
plt.show()

# ğŸ”¸ ×©×œ×‘ ×©× ×™ â€“ ×¤×¦×™×¢×” ×§×˜×œ× ×™×ª / ×œ× ×§×˜×œ× ×™×ª
df_stage2 = df[df['Injury Binary'] == 1].copy()
df_stage2['Fatal Binary'] = df_stage2['Injury Severity'].apply(lambda x: 1 if x == 'fatal injury' else 0)

X2 = df_stage2.drop(['Injury Severity', 'Injury Binary', 'Fatal Binary'], axis=1)
y2 = df_stage2['Fatal Binary']
X2 = pd.get_dummies(X2, drop_first=True)
X2.columns = X2.columns.str.replace('[^A-Za-z0-9_]+', '_', regex=True)
X2 = X2.drop(columns=X2.select_dtypes(include=['datetime64']).columns)
X2 = X2.astype(float)

X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, test_size=0.3, random_state=42, stratify=y2)
X_train2, y_train2 = SMOTE(sampling_strategy={1: y_train2.value_counts()[1]*4}, random_state=42).fit_resample(X_train2, y_train2)

model2 = lgb.LGBMClassifier(n_estimators=500, max_depth=10, learning_rate=0.03, class_weight='balanced', random_state=42)
model2.fit(X_train2, y_train2)

probs2 = model2.predict_proba(X_test2)[:,1]
thresh2 = 0.3
y_pred2 = (probs2 >= thresh2).astype(int)

print("\n×©×œ×‘ 2 â€“ ×¤×¦×™×¢×” ×§×˜×œ× ×™×ª / ×œ× ×§×˜×œ× ×™×ª")
print(classification_report(y_test2, y_pred2))
sns.heatmap(confusion_matrix(y_test2, y_pred2), annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix - Stage 2")
plt.show()

# ğŸ”» ×©×œ×‘ ×©×œ×™×©×™ â€“ ×§×œ×” / ×—××•×¨×”
stage3_df = df_stage2[df_stage2['Injury Severity'].isin(['possible injury', 'suspected minor injury', 'suspected serious injury'])].copy()
stage3_df['Severity'] = stage3_df['Injury Severity'].apply(lambda x: 'serious' if x == 'suspected serious injury' else 'light')

X3 = stage3_df.drop(['Injury Severity', 'Injury Binary', 'Fatal Binary', 'Severity'], axis=1)
y3 = stage3_df['Severity']
le3 = LabelEncoder()
y3_encoded = le3.fit_transform(y3)

X3 = pd.get_dummies(X3, drop_first=True)
X3.columns = X3.columns.str.replace('[^A-Za-z0-9_]+', '_', regex=True)
X3 = X3.drop(columns=X3.select_dtypes(include=['datetime64']).columns)
X3 = X3.astype(float)

X_train3, X_test3, y_train3, y_test3 = train_test_split(X3, y3_encoded, test_size=0.3, random_state=42, stratify=y3_encoded)
X_train3, y_train3 = SMOTE(sampling_strategy={1: y_train3.tolist().count(1)*3}, random_state=42).fit_resample(X_train3, y_train3)

model3 = lgb.LGBMClassifier(n_estimators=500, max_depth=10, learning_rate=0.03, class_weight='balanced', random_state=42)
model3.fit(X_train3, y_train3)
y_pred3 = model3.predict(X_test3)

print("\n×©×œ×‘ 3 â€“ ×—×•××¨×ª ×¤×¦×™×¢×”: ×§×œ×” / ×—××•×¨×”")
print(classification_report(le3.inverse_transform(y_test3), le3.inverse_transform(y_pred3)))
sns.heatmap(confusion_matrix(le3.inverse_transform(y_test3), le3.inverse_transform(y_pred3)), annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix - Stage 3")
plt.show()

"""
> **(××•×“×•×œ×¨×™)××•×“×œ ×ª×œ×ª ×©×œ×‘×™**


*   ××•×“×œ 1: ×™×© ×¤×¦×™×¢×”/××™×Ÿ ×¤×¦×™×¢×”
*   ××•×“×œ ×©× ×™: ×¤×¦×™×¢×” ×§×˜×œ× ×™×ª/×œ× ×§×˜×œ× ×™×ª
*   ××•×“×œ 3: ×¤×¦×™×¢×” ×—××•×¨×”/ ×§×œ×”

---

"""

# # ×¤×™×™×¤×œ×™×™×Ÿ ××•×“×•×œ×¨×™ ×©×œ LightGBM (3 ×©×œ×‘×™×) + ×“×•×—×•×ª ××™××•×Ÿ/×‘×“×™×§×”Â +Â ×”×“×¤×¡×ªÂ ×¡×£
# # ---------------- Config ----------------

## @dataclass
# class LGBConfig:
#     random_state: int = 42
#     test_size: float = 0.30
#     use_smote: bool = True

#     stage1_threshold_strategy: str = "f2"
#     stage1_fixed_threshold: float = 0.30
#     stage1_recall_floor: float = 0.95

#     lgb_stage1: Dict[str, Any] = None
#     lgb_stage2: Dict[str, Any] = None
#     lgb_stage3: Dict[str, Any] = None

#     smote_stage1: Dict[str, Any] = None
#     smote_stage2: Dict[str, Any] = None
#     smote_stage3: Dict[str, Any] = None

#     def __post_init__(self):
#         if self.lgb_stage1 is None:
#             self.lgb_stage1 = dict(n_estimators=500, max_depth=10, learning_rate=0.03,
#                                    class_weight='balanced', random_state=self.random_state)
#         if self.lgb_stage2 is None:
#             self.lgb_stage2 = dict(n_estimators=500, max_depth=10, learning_rate=0.03,
#                                    class_weight='balanced', random_state=self.random_state)
#         if self.lgb_stage3 is None:
#             self.lgb_stage3 = dict(n_estimators=500, max_depth=10, learning_rate=0.03,
#                                    class_weight='balanced', random_state=self.random_state)

#         if self.smote_stage1 is None:
#             self.smote_stage1 = dict(random_state=self.random_state)
#         if self.smote_stage2 is None:
#             self.smote_stage2 = dict(random_state=self.random_state)
#         if self.smote_stage3 is None:
#             self.smote_stage3 = dict(random_state=self.random_state)


# # ---------------- Utils ----------------

# def sanitize_features(X: pd.DataFrame) -> pd.DataFrame:
#     X = X.copy()
#     dt_cols = X.select_dtypes(include=['datetime64[ns]', 'datetime64[ns, UTC]']).columns
#     X.drop(columns=dt_cols, inplace=True)
#     for c in X.columns:
#         if X[c].dtype == 'O':
#             X[c] = X[c].astype(str)
#     X.columns = X.columns.str.replace('[^A-Za-z0-9_]+', '_', regex=True)
#     return X

# def one_hot_all(X: pd.DataFrame) -> pd.DataFrame:
#     X = sanitize_features(X)
#     X = pd.get_dummies(X, drop_first=True)
#     X.columns = X.columns.str.replace('[^A-Za-z0-9_]+', '_', regex=True)
#     X = X.astype(float)
#     return X

# def check_bad_feature_names(X: pd.DataFrame):
#     bad_cols = [c for c in X.columns if any(ch in c for ch in ['"', "'", "{", "}", "[", "]", ":", ",", " ", "/", "\\", "#", "?", "&", "="])]
#     if bad_cols:
#         print("âš  × ××¦××• ×¢××•×“×•×ª ×‘×¢×™×™×ª×™×•×ª:", bad_cols)
#     else:
#         print("âœ… ××™×Ÿ ×¢××•×“×•×ª ×‘×¢×™×™×ª×™×•×ª")

# def split_xy(X: pd.DataFrame, y: np.ndarray, cfg: LGBConfig):
#     return train_test_split(X, y, test_size=cfg.test_size, random_state=cfg.random_state, stratify=y)

# def print_report(title: str, y_true, y_pred):
#     print(f"\n=== {title} ===")
#     print(classification_report(y_true, y_pred, digits=2))
#     print("Confusion Matrix:")
#     print(confusion_matrix(y_true, y_pred))

# def plot_confusion_matrix(y_true, y_pred, title="Confusion Matrix", normalize=False):
#     labels = np.unique(np.concatenate([np.asarray(y_true), np.asarray(y_pred)]))
#     cm = confusion_matrix(y_true, y_pred, labels=labels)
#     fmt = "d"
#     if normalize:
#         cm = cm.astype(float) / cm.sum(axis=1, keepdims=True)
#         fmt = ".2f"
#     plt.figure(figsize=(7, 6))
#     sns.heatmap(cm, annot=True, fmt=fmt, cmap="Blues",
#                 xticklabels=labels, yticklabels=labels, cbar=True)
#     plt.xlabel("Predicted"); plt.ylabel("True"); plt.title(title); plt.tight_layout(); plt.show()

# def choose_threshold_f2(y_true: np.ndarray, y_prob: np.ndarray, default: float = 0.3) -> float:
#     prec, rec, thr = precision_recall_curve(y_true, y_prob)
#     beta = 2
#     f2 = (1 + beta**2) * (prec * rec) / np.clip(beta**2 * prec + rec, 1e-9, None)
#     if len(thr) == 0: return float(default)
#     best = int(np.nanargmax(f2))
#     return float(thr[max(best-1, 0)])

# def choose_threshold_min_recall(y_true: np.ndarray, y_prob: np.ndarray, recall_floor=0.95, default=0.3) -> float:
#     prec, rec, thr = precision_recall_curve(y_true, y_prob)
#     idx = np.where(rec >= recall_floor)[0]
#     if len(idx) == 0 or len(thr) == 0:
#         return float(default)
#     best = idx[np.argmax(prec[idx])]
#     return float(thr[max(best-1, 0)])


# # ---------------- Stage 1 ----------------

# def preprocess_stage1(df: pd.DataFrame) -> pd.DataFrame:
#     out = df.dropna(subset=['Injury Severity']).copy()
#     out['Injury Severity'] = out['Injury Severity'].str.lower()
#     out['Injury Binary'] = (out['Injury Severity'] != 'no apparent injury').astype(int)
#     return out

# def build_xy_stage1(df1: pd.DataFrame) -> Tuple[pd.DataFrame, np.ndarray]:
#     X = df1.drop(columns=['Injury Severity', 'Injury Binary'])
#     X = one_hot_all(X)
#     y = df1['Injury Binary'].to_numpy()
#     return X, y

# def train_stage1_lgb(df: pd.DataFrame, cfg: LGBConfig) -> Dict[str, Any]:
#     df1 = preprocess_stage1(df)
#     X_all, y_all = build_xy_stage1(df1)
#     X_tr, X_te, y_tr, y_te = split_xy(X_all, y_all, cfg)
#     X_tr_orig, y_tr_orig = X_tr.copy(), y_tr.copy()

#     if cfg.use_smote:
#         sm = SMOTE(**cfg.smote_stage1)
#         X_tr, y_tr = sm.fit_resample(X_tr, y_tr)

#     model = lgb.LGBMClassifier(**cfg.lgb_stage1)
#     model.fit(X_tr, y_tr)

#     y_prob_te = model.predict_proba(X_te)[:, 1]
#     thr = choose_threshold_min_recall(y_te, y_prob_te, recall_floor=cfg.stage1_recall_floor) \
#           if cfg.stage1_threshold_strategy == "min_recall" else \
#           choose_threshold_f2(y_te, y_prob_te) if cfg.stage1_threshold_strategy == "f2" \
#           else cfg.stage1_fixed_threshold

#     print(f"[Info] Stage 1 threshold used: {thr:.4f}")  # âœ… ×”×“×¤×¡×ª ×”×¡×£

#     y_pred_te = (y_prob_te >= thr).astype(int)
#     print_report("Stage 1 â€“ TEST", y_te, y_pred_te)
#     plot_confusion_matrix(y_te, y_pred_te, title="Stage 1 â€“ TEST")

#     y_pred_tr_fit = (model.predict_proba(X_tr)[:, 1] >= thr).astype(int)
#     print_report("Stage 1 â€“ TRAIN-FIT", y_tr, y_pred_tr_fit)

#     y_pred_tr_orig = (model.predict_proba(X_tr_orig)[:, 1] >= thr).astype(int)
#     print_report("Stage 1 â€“ TRAIN-ORIG", y_tr_orig, y_pred_tr_orig)

#     return {"model": model, "threshold": thr}


# # ---------------- Stage 2 ----------------

# def build_xy_stage2(df1: pd.DataFrame) -> Tuple[pd.DataFrame, np.ndarray]:
#     df2 = df1.copy()
#     df2['Fatal Binary'] = (df2['Injury Severity'].str.lower() == 'fatal injury').astype(int)
#     df2 = df2[df2['Injury Binary'] == 1].copy()
#     X = df2.drop(columns=['Injury Severity', 'Injury Binary', 'Fatal Binary'])
#     X = one_hot_all(X)
#     y = df2['Fatal Binary'].to_numpy()
#     return X, y

# def train_stage2_lgb(df: pd.DataFrame, cfg: LGBConfig) -> Optional[Dict[str, Any]]:
#     df1 = preprocess_stage1(df)
#     X_all, y_all = build_xy_stage2(df1)
#     if len(np.unique(y_all)) < 2:
#         return None

#     X_tr, X_te, y_tr, y_te = split_xy(X_all, y_all, cfg)
#     X_tr_orig, y_tr_orig = X_tr.copy(), y_tr.copy()

#     if cfg.use_smote:
#         sm = SMOTE(**cfg.smote_stage2)
#         X_tr, y_tr = sm.fit_resample(X_tr, y_tr)

#     model = lgb.LGBMClassifier(**cfg.lgb_stage2)
#     model.fit(X_tr, y_tr)

#     thr = 0.5
#     print(f"[Info] Stage 2 threshold used: {thr:.4f}")  # âœ… ×”×“×¤×¡×ª ×”×¡×£

#     y_pred_te = (model.predict_proba(X_te)[:, 1] >= thr).astype(int)
#     print_report("Stage 2 â€“ TEST", y_te, y_pred_te)
#     plot_confusion_matrix(y_te, y_pred_te, title="Stage 2 â€“ TEST")

#     y_pred_tr_fit = (model.predict_proba(X_tr)[:, 1] >= thr).astype(int)
#     print_report("Stage 2 â€“ TRAIN-FIT", y_tr, y_pred_tr_fit)

#     y_pred_tr_orig = (model.predict_proba(X_tr_orig)[:, 1] >= thr).astype(int)
#     print_report("Stage 2 â€“ TRAIN-ORIG", y_tr_orig, y_pred_tr_orig)

#     return {"model": model, "threshold": thr}


# # ---------------- Stage 3 ----------------

# def build_xy_stage3(df1: pd.DataFrame) -> Tuple[pd.DataFrame, np.ndarray, LabelEncoder]:
#     subset = ['possible injury', 'suspected minor injury', 'suspected serious injury']
#     df3 = df1[df1['Injury Severity'].str.lower().isin(subset)].copy()
#     df3['Severity'] = np.where(df3['Injury Severity'].str.lower() == 'suspected serious injury',
#                                'serious', 'light')
#     X = df3.drop(columns=['Injury Severity', 'Injury Binary', 'Severity'])
#     X = one_hot_all(X)
#     le = LabelEncoder()
#     y = le.fit_transform(df3['Severity'].astype(str).values)
#     return X, y, le

# def train_stage3_lgb(df: pd.DataFrame, cfg: LGBConfig) -> Optional[Dict[str, Any]]:
#     df1 = preprocess_stage1(df)
#     try:
#         X_all, y_all, le = build_xy_stage3(df1)
#     except ValueError:
#         return None
#     if len(np.unique(y_all)) < 2:
#         return None

#     X_tr, X_te, y_tr, y_te = split_xy(X_all, y_all, cfg)
#     X_tr_orig, y_tr_orig = X_tr.copy(), y_tr.copy()

#     if cfg.use_smote:
#         sm = SMOTE(**cfg.smote_stage3)
#         X_tr, y_tr = sm.fit_resample(X_tr, y_tr)

#     model = lgb.LGBMClassifier(**cfg.lgb_stage3)
#     model.fit(X_tr, y_tr)

#     thr = 0.5
#     print(f"[Info] Stage 3 threshold used: {thr:.4f}")  # âœ… ×”×“×¤×¡×ª ×”×¡×£

#     y_pred_te = model.predict(X_te)
#     print_report("Stage 3 â€“ TEST", le.inverse_transform(y_te), le.inverse_transform(y_pred_te))
#     plot_confusion_matrix(le.inverse_transform(y_te), le.inverse_transform(y_pred_te), title="Stage 3 â€“ TEST")

#     y_pred_tr_fit = model.predict(X_tr)
#     print_report("Stage 3 â€“ TRAIN-FIT", le.inverse_transform(y_tr), le.inverse_transform(y_pred_tr_fit))

#     y_pred_tr_orig = model.predict(X_tr_orig)
#     print_report("Stage 3 â€“ TRAIN-ORIG", le.inverse_transform(y_tr_orig), le.inverse_transform(y_pred_tr_orig))

#     return {"model": model, "label_encoder": le, "threshold": thr}


# # ---------------- Run All ----------------

# def run_pipeline_lgb(df: pd.DataFrame, cfg: LGBConfig) -> Dict[str, Any]:
#     stage1 = train_stage1_lgb(df, cfg)
#     stage2 = train_stage2_lgb(df, cfg)
#     stage3 = train_stage3_lgb(df, cfg)
#     return {"stage1": stage1, "stage2": stage2, "stage3": stage3}


# # ---------------- Example Run ----------------

# cfg = LGBConfig(
#     random_state=42,
#     test_size=0.30,
#     use_smote=True,
#     stage1_threshold_strategy="min_recall",
#     stage1_fixed_threshold=0.30,
#     stage1_recall_floor=0.95
# )

# results = run_pipeline_lgb(df, cfg)

"""Grid Search (×–××Ÿ ×”×¨×¦×” ××¨×•×š)

```
×ª×•×¦××•×ª ×¤×—×•×ª ×˜×•×‘×•×ª
```


"""

# from sklearn.metrics import make_scorer, f1_score
# import lightgbm as lgb
# import matplotlib.pyplot as plt
# import seaborn as sns
# from sklearn.model_selection import train_test_split, GridSearchCV
# from imblearn.over_sampling import SMOTE
# from sklearn.preprocessing import LabelEncoder, StandardScaler
# from sklearn.metrics import (classification_report, confusion_matrix,accuracy_score,precision_score,recall_score,f1_score,make_scorer)

# # âœ¨ ×™×¦×™×¨×ª ×¢××•×“×ª ×¤×¦×™×¢×” / ××™×Ÿ ×¤×¦×™×¢×”
# df = df.dropna(subset=['Injury Severity'])
# df['Injury Binary'] = df['Injury Severity'].apply(lambda x: 0 if x == 'no apparent injury' else 1)

# # ğŸ”¹ ×©×œ×‘ ×¨××©×•×Ÿ â€“ ×¤×¦×™×¢×” / ××™×Ÿ ×¤×¦×™×¢×”
# X = df.drop(['Injury Severity', 'Injury Binary'], axis=1)
# y = df['Injury Binary']
# X = pd.get_dummies(X, drop_first=True)
# X.columns = X.columns.str.replace('[^A-Za-z0-9_]+', '_', regex=True)
# X = X.drop(columns=X.select_dtypes(include=['datetime64']).columns)
# X = X.astype(float)

# # Split + SMOTE
# X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)
# X_train1, y_train1 = SMOTE(random_state=42).fit_resample(X_train1, y_train1)

# # âœ… ×”×’×“×¨×ª ×¤×¨××˜×¨×™× ×œ×—×™×¤×•×©
# param_grid = {
#     'n_estimators': [200, 500],
#     'max_depth': [5, 10],
#     'learning_rate': [0.01, 0.03, 0.1],
#     'num_leaves': [15, 31, 63]
# }

# # ××•×“×œ ×‘×¡×™×¡
# base_model = lgb.LGBMClassifier(class_weight='balanced', random_state=42)

# # GridSearchCV
# grid_search = GridSearchCV(
#     estimator=base_model,
#     param_grid=param_grid,
#     scoring=make_scorer(recall_score),
#     cv=3,
#     verbose=1,
#     n_jobs=-1
# )

# # ×”×¨×¦×ª ×”×—×™×¤×•×©
# grid_search.fit(X_train1, y_train1)

# # ×”×“×¤×¡×ª ×”×ª×•×¦××” ×”×˜×•×‘×” ×‘×™×•×ª×¨
# print("Best Parameters:", grid_search.best_params_)
# print("Best F1 Score (train, CV):", grid_search.best_score_)

# # ×©×™××•×© ×‘××•×“×œ ×”×˜×•×‘ ×‘×™×•×ª×¨
# best_model1 = grid_search.best_estimator_

# # ×ª×—×–×™×•×ª
# probs1 = best_model1.predict_proba(X_test1)[:, 1]
# thresh1 = 0.3
# y_pred1 = (probs1 >= thresh1).astype(int)

# # ×“×•"×—
# print("\n×©×œ×‘ 1 â€“ ×¤×¦×™×¢×” / ××™×Ÿ ×¤×¦×™×¢×”")
# print(classification_report(y_test1, y_pred1))
# sns.heatmap(confusion_matrix(y_test1, y_pred1), annot=True, fmt='d', cmap='Blues')
# plt.title("Confusion Matrix - Stage 1")
# plt.show()

"""Cross Validation"""

# from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
# from sklearn.metrics import classification_report, confusion_matrix, make_scorer, f1_score
# # âœ… ×©×œ×‘ ×¨××©×•×Ÿ â€“ ××•×“×œ LightGBM ×¢× Cross Validation (×›×•×œ×œ SMOTE)

# # ğŸ¯ ×”×›× ×ª ×¢××•×“×ª ×”××˜×¨×” ×”×‘×™× ××¨×™×ª
# df = df.dropna(subset=['Injury Severity'])
# df['Injury Binary'] = df['Injury Severity'].apply(lambda x: 0 if x == 'no apparent injury' else 1)

# # ğŸ§  ×”×›× ×ª ×ª×›×•× ×•×ª
# X = df.drop(columns=['Injury Severity', 'Injury Binary'])
# y = df['Injury Binary']

# # ×§×™×“×•×“ One-Hot ×œ×¢××•×“×•×ª ×§×˜×’×•×¨×™××œ×™×•×ª
# X = pd.get_dummies(X, drop_first=True)
# X.columns = X.columns.str.replace('[^A-Za-z0-9_]+', '_', regex=True)

# # ×”×¡×¨×ª ×¢××•×“×•×ª ××¡×•×’ datetime
# X = X.drop(columns=X.select_dtypes(include=['datetime64']).columns)
# X = X.astype(float)

# # ğŸ”„ ×—×œ×•×§×ª ×”× ×ª×•× ×™×
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)

# # âš–ï¸ ××™×–×•×Ÿ ×”× ×ª×•× ×™× ×¢× SMOTE
# smote = SMOTE(random_state=42)
# X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

# # ğŸŒŸ ×”×’×“×¨×ª ×”××•×“×œ
# model = lgb.LGBMClassifier(n_estimators=500, max_depth=10, learning_rate=0.03,
#                            class_weight='balanced', random_state=42)

# # ğŸ§ª Cross Validation
# cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
# f1_weighted = make_scorer(f1_score, average='weighted')
# scores = cross_val_score(model, X_resampled, y_resampled, cv=cv, scoring=f1_weighted)

# # ğŸ“Š ×ª×•×¦××”
# print(f"Weighted F1 CV Mean: {scores.mean():.4f}")
# print(f"Weighted F1 CV Std: {scores.std():.4f}")

"""XGBoost
---


"""

import xgboost as xgb
from catboost import CatBoostClassifier
from sklearn.model_selection import train_test_split, cross_val_score
from imblearn.over_sampling import SMOTENC
from imblearn.over_sampling import SMOTE
from dataclasses import dataclass
from typing import Dict, Any, Optional
from sklearn.metrics import (
    classification_report, confusion_matrix, precision_recall_curve
)
from sklearn.metrics import classification_report, f1_score, confusion_matrix, precision_recall_fscore_support,accuracy_score, precision_score, recall_score
from sklearn.preprocessing import LabelEncoder
from sklearn.utils.class_weight import compute_class_weight
import warnings
warnings.filterwarnings('ignore')
from sklearn.model_selection import GridSearchCV

""">**××•×“×œ ×™×© ×¤×¦×™×¢×”/××™×Ÿ ×¤×¦×™×¢×” ** *italicized text*"""

# ============================================
# XGBoost Pipeline (Stage 1 ×‘×œ×‘×“: Injury / No Injury)
# ×›×•×œ×œ ×¦×™×•×¨ Confusion Matrix
# ============================================

from dataclasses import dataclass
from typing import Dict, Any
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    classification_report, confusion_matrix, precision_recall_curve
)
from imblearn.over_sampling import SMOTE
import xgboost as xgb

# --- ×œ×¦×™×•×¨ ××˜×¨×™×¦×•×ª ---
import matplotlib.pyplot as plt
import seaborn as sns


# ---------- Config (×”×’×“×¨×•×ª) ----------

@dataclass
class TrainConfig:
    test_size: float = 0.30          # ×’×•×“×œ ×¡×˜ ×”×‘×“×™×§×” (30%)
    random_state: int = 42           # ×–×¨×™×¢×ª ×¨× ×“×•× ×œ×©×—×–×•×¨×™×•×ª
    use_smote: bool = True           # ×œ×”×¤×¢×™×œ SMOTE ×œ××™×–×•×Ÿ ×”××—×œ×§×•×ª ×‘-Train
    threshold_strategy: str = "f2"   # ××™×š ×œ×‘×—×•×¨ ×¡×£: "f2" (××¢×“×™×£ Recall) ××• "fixed"
    fixed_threshold: float = 0.30    # ×× threshold_strategy="fixed" â€“ ×–×” ×”×¡×£
    # ×¤×¨××˜×¨×™ XGBoost
    xgb_stage1: Dict[str, Any] = None

    def __post_init__(self):
        if self.xgb_stage1 is None:
            self.xgb_stage1 = dict(
                n_estimators=350, max_depth=6, learning_rate=0.05,
                subsample=0.8, colsample_bytree=0.8,
                random_state=self.random_state, n_jobs=-1
            )


# ---------- Utilities (×¢×–×¨) ----------

def _drop_existing(df: pd.DataFrame, cols: list) -> pd.DataFrame:
    return df.drop(columns=[c for c in cols if c in df.columns], errors='ignore')

def _get_dummies(X: pd.DataFrame) -> pd.DataFrame:
    X = X.copy()
    for c in X.columns:
        if X[c].dtype == 'O':
            X[c] = X[c].astype(str)
    return pd.get_dummies(X, drop_first=True)

def _split(X: pd.DataFrame, y: np.ndarray, cfg: TrainConfig):
    return train_test_split(X, y, test_size=cfg.test_size,
                            random_state=cfg.random_state, stratify=y)

def _choose_threshold_f2(y_true: np.ndarray, y_prob: np.ndarray, default: float = 0.3) -> float:
    prec, rec, thr = precision_recall_curve(y_true, y_prob)
    beta = 2
    denom = np.clip(beta**2 * prec + rec, 1e-9, None)
    f2 = (1 + beta**2) * (prec * rec) / denom
    if len(thr) == 0:
        return float(default)
    best = int(np.nanargmax(f2))
    return float(thr[max(best - 1, 0)])

def _print_report(title: str, y_true, y_pred):
    print(f"\n=== {title} ===")
    print(classification_report(y_true, y_pred, digits=3))
    print("Confusion Matrix:")
    print(confusion_matrix(y_true, y_pred))

def plot_confusion_matrix(y_true, y_pred, title="Confusion Matrix", normalize=False):
    """
    ××¦×™×™×¨ ××˜×¨×™×¦×ª ×‘×œ×‘×•×œ ×‘×¡×’× ×•×Ÿ ×‘×¨×•×¨
    """
    labels = np.unique(np.concatenate([np.asarray(y_true), np.asarray(y_pred)]))
    cm = confusion_matrix(y_true, y_pred, labels=labels)
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1, keepdims=True)
        fmt = ".2f"
    else:
        fmt = "d"

    plt.figure(figsize=(7, 6))
    ax = sns.heatmap(cm, annot=True, fmt=fmt, cmap="Blues",
                     xticklabels=labels, yticklabels=labels, cbar=True)
    ax.set_xlabel("Predicted", fontsize=11)
    ax.set_ylabel("True", fontsize=11)
    ax.set_title(title, fontsize=13, pad=12)
    plt.tight_layout()
    plt.show()


# ---------- Stage 1: Injury / No Injury ----------

def preprocess_stage1(df: pd.DataFrame) -> pd.DataFrame:
    if 'Injury Severity' not in df.columns:
        raise ValueError("×¢××•×“×ª 'Injury Severity' ×œ× × ××¦××” ×‘-df.")
    out = df.dropna(subset=['Injury Severity']).copy()
    out['Injury Severity'] = out['Injury Severity'].str.lower()
    out['Injury Binary'] = (out['Injury Severity'] != 'no apparent injury').astype(int)
    return out

def build_xy_stage1(df: pd.DataFrame) -> (pd.DataFrame, np.ndarray):
    X = _drop_existing(df, ['Injury Severity', 'Injury Binary'])
    X = _get_dummies(X)
    y = df['Injury Binary'].astype(int).values
    return X, y

def train_stage1(df: pd.DataFrame, cfg: TrainConfig) -> Dict[str, Any]:
    df1 = preprocess_stage1(df)
    X_all, y_all = build_xy_stage1(df1)
    X_tr, X_te, y_tr, y_te = _split(X_all, y_all, cfg)

    if cfg.use_smote:
        sm = SMOTE(random_state=cfg.random_state)
        X_tr, y_tr = sm.fit_resample(X_tr, y_tr)

    model1 = xgb.XGBClassifier(**cfg.xgb_stage1)
    model1.fit(X_tr, y_tr)

    y_prob = model1.predict_proba(X_te)[:, 1]
    thr = (_choose_threshold_f2(y_te, y_prob, cfg.fixed_threshold)
           if cfg.threshold_strategy == "f2" else float(cfg.fixed_threshold))
    y_pred = (y_prob >= thr).astype(int)

    _print_report("Stage 1: Injury / No Injury", y_te, y_pred)
    plot_confusion_matrix(y_te, y_pred, title="Confusion Matrix - Stage 1 (Injury/No Injury)", normalize=False)

    feature_columns = X_all.columns.tolist()
    return {
        "model": model1,
        "feature_columns": feature_columns,
        "X_test": X_te, "y_test": y_te,
        "y_prob": y_prob, "y_pred": y_pred,
        "threshold": thr
    }


# ---------- Predict on NEW data ----------

def transform_new_input(df_new: pd.DataFrame, feature_columns: list) -> pd.DataFrame:
    df_new = df_new.copy()
    df_new = df_new.drop(columns=[c for c in ['Injury Severity','Injury Binary'] if c in df_new.columns],
                         errors='ignore')
    for c in df_new.columns:
        if df_new[c].dtype == 'O':
            df_new[c] = df_new[c].astype(str)
    X_new = pd.get_dummies(df_new, drop_first=True)
    X_new = X_new.reindex(columns=feature_columns, fill_value=0)  # ×™×™×©×•×¨ ×œ×¢××•×“×•×ª ×”××™××•×Ÿ
    return X_new

def predict_new(df_new: pd.DataFrame, stage1_outputs: dict):
    model = stage1_outputs["model"]
    cols = stage1_outputs["feature_columns"]
    thr = stage1_outputs["threshold"]
    X_new = transform_new_input(df_new, cols)
    prob = model.predict_proba(X_new)[:, 1]
    pred = (prob >= thr).astype(int)
    return pred, prob


# ---------- Run Pipeline (Stage 1 ×‘×œ×‘×“) ----------

def run_pipeline(df: pd.DataFrame, cfg: TrainConfig) -> Dict[str, Any]:
    stage1 = train_stage1(df, cfg)
    print(f"\n[Info] Stage 1 threshold used: {stage1['threshold']:.3f}")
    return {"stage1": stage1}


# ===== ×©×™××•×© =====
cfg = TrainConfig(threshold_strategy="f2", fixed_threshold=0.30, use_smote=True, random_state=42)
results = run_pipeline(df, cfg)
# ×“×•×’××” ×œ×”×¡×™×§ ×¢×œ ×©×•×¨×” ×—×“×©×”:
# df_new = pd.DataFrame([{...}])
# pred, prob = predict_new(df_new, results["stage1"])
# print(pred, prob)

""">**××•×“×œ 5 ×§×˜×’×¨×•×™×•×ª**"""

# ===============================
# 1. ×”×’×“×¨×ª ×¢××•×“×ª ×”××˜×¨×”
# ===============================

target_column = 'Injury Severity'

# ===============================
# 2. ×”×›× ×ª ×¤×™×¦'×¨×™× ×•××˜×¨×”
# ===============================

# ×”×¤×¨×“×ª ×¤×™×¦'×¨×™× ×•×¢××•×“×ª ××˜×¨×”
X = df.drop(columns=[target_column])
y = df[target_column]


# ×§×™×“×•×“ ×§×˜×’×•×¨×™×•×ª
X = pd.get_dummies(X)

# ×§×™×“×•×“ ×¢××•×“×ª ××˜×¨×”
le = LabelEncoder()
y = le.fit_transform(y)

# ===============================
# 3. ×¤×™×¦×•×œ ×œ-Train ×•-Test
# ===============================

X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.3,
    random_state=42,
    stratify=y
)

# ===============================
# 4. ×”×—×œ×ª SMOTE ×¢×œ ×¡×˜ ×”××™××•×Ÿ ×‘×œ×‘×“
# ===============================

smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

print(f"âœ… ×¡×™×™×× ×• SMOTE: ×›×¢×ª ×™×© {np.bincount(y_train_resampled)} ×“×•×’×××•×ª ×œ×›×œ ×§×˜×’×•×¨×™×”.")

# ===============================
# 5. ×‘× ×™×™×ª ××•×“×œ XGBoost
# ===============================

model = xgb.XGBClassifier(
    objective='multi:softprob',
    eval_metric='mlogloss',
    num_class=len(np.unique(y)),
    random_state=42
)

# ===============================
# 6. ××™××•×Ÿ ×”××•×“×œ
# ===============================

model.fit(X_train_resampled, y_train_resampled)

# ===============================
# 7. ×—×™×–×•×™
# ===============================

y_pred = model.predict(X_test)

# ===============================
# 8. ×“×•×— ×‘×™×¦×•×¢×™×
# ===============================

print("\nğŸ”µ ×ª×•×¦××•×ª ×”××•×“×œ ××—×¨×™ SMOTE:\n")
print(classification_report(
    y_test,
    y_pred,
    target_names=le.classes_
))

# ===============================
# 9. ×—×™×©×•×‘ Macro F1
# ===============================

macro_f1 = f1_score(
    y_test,
    y_pred,
    average='macro'
)

print(f"\nğŸ¯ Macro F1 Score ××—×¨×™ SMOTE: {macro_f1:.4f}")



# ===============================
# ×’×¨×£ ×—×©×™×‘×•×ª ×ª×›×•× ×•×ª (Feature Importance)
# ===============================

import matplotlib.pyplot as plt

# × ×™×§×— ××ª ×”×—×©×™×‘×•×™×•×ª ××ª×•×š ×”××•×“×œ
importances = model.feature_importances_
feature_names = X.columns

# ×¡×™×“×•×¨ ×œ×¤×™ ×¡×“×¨ ×™×•×¨×“
indices = np.argsort(importances)[::-1]

# × ×‘×—×¨ ××ª 20 ×”×¤×™×¦'×¨×™× ×”×›×™ ×—×©×•×‘×™×
top_n = 30
top_features = feature_names[indices][:top_n]
top_importances = importances[indices][:top_n]

# ×¦×™×•×¨ ×’×¨×£
plt.figure(figsize=(12, 8))
plt.barh(top_features[::-1], top_importances[::-1])  # ×”×¤×•×š ×›×“×™ ×©×”×¤×™×¦'×¨ ×”×›×™ ×—×©×•×‘ ×™×”×™×” ×œ××¢×œ×”
plt.xlabel('Feature Importance')
plt.title('Top 20 Most Important Features')
plt.tight_layout()
plt.show()

# # ===============================
# # 1. ×”×›× ×ª ×¢××•×“×ª ××˜×¨×” ×•×¤×™×¦'×¨×™×
# # ===============================

# target_column = 'Injury Severity'

# X = df.drop(columns=[target_column])
# y = df[target_column]


# # ×”××¨×ª ×›×œ ×”×¢××•×“×•×ª ×”×§×˜×’×•×¨×™××œ×™×•×ª ×œ×¡×•×’ (×›×™ ×›×›×” ×¡××•×˜ ×¢×•×‘×“ ×’× ×œ×œ× ×“××™×–)-'category'
# for col in X.columns:
#     if X[col].dtype == 'object':
#         X[col] = X[col].astype('category')

# # ×§×™×“×•×“ ×¢××•×“×ª ×”××˜×¨×”
# le = LabelEncoder()
# y = le.fit_transform(y)

# # ===============================
# # 2. ×¤×™×¦×•×œ ×œ-Train/Test
# # ===============================

# X_train, X_test, y_train, y_test = train_test_split(
#     X, y,
#     test_size=0.3,
#     random_state=42,
#     stratify=y
# )

# # ===============================
# # 3. SMOTE ×œ××™×–×•×Ÿ ×¡×˜ ×”××™××•×Ÿ
# # ===============================

# # ×©×™××™ ×œ×‘ - SMOTE ×¢×•×‘×“ ×¨×§ ×¢×œ × ×ª×•× ×™× × ×•××¨×™×™×, ××– × ××™×¨ ×§×˜×’×•×¨×™×•×ª ×œ-int ×–×× ×™
# X_train_encoded = X_train.copy()

# for col in X_train_encoded.select_dtypes(['category']).columns:
#     X_train_encoded[col] = X_train_encoded[col].cat.codes

# smote = SMOTE(random_state=42)
# X_train_resampled, y_train_resampled = smote.fit_resample(X_train_encoded, y_train)

# print(f"âœ… ×¡×™×™×× ×• SMOTE: {np.bincount(y_train_resampled)}")

# # ===============================
# # 4. ×‘× ×™×™×ª ××•×“×œ XGBoost
# # ===============================

# model = xgb.XGBClassifier(
#     objective='multi:softprob',
#     eval_metric='mlogloss',
#     tree_method='hist',
#     enable_categorical=True,
#     num_class=len(np.unique(y)),
#     random_state=42
# )

# # ===============================
# # 5. ××™××•×Ÿ ×”××•×“×œ
# # ===============================

# model.fit(X_train_resampled, y_train_resampled)

# # ===============================
# # 6. ×—×™×–×•×™
# # ===============================

# # ×’× ×¢×œ ×”-X_test × ×¦×˜×¨×š ×œ×§×•×“×“ ×–×× ×™×ª ×›×“×™ ×œ×—×–×•×ª
# X_test_encoded = X_test.copy()
# for col in X_test_encoded.select_dtypes(['category']).columns:
#     X_test_encoded[col] = X_test_encoded[col].cat.codes

# y_pred = model.predict(X_test_encoded)

# # ===============================
# # 7. ×“×•×— ×‘×™×¦×•×¢×™×
# # ===============================

# print("\nğŸ”µ ×ª×•×¦××•×ª ×”××•×“×œ ××—×¨×™ SMOTE ×•××™××•×Ÿ ×—×›×:\n")
# print(classification_report(
#     y_test,
#     y_pred,
#     target_names=le.classes_
# ))

# # ===============================
# # 8.  ×—×™×©×•×‘ ××“×“ Macro F1
# # ===============================

# macro_f1 = f1_score(
#     y_test,
#     y_pred,
#     average='macro'
# )

# print(f"\nğŸ¯ Macro F1 Score ××—×¨×™ SMOTE: {macro_f1:.4f}")

# # ===============================
# # 9. Feature Importance
# # ===============================

# import matplotlib.pyplot as plt

# # × ×™×§×— ××ª ×”×—×©×™×‘×•×™×•×ª ××ª×•×š ×”××•×“×œ
# importances = model.feature_importances_
# feature_names = X_train_encoded.columns

# # ×¡×™×“×•×¨ ×œ×¤×™ ×¡×“×¨ ×™×•×¨×“
# indices = np.argsort(importances)[::-1]

# # × ×‘×—×¨ ××ª 30 ×”×¤×™×¦'×¨×™× ×”×›×™ ×—×©×•×‘×™×
# top_n = 30
# top_features = feature_names[indices][:top_n]
# top_importances = importances[indices][:top_n]

# # ×¦×™×•×¨ ×’×¨×£
# plt.figure(figsize=(12, 8))
# plt.barh(top_features[::-1], top_importances[::-1])
# plt.xlabel('Feature Importance')
# plt.title('Top 20 Most Important Features')
# plt.tight_layout()
# plt.show()



# # ===============================
# # 10.×¡×™× ×•×Ÿ ×¤×™×¦'×¨×™× ×—×œ×©×™× ×œ×¤×™ Feature Importance
# # ===============================

# # ×§×•×“× ×›×œ - × ×¢×©×” DataFrame ×©×œ ×”×—×©×™×‘×•×™×•×ª
# feature_importance_df = pd.DataFrame({
#     'Feature': feature_names,
#     'Importance': importances
# })

# # ××™×•×Ÿ ×¢×•×œ×” ×œ×¤×™ Importance (××”×›×™ ×¤×—×•×ª ×—×©×•×‘)
# feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

# # ×”×¦×’×ª ×”×˜×‘×œ×” - ×œ××©×œ 30 ×”×¤×™×¦'×¨×™× ×”×›×™ ×¤×—×•×ª ×—×©×•×‘×™×
# print("\nğŸ”µ 30 ×”×¤×™×¦'×¨×™× ×”×›×™ ×¤×—×•×ª ×—×©×•×‘×™×:\n")
# print(feature_importance_df.head(50))

# # ××¤×©×¨×•×ª ××•×˜×•××˜×™×ª ×œ×¡× ×Ÿ - ×œ××©×œ, ×œ×”×¡×™×¨ ××ª ×›×œ ×”×¤×™×¦'×¨×™× ×¢× Importance ×§×˜×Ÿ ×-0.001
# threshold = 0.0038

# # ××¦×™××ª ×”×¤×™×¦'×¨×™× ×”××•×¢××“×™× ×œ×”×¡×¨×”
# features_to_drop = feature_importance_df[feature_importance_df['Importance'] < threshold]['Feature'].tolist()

# print(f"\nğŸš® × ××¦××• {len(features_to_drop)} ×¤×™×¦'×¨×™× ×¢× ×—×©×™×‘×•×ª × ××•×›×” ×-{threshold}.")
# print(features_to_drop)

# # ===============================
# # 11.××•×¤×¦×™×•× ×œ×™: ×œ××—×•×§ ××•×ª× ××”-DataFrame
# # ===============================

# X_filtered = X.drop(columns=features_to_drop)

# print(f"\nâœ… ×”×“××˜×” ×”×—×“×© ××—×¨×™ ×¡×™× ×•×Ÿ ××›×™×œ {X_filtered.shape[1]} ×¤×™×¦'×¨×™× ×‘××§×•× {X.shape[1]}.")



# # ===============================
# # 12.×¤×™×¦×•×œ ×—×“×© ×œ-Train/Test ×¢×œ ×”×“××˜×” ×”××¡×•× ×Ÿ
# # ===============================

# X_train, X_test, y_train, y_test = train_test_split(
#     X_filtered, y,
#     test_size=0.3,
#     random_state=42,
#     stratify=y
# )

# # ===============================
# # 13.×˜×™×¤×•×œ ×‘-SMOTE ×©×•×‘ ×¢×œ ×”×“××˜×” ×”×—×“×©
# # ===============================

# X_train_encoded = X_train.copy()

# for col in X_train_encoded.select_dtypes(['category']).columns:
#     X_train_encoded[col] = X_train_encoded[col].cat.codes

# smote = SMOTE(random_state=42)
# X_train_resampled, y_train_resampled = smote.fit_resample(X_train_encoded, y_train)

# # ===============================
# # 14.×‘× ×™×™×ª ××•×“×œ ×—×“×© ×•××™××•×Ÿ
# # ===============================

# model = xgb.XGBClassifier(
#     objective='multi:softprob',
#     eval_metric='mlogloss',
#     tree_method='hist',
#     enable_categorical=True,
#     num_class=len(np.unique(y)),
#     random_state=42
# )

# model.fit(X_train_resampled, y_train_resampled)

# # ===============================
# # 15.×—×™×–×•×™ ××—×“×©
# # ===============================

# X_test_encoded = X_test.copy()
# for col in X_test_encoded.select_dtypes(['category']).columns:
#     X_test_encoded[col] = X_test_encoded[col].cat.codes

# y_pred = model.predict(X_test_encoded)

# # ===============================
# # 16.×“×•×— ×‘×™×¦×•×¢×™× ××—×“×©
# # ===============================

# print("\nğŸ”µ ×ª×•×¦××•×ª ×”××•×“×œ ××—×¨×™ ××—×™×§×ª ×¤×™×¦'×¨×™×:\n")
# print(classification_report(
#     y_test,
#     y_pred,
#     target_names=le.classes_
# ))

# macro_f1 = f1_score(
#     y_test,
#     y_pred,
#     average='macro'
# )

# print(f"\nğŸ¯ Macro F1 Score ××—×¨×™ ××—×™×§×ª ×¤×™×¦'×¨×™×: {macro_f1:.4f}")

# # ========================================
# # XGBoost ×¢× SMOTE ×××•×§×“, ××©×§×œ×™× ××•×ª×××™× ×•Ö¾Threshold ×—×›×
# # ========================================

# # ========================================
# # 1. ×”×›× ×ª ×”× ×ª×•× ×™×
# # ========================================
# target_col = 'Injury Severity'
# X = df.drop(columns=[target_col], errors='ignore').copy()
# y = df[target_col].copy()

# # ×”××¨×ª ×¢××•×“×•×ª ××•×‘×™×™×§×˜ ×œ×§×˜×’×•×¨×™×”
# for col in X.select_dtypes(include='object').columns:
#     X[col] = X[col].astype('category')

# # ×§×™×“×•×“ y
# label_encoder = LabelEncoder()
# y_encoded = label_encoder.fit_transform(y)
# classes = label_encoder.classes_

# # ×¤×™×¦×•×œ ×œÖ¾Train/Test
# X_train, X_test, y_train, y_test = train_test_split(
#     X, y_encoded, test_size=0.3, stratify=y_encoded, random_state=42
# )

# # ×§×™×“×•×“ ×§×˜×’×•×¨×™×•×ª ××¡×¤×¨×™ ×¢×‘×•×¨ SMOTE
# X_train_enc = X_train.copy()
# for col in X_train_enc.select_dtypes(include='category').columns:
#     X_train_enc[col] = X_train_enc[col].cat.codes

# # ========================================
# # 2. SMOTE ×××•×§×“ ×œ×§×˜×’×•×¨×™×•×ª ×—×œ×©×•×ª ×›×•×œ×œ ×§×˜×œ× ×™
# # ========================================
# target_classes = ['possible injury', 'suspected minor injury', 'suspected serious injury', 'fatal injury']
# target_indices = [np.where(classes == cls)[0][0] for cls in target_classes]

# _, counts = np.unique(y_train, return_counts=True)
# max_count = max(counts)
# sampling_strategy = {cls_idx: max_count for cls_idx in target_indices}

# smote = SMOTE(sampling_strategy=sampling_strategy, random_state=42)
# X_resampled, y_resampled = smote.fit_resample(X_train_enc, y_train)

# # ========================================
# # 3. ××©×§×œ×™× ××—×•×–×§×™× ×œ×§×˜×’×•×¨×™×•×ª ×”×—×©×•×‘×•×ª
# # ========================================
# weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_resampled), y=y_resampled)
# for idx in target_indices:
#     weights[idx] *= 1.5
# sample_weights = np.array([weights[i] for i in y_resampled])

# # ========================================
# # 4. ××™××•×Ÿ ××•×“×œ XGBoost
# # ========================================
# model = xgb.XGBClassifier(
#     objective='multi:softprob',
#     num_class=len(np.unique(y_encoded)),
#     eval_metric='mlogloss',
#     tree_method='hist',
#     enable_categorical=True,
#     n_estimators=300,
#     max_depth=8,
#     learning_rate=0.05,
#     subsample=0.8,
#     random_state=42
# )

# model.fit(X_resampled, y_resampled, sample_weight=sample_weights)

# # ========================================
# # 5. ×—×™×–×•×™ ×¢× threshold ××•×ª×× ×œÖ¾serious ×•Ö¾fatal
# # ========================================
# X_test_enc = X_test.copy()
# for col in X_test_enc.select_dtypes(include='category').columns:
#     X_test_enc[col] = X_test_enc[col].cat.codes

# y_proba = model.predict_proba(X_test_enc)
# serious_index = np.where(classes == 'suspected serious injury')[0][0]
# fatal_index = np.where(classes == 'fatal injury')[0][0]

# threshold_serious = 0.25
# threshold_fatal = 0.25

# y_pred = []
# for probs in y_proba:
#     if probs[fatal_index] >= threshold_fatal:
#         y_pred.append(fatal_index)
#     elif probs[serious_index] >= threshold_serious:
#         y_pred.append(serious_index)
#     else:
#         y_pred.append(np.argmax(probs))

# # ========================================
# # 6. ×“×•×— ×‘×™×¦×•×¢×™× + ××˜×¨×™×¦×ª ×‘×œ×‘×•×œ
# # ========================================
# print("\n\U0001F4CA ×“×•×— ×‘×™×¦×•×¢×™× ×œ×¤×™ ×§×˜×’×•×¨×™×•×ª:")
# print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))

# macro_f1 = f1_score(y_test, y_pred, average='macro')
# print(f"\n\U0001F3AF Macro F1 Score: {macro_f1:.4f}")

# cm = confusion_matrix(y_test, y_pred)
# plt.figure(figsize=(8, 6))
# sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
#             xticklabels=label_encoder.classes_,
#             yticklabels=label_encoder.classes_)
# plt.xlabel('Predicted')
# plt.ylabel('Actual')
# plt.title('Confusion Matrix')
# plt.tight_layout()
# plt.show()

# # ========================================
# # 7. × ×™×ª×•×— threshold ×œÖ¾serious ×•Ö¾fatal
# # ========================================
# print("\n\U0001F4C8 × ×™×ª×•×— threshold ×œÖ¾'suspected serious injury' ×•Ö¾'fatal injury':")
# thresholds = np.linspace(0.1, 0.5, 9)
# for t in thresholds:
#     y_pred_tmp = []
#     for probs in y_proba:
#         if probs[fatal_index] >= t:
#             y_pred_tmp.append(fatal_index)
#         elif probs[serious_index] >= t:
#             y_pred_tmp.append(serious_index)
#         else:
#             y_pred_tmp.append(np.argmax(probs))
#     prec, rec, f1, _ = precision_recall_fscore_support(y_test, y_pred_tmp, labels=[serious_index, fatal_index], average='macro')
#     print(f"Threshold={t:.2f} | Recall: {rec:.2f} | Precision: {prec:.2f} |Â F1:Â {f1:.2f}")

"""> **××•×“×œ: 3 ×§×˜×’×•×¨×™×•×ª - ××™×Ÿ ×¤×¦×™×¢×”/×¤×¦×™×¢×” ×§×œ×”/×¤×¦×™×¢×” ×—××•×¨×”**

"""

def train_best_balanced_model(df, target_column='Injury Severity'):

    # ğŸ¯ ××™×¤×•×™ ×œÖ¾3 ×§×˜×’×•×¨×™×•×ª
    injury_mapping = {
        'no apparent injury': 'no injury',
        'possible injury': 'minor injury',
        'suspected minor injury': 'minor injury',
        'suspected serious injury': 'severe injury',
        'fatal injury': 'severe injury'
    }
    df = df[df[target_column].isin(injury_mapping.keys())].copy()
    df[target_column] = df[target_column].map(injury_mapping)

    # ğŸ§ª ×”×›× ×”
    X = df.drop(columns=[target_column])
    y = df[target_column]

    for col in X.select_dtypes(include='object').columns:
        X[col] = X[col].astype('category')

    le = LabelEncoder()
    y_encoded = le.fit_transform(y)

    X_train, X_test, y_train, y_test = train_test_split(
        X, y_encoded, stratify=y_encoded, test_size=0.3, random_state=42
    )

    # ğŸ”£ ×§×™×“×•×“ ×§×˜×’×•×¨×™×•×ª ×‘××¡×¤×¨×™×
    X_train_encoded = X_train.copy()
    for col in X_train_encoded.select_dtypes(['category']).columns:
        X_train_encoded[col] = X_train_encoded[col].cat.codes

    # ğŸ§¬ SMOTE ×××•×§×“ ×œ×¤×¦×™×¢×•×ª ×—××•×¨×•×ª
    label_map = dict(zip(le.classes_, le.transform(le.classes_)))
    severe_label = label_map['severe injury']
    severe_count = np.sum(y_train == severe_label)
    target_count = max(severe_count + 1000, 3000)

    smote = SMOTE(sampling_strategy={severe_label: target_count}, random_state=42)
    X_train_resampled, y_train_resampled = smote.fit_resample(X_train_encoded, y_train)

    # âš–ï¸ ×—×™×©×•×‘ ××©×§×œ×™×
    class_weights = compute_class_weight('balanced', classes=np.unique(y_train_resampled), y=y_train_resampled)
    weight_dict = dict(zip(np.unique(y_train_resampled), class_weights))
    sample_weights = np.array([weight_dict[i] for i in y_train_resampled])

    # ğŸš€ ××™××•×Ÿ ××•×“×œ XGBoost
    model = xgb.XGBClassifier(
        objective='multi:softprob',
        eval_metric='mlogloss',
        tree_method='hist',
        enable_categorical=True,
        num_class=len(np.unique(y_encoded)),
        learning_rate=0.05,
        max_depth=5,
        min_child_weight=10,
        n_estimators=300,
        subsample=0.8,
        colsample_bytree=0.8,
        reg_alpha=0.5,
        reg_lambda=1.0,
        random_state=42
    )
    model.fit(X_train_resampled, y_train_resampled, sample_weight=sample_weights)

    # ğŸ” ×—×™×–×•×™
    X_test_encoded = X_test.copy()
    for col in X_test_encoded.select_dtypes(['category']).columns:
        X_test_encoded[col] = X_test_encoded[col].cat.codes

    y_pred = model.predict(X_test_encoded)

    # ğŸ“Š ×—×™×©×•×‘ ××“×“×™×
    macro_f1 = f1_score(y_test, y_pred, average='macro')
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='macro')
    recall = recall_score(y_test, y_pred, average='macro')

    # ğŸ” Cross-Validation
    cv_score = cross_val_score(
        model, X_train_resampled, y_train_resampled,
        scoring='f1_macro', cv=5
    ).mean()

    # ğŸ“‹ ×“×•×— ×¡×™×•×•×’ ××¤×•×¨×˜
    print("\nğŸ“ˆ ×“×•\"×— ×¡×™×•×•×’ (3 ×§×˜×’×•×¨×™×•×ª - XGBoost):")
    print(classification_report(y_test, y_pred, target_names=le.classes_))

    # ğŸ“‰ ××˜×¨×™×¦×ª ×‘×œ×‘×•×œ
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=le.classes_, yticklabels=le.classes_)
    plt.title("Confusion Matrix - XGBoost (3 Classes)")
    plt.xlabel("Predicted")
    plt.ylabel("True")
    plt.tight_layout()
    plt.show()

    # ğŸ§¾ ××“×“×™× ××¡×›××™×
    print("\nğŸ“‹ Evaluation Metrics:")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"Precision (macro): {precision:.4f}")
    print(f"Recall (macro): {recall:.4f}")
    print(f"F1 Score (macro): {macro_f1:.4f}")

    # ğŸ§  ×”×—×–×¨×ª ×ª×•×¦××•×ª
    return {
        'model': model,
        'label_encoder': le,
        'macro_f1_test': macro_f1,
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'macro_f1_train_cv': cv_score,
        'classification_report': classification_report(y_test, y_pred, target_names=le.classes_, output_dict=True),
        'confusion_matrix': cm
    }

# ğŸ“¤ ×”×¤×¢×œ×ª ×”×¤×•× ×§×¦×™×” ×¢×œ ×”×“××˜×” ×©×œ×š
results = train_best_balanced_model(df)

# ğŸ–¨ï¸ ×”×“×¤×¡×ª ×ª×•×¦××•×ª ××¡×›××•×ª
print("\nğŸ” ×ª×•×¦××•×ª ×›×œ×œ×™×•×ª:")
print("ğŸ¯ Macro F1 (Test):", results['macro_f1_test'])
print("ğŸ“š Macro F1 (Train CV):", results['macro_f1_train_cv'])
print("âœ… Accuracy:", results['accuracy'])
print("âœ… Precision (macro):", results['precision'])
print("âœ… Recall (macro):", results['recall'])

"""> **××•×“×œ ×“×•-×©×œ×‘×™**

*   ×©×œ×‘ 1: ×™×© ×¤×¦×™×¢×”/××™×Ÿ ×¤×¦×™×¢×”
*   ×©×œ×‘ 2: ×¤×¦×™×¢×” ×§×˜×œ× ×™×ª/×§×œ×”/×§×©×”

"""

# ============================================
# Modular 2-Stage XGBoost Pipeline (SMOTE + F2)
# ×›×•×œ×œ ×¦×™×•×¨ Confusion Matrix ×œ×©× ×™ ×”×©×œ×‘×™×
# ============================================

# ---------- Config (×”×’×“×¨×•×ª) ----------

@dataclass
class TrainConfig:
    test_size: float = 0.30          # ×’×•×“×œ ×¡×˜ ×”×‘×“×™×§×” (30%)
    random_state: int = 42           # ×–×¨×™×¢×ª ×¨× ×“×•× ×œ×©×—×–×•×¨×™×•×ª
    use_smote: bool = True           # ×œ×”×¤×¢×™×œ SMOTE ×œ××™×–×•×Ÿ ×”××—×œ×§×•×ª ×‘-Train
    threshold_strategy: str = "f2"   # ××™×š ×œ×‘×—×•×¨ ×¡×£: "f2" (××¢×“×™×£ Recall) ××• "fixed"
    fixed_threshold: float = 0.30    # ×× threshold_strategy="fixed" â€“ ×–×” ×”×¡×£
    # ×¤×¨××˜×¨×™ XGBoost
    xgb_stage1: Dict[str, Any] = None
    xgb_stage2: Dict[str, Any] = None

    def __post_init__(self):
        if self.xgb_stage1 is None:
            self.xgb_stage1 = dict(
                n_estimators=350, max_depth=6, learning_rate=0.05,
                subsample=0.8, colsample_bytree=0.8,
                random_state=self.random_state, n_jobs=-1
            )
        if self.xgb_stage2 is None:
            self.xgb_stage2 = dict(
                objective='multi:softprob',
                n_estimators=350, max_depth=6, learning_rate=0.05,
                subsample=0.8, colsample_bytree=0.8,
                random_state=self.random_state, n_jobs=-1
            )


# ---------- Utilities (×¢×–×¨) ----------

def _drop_existing(df: pd.DataFrame, cols: list) -> pd.DataFrame:
    return df.drop(columns=[c for c in cols if c in df.columns], errors='ignore')

def _get_dummies(X: pd.DataFrame) -> pd.DataFrame:
    X = X.copy()
    for c in X.columns:
        if X[c].dtype == 'O':
            X[c] = X[c].astype(str)
    return pd.get_dummies(X, drop_first=True)

def _split(X: pd.DataFrame, y: np.ndarray, cfg: TrainConfig):
    return train_test_split(X, y, test_size=cfg.test_size,
                            random_state=cfg.random_state, stratify=y)

def _choose_threshold_f2(y_true: np.ndarray, y_prob: np.ndarray, default: float = 0.3) -> float:
    prec, rec, thr = precision_recall_curve(y_true, y_prob)
    beta = 2
    denom = np.clip(beta**2 * prec + rec, 1e-9, None)
    f2 = (1 + beta**2) * (prec * rec) / denom
    if len(thr) == 0:
        return float(default)
    best = int(np.nanargmax(f2))
    return float(thr[max(best - 1, 0)])

def _print_report(title: str, y_true, y_pred):
    print(f"\n=== {title} ===")
    print(classification_report(y_true, y_pred, digits=3))
    print("Confusion Matrix:")
    print(confusion_matrix(y_true, y_pred))

def plot_confusion_matrix(y_true, y_pred, title="Confusion Matrix", normalize=False):
    """
    ××¦×™×™×¨ ××˜×¨×™×¦×ª ×‘×œ×‘×•×œ ×‘×¡×’× ×•×Ÿ ×‘×¨×•×¨:
    - ×¦×™×¨ Y = True, ×¦×™×¨ X = Predicted
    - ×›×•×ª×¨×ª ×œ××¢×œ×”
    - ××¤×©×¨ ×œ× ×¨××œ ×œ××—×•×–×™× ×‘×©×•×¨×•×ª (normalize=True)
    """
    # ×‘×—×™×¨×ª ×¡×“×¨ ×ª×•×•×™×•×ª ×¢×§×‘×™ (×›×œ ×”×¢×¨×›×™× ×©×”×•×¤×™×¢×• ×‘×××ª ××• ×‘×ª×—×–×™×ª)
    labels = np.unique(np.concatenate([np.asarray(y_true), np.asarray(y_pred)]))
    cm = confusion_matrix(y_true, y_pred, labels=labels)
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1, keepdims=True)
        fmt = ".2f"
    else:
        fmt = "d"

    plt.figure(figsize=(7, 6))
    ax = sns.heatmap(cm, annot=True, fmt=fmt, cmap="Blues",
                     xticklabels=labels, yticklabels=labels, cbar=True)
    ax.set_xlabel("Predicted", fontsize=11)
    ax.set_ylabel("True", fontsize=11)
    ax.set_title(title, fontsize=13, pad=12)
    plt.tight_layout()
    plt.show()


# ---------- Stage 1: Injury / No Injury ----------

def preprocess_stage1(df: pd.DataFrame) -> pd.DataFrame:
    if 'Injury Severity' not in df.columns:
        raise ValueError("×¢××•×“×ª 'Injury Severity' ×œ× × ××¦××” ×‘-df.")
    out = df.dropna(subset=['Injury Severity']).copy()
    out['Injury Severity'] = out['Injury Severity'].str.lower()
    out['Injury Binary'] = (out['Injury Severity'] != 'no apparent injury').astype(int)
    return out

def build_xy_stage1(df: pd.DataFrame) -> (pd.DataFrame, np.ndarray):
    X = _drop_existing(df, ['Injury Severity', 'Injury Binary'])
    X = _get_dummies(X)
    y = df['Injury Binary'].astype(int).values
    return X, y

def train_stage1(df: pd.DataFrame, cfg: TrainConfig) -> Dict[str, Any]:
    df1 = preprocess_stage1(df)
    X_all, y_all = build_xy_stage1(df1)
    X_tr, X_te, y_tr, y_te = _split(X_all, y_all, cfg)

    if cfg.use_smote:
        sm = SMOTE(random_state=cfg.random_state)
        X_tr, y_tr = sm.fit_resample(X_tr, y_tr)

    model1 = xgb.XGBClassifier(**cfg.xgb_stage1)
    model1.fit(X_tr, y_tr)

    y_prob = model1.predict_proba(X_te)[:, 1]
    thr = (_choose_threshold_f2(y_te, y_prob, cfg.fixed_threshold)
           if cfg.threshold_strategy == "f2" else float(cfg.fixed_threshold))
    y_pred = (y_prob >= thr).astype(int)

    _print_report("Stage 1: Injury / No Injury", y_te, y_pred)
    plot_confusion_matrix(y_te, y_pred, title="Confusion Matrix - Stage 1 (Injury/No Injury)", normalize=False)

    feature_columns = X_all.columns.tolist()
    return {
        "model": model1,
        "feature_columns": feature_columns,
        "X_test": X_te, "y_test": y_te,
        "y_prob": y_prob, "y_pred": y_pred,
        "threshold": thr
    }


# ---------- Stage 2: light / serious / fatal (×œ× ×—×•×‘×” ×›×¨×’×¢) ----------

def map_injury(df: pd.DataFrame, mapping: Dict[str, str]) -> pd.DataFrame:
    out = df.copy()
    out['Injury Severity'] = out['Injury Severity'].str.lower()
    out['Injury Mapped'] = out['Injury Severity'].map(mapping)
    return out

def build_xy_stage2(df_mapped: pd.DataFrame) -> (pd.DataFrame, np.ndarray, LabelEncoder):
    df2 = df_mapped[df_mapped['Injury Mapped'].notna()].copy()
    if df2.empty:
        raise ValueError("××™×Ÿ ×“×’×™××•×ª ××ª×•×™×’×•×ª ×œ-Injury Mapped ×¢×‘×•×¨ ×©×œ×‘ 2.")
    X = _drop_existing(df2, ['Injury Severity', 'Injury Binary', 'Injury Mapped'])
    X = _get_dummies(X)
    le = LabelEncoder()
    y = le.fit_transform(df2['Injury Mapped'].astype(str).values)
    return X, y, le

def train_stage2(df: pd.DataFrame, cfg: TrainConfig, mapping: Dict[str, str]) -> Optional[Dict[str, Any]]:
    df_mapped = map_injury(df, mapping)
    try:
        X_all, y_all, le = build_xy_stage2(df_mapped)
    except ValueError as e:
        print(f"\n[Stage 2] {e}")
        return None

    if len(np.unique(y_all)) < 2:
        print("\n[Stage 2] ×¨×§ ××—×œ×§×” ××—×ª ×œ××—×¨ ×”××™×¤×•×™ â€“ ×œ× × ×™×ª×Ÿ ×œ×××Ÿ ××¡×•×•×’ ×¨×‘Ö¾××—×œ×§×ª×™.")
        return None

    X_tr, X_te, y_tr, y_te = _split(X_all, y_all, cfg)

    if cfg.use_smote:
        sm = SMOTE(random_state=cfg.random_state)
        X_tr, y_tr = sm.fit_resample(X_tr, y_tr)

    model2 = xgb.XGBClassifier(**cfg.xgb_stage2)
    model2.fit(X_tr, y_tr)

    y_pred_enc = model2.predict(X_te)
    y_true_lbl = le.inverse_transform(y_te)
    y_pred_lbl = le.inverse_transform(y_pred_enc)

    _print_report("Stage 2: light / serious / fatal", y_true_lbl, y_pred_lbl)
    plot_confusion_matrix(y_true_lbl, y_pred_lbl, title="Confusion Matrix - Stage 2 (light/serious/fatal)", normalize=False)

    return {
        "model": model2,
        "label_encoder": le,
        "X_test": X_te, "y_test_labels": y_true_lbl,
        "y_pred_labels": y_pred_lbl
    }


# ---------- Predict on NEW data (×”×¡×§×” ×¢×œ × ×ª×•× ×™× ×—×“×©×™×) ----------

def transform_new_input(df_new: pd.DataFrame, feature_columns: list) -> pd.DataFrame:
    df_new = df_new.copy()
    df_new = df_new.drop(columns=[c for c in ['Injury Severity','Injury Binary','Injury Mapped'] if c in df_new.columns],
                         errors='ignore')
    for c in df_new.columns:
        if df_new[c].dtype == 'O':
            df_new[c] = df_new[c].astype(str)
    X_new = pd.get_dummies(df_new, drop_first=True)
    X_new = X_new.reindex(columns=feature_columns, fill_value=0)  # ×™×™×©×•×¨ ×œ×¢××•×“×•×ª ×”××™××•×Ÿ
    return X_new

def predict_new(df_new: pd.DataFrame, stage1_outputs: dict):
    model = stage1_outputs["model"]
    cols = stage1_outputs["feature_columns"]
    thr = stage1_outputs["threshold"]
    X_new = transform_new_input(df_new, cols)
    prob = model.predict_proba(X_new)[:, 1]
    pred = (prob >= thr).astype(int)
    return pred, prob


# ---------- Run Pipeline (×”×¨×¦×”) ----------

def run_pipeline(df: pd.DataFrame, cfg: TrainConfig, mapping: Dict[str, str]) -> Dict[str, Any]:
    stage1 = train_stage1(df, cfg)
    stage2 = train_stage2(df, cfg, mapping)  # ××¤×©×¨ ×œ×”×¢×¨×™×š ×’× ×©×œ×‘ 2; ×× ×œ× ×¦×¨×™×š â€“ ××¤×©×¨ ×œ×”×©×‘×™×ª
    print(f"\n[Info] Stage 1 threshold used: {stage1['threshold']:.3f}")
    return {"stage1": stage1, "stage2": stage2}


# ===== ×©×™××•×© =====
mapping = {
    'possible injury': 'light injury',
    'suspected minor injury': 'light injury',
    'suspected serious injury': 'serious injury',
    'fatal injury': 'fatal injury'
}
cfg = TrainConfig(threshold_strategy="f2", fixed_threshold=0.30, use_smote=True, random_state=42)
results = run_pipeline(df, cfg, mapping)
# ×“×•×’××” ×œ×”×¡×™×§ ×¢×œ ×©×•×¨×” ×—×“×©×”:
# df_new = pd.DataFrame([{...}])
# pred, prob = predict_new(df_new, results["stage1"])
# print(pred, prob)

# ğŸ› ï¸ ×©×œ×‘ 0: ××—×™×§×ª ×¢×¨×›×™× ×—×¡×¨×™× ×‘×¢××•×“×ª ×”××˜×¨×”
df = df.dropna(subset=['Injury Severity'])

# âœ¨ ×©×œ×‘ 1: ×™×¦×™×¨×ª ×¢××•×“×ª ×¤×¦×™×¢×” / ××™×Ÿ ×¤×¦×™×¢×”
df['Injury Binary'] = df['Injury Severity'].apply(
    lambda x: 0 if x == 'no apparent injury' else 1
)

# ğŸ› ï¸ ×‘× ×™×™×ª X ×•-y ×œ×©×œ×‘ ×”×¨××©×•×Ÿ
X = df.drop(['Injury Severity', 'Injury Binary'], axis=1)
y_stage1 = df['Injury Binary']


# ×§×™×“×•×“ ×˜×§×¡×˜×™×
le_features = LabelEncoder()
for col in cat_cols:
    X[col] = le_features.fit_transform(X[col])

# ×¤×™×¦×•×œ ×œ-Train/Test
X_train1, X_test1, y_train1, y_test1 = train_test_split(
    X, y_stage1, test_size=0.3, random_state=42, stratify=y_stage1
)

# SMOTE ×¢×œ ×§×‘×•×¦×ª ×”××™××•×Ÿ
smote = SMOTE(random_state=42)
X_res1, y_res1 = smote.fit_resample(X_train1, y_train1)

# ğŸš€ ××™××•×Ÿ ××•×“×œ XGBoost ×œ×©×œ×‘ ×”×¨××©×•×Ÿ
xgb_model_stage1 = xgb.XGBClassifier(
    n_estimators=300,
    max_depth=6,
    learning_rate=0.05,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42
)
xgb_model_stage1.fit(X_res1, y_res1)

# ğŸ¯ ×—×™×–×•×™ ×¢× ×¡×£ ××•×ª××
y_probs1 = xgb_model_stage1.predict_proba(X_test1)[:, 1]
threshold = 0.3
y_pred1_threshold = (y_probs1 >= threshold).astype(int)

# ğŸ“ˆ ×“×•"×— ×‘×™×¦×•×¢×™× ×©×œ×‘ ×¨××©×•×Ÿ
print("\nğŸ“ˆ ×“×•\"×— ×¡×™×•×•×’ - ×©×œ×‘ ×¨××©×•×Ÿ (×¤×¦×™×¢×” / ××™×Ÿ ×¤×¦×™×¢×”):")
print(classification_report(y_test1, y_pred1_threshold))

# ğŸ“Š ××˜×¨×™×¦×ª ×‘×œ×‘×•×œ - ×©×œ×‘ ×¨××©×•×Ÿ
plt.figure(figsize=(8,6))
sns.heatmap(confusion_matrix(y_test1, y_pred1_threshold), annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix - Stage 1 (Injury/No Injury)')
plt.show()

# ğŸ§¹ ×©×œ×‘ ××¢×‘×¨ ×œ×©×œ×‘ ×©× ×™: ×ª×¦×¤×™×•×ª ×©×—×–×™× ×• ××¦×œ× "×¤×¦×™×¢×”"
X_test_stage2 = X_test1[y_pred1_threshold == 1]
indexes_stage2 = X_test_stage2.index
y_true_stage2 = df.loc[indexes_stage2, 'Injury Severity']

# âœ¨ ×©×œ×‘ 2: ×‘× ×™×™×ª ×“××˜×” ×—×“×© ×¢× ××™×¤×•×™ ×œ-3 ×§×˜×’×•×¨×™×•×ª
injury_mapping = {
    'possible injury': 'light injury',
    'suspected minor injury': 'light injury',
    'suspected serious injury': 'serious injury',
    'fatal injury': 'fatal injury'
}

df_stage2 = df[df['Injury Severity'].isin(injury_mapping.keys())].copy()
df_stage2['Injury Mapped'] = df_stage2['Injury Severity'].map(injury_mapping)

# ×‘× ×™×™×ª X ×•-y
X_stage2 = df_stage2.drop(['Injury Severity', 'Injury Binary', 'Injury Mapped'], axis=1)
y_stage2 = df_stage2['Injury Mapped']


# ğŸ¯ One-Hot Encoding ×œ×¢××•×“×•×ª ×˜×§×¡×˜
X_stage2 = pd.get_dummies(X_stage2, columns=cat_cols, drop_first=True)

# ×§×™×“×•×“ y (×”×ª×•×¦××”) ×‘×¢×–×¨×ª LabelEncoder
le_y = LabelEncoder()
y_stage2_encoded = le_y.fit_transform(y_stage2)

# ×¤×™×¦×•×œ ×œ-Train/Test
X_train2, X_test2, y_train2, y_test2 = train_test_split(
    X_stage2, y_stage2_encoded, test_size=0.3, random_state=42, stratify=y_stage2_encoded
)

# SMOTE
smote2 = SMOTE(random_state=42)
X_res2, y_res2 = smote2.fit_resample(X_train2, y_train2)

# ğŸš€ ××™××•×Ÿ ××•×“×œ XGBoost ×œ×©×œ×‘ ×”×©× ×™
xgb_model_stage2 = xgb.XGBClassifier(
    n_estimators=300,
    max_depth=6,
    learning_rate=0.05,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42
)
xgb_model_stage2.fit(X_res2, y_res2)

# ğŸ”® ×—×™×–×•×™
y_pred2_encoded = xgb_model_stage2.predict(X_test2)

# ğŸ§  ×”×—×–×¨×ª ×”×ª×•×¦××•×ª ×œ×©××•×ª
y_pred2_labels = le_y.inverse_transform(y_pred2_encoded)
y_test2_labels = le_y.inverse_transform(y_test2)

# ğŸ“ˆ ×“×•"×— ×‘×™×¦×•×¢×™× ×©×œ×‘ ×©× ×™
print("\nğŸ“ˆ ×“×•\"×— ×¡×™×•×•×’ - ×©×œ×‘ ×©× ×™ (3 ×§×˜×’×•×¨×™×•×ª - light/serious/fatal ×¢× One-Hot):")
print(classification_report(y_test2_labels, y_pred2_labels))

# ğŸ“Š ××˜×¨×™×¦×ª ×‘×œ×‘×•×œ
plt.figure(figsize=(8,6))
sns.heatmap(confusion_matrix(y_test2_labels, y_pred2_labels, labels=le_y.classes_),
            annot=True, fmt='d', cmap='Blues',
            xticklabels=le_y.classes_, yticklabels=le_y.classes_)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix - Stage 2 (Mapped Injury Type with One-Hot Encoding)')
plt.show()

"""
> **××•×“×œ ×ª×œ×ª ×©×œ×‘×™**

*   ××•×“×œ 1: ×™×© ×¤×¦×™×¢×”/××™×Ÿ ×¤×¦×™×¢×”
*   ××•×“×œ ×©× ×™: ×¤×¦×™×¢×” ×§×˜×œ× ×™×ª/×œ× ×§×˜×œ× ×™×ª
*   ××•×“×œ 3: ×¤×¦×™×¢×” ×—××•×¨×”/ ×§×œ×”
"""

# âœ… ×©×™×¤×•×¨ ×›×•×œ×œ ×œ××•×“×œ XGBoost ×‘×©×œ×•×©×” ×©×œ×‘×™×: ×¤×¦×™×¢×” / ×§×˜×œ× ×™×ª / ×¨××ª ×—×•××¨×”

# Function to print comprehensive metrics
def print_metrics(y_true, y_pred, stage_name, set_type="Test"):
    print(f"\n{'='*50}")
    print(f"{stage_name} - {set_type} Set Metrics")
    print(f"{'='*50}")
    print(f"Accuracy: {accuracy_score(y_true, y_pred):.4f}")
    print(f"Precision: {precision_score(y_true, y_pred, average='weighted'):.4f}")
    print(f"Recall: {recall_score(y_true, y_pred, average='weighted'):.4f}")
    print(f"F1-Score: {f1_score(y_true, y_pred, average='weighted'):.4f}")
    print("\nDetailed Classification Report:")
    print(classification_report(y_true, y_pred))

# Function to plot confusion matrix
def plot_confusion_matrix(y_true, y_pred, title, labels=None):
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(8, 6))
    if labels:
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                   xticklabels=labels, yticklabels=labels)
    else:
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title(title)
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.tight_layout()
    plt.show()

# âœ¨ ×™×¦×™×¨×ª ×¢××•×“×ª ×¤×¦×™×¢×” / ××™×Ÿ ×¤×¦×™×¢×”
df = df.dropna(subset=['Injury Severity'])
df['Injury Binary'] = df['Injury Severity'].apply(lambda x: 0 if x == 'no apparent injury' else 1)

print(f"Dataset shape: {df.shape}")
print(f"Injury distribution: {df['Injury Binary'].value_counts()}")

# ğŸ”¹ ×©×œ×‘ ×¨××©×•×Ÿ â€“ ×¤×¦×™×¢×” / ××™×Ÿ ×¤×¦×™×¢×”
print("\n" + "="*60)
print("STAGE 1: INJURY / NO INJURY CLASSIFICATION")
print("="*60)

X = df.drop(['Injury Severity', 'Injury Binary'], axis=1)
y = df['Injury Binary']

# Preprocessing
X = pd.get_dummies(X, drop_first=True)
X.columns = X.columns.str.replace('[^A-Za-z0-9_]+', '_', regex=True)
X = X.drop(columns=X.select_dtypes(include=['datetime64']).columns)
X = X.astype(float)

print(f"Features shape: {X.shape}")
print(f"Class distribution before SMOTE: {y.value_counts()}")

# Train-test split
X_train1, X_test1, y_train1, y_test1 = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

# Apply SMOTE
smote1 = SMOTE(random_state=42)
X_train1_smote, y_train1_smote = smote1.fit_resample(X_train1, y_train1)
print(f"Class distribution after SMOTE: {pd.Series(y_train1_smote).value_counts()}")

# Calculate class weights
class_weights1 = compute_class_weight('balanced', classes=np.unique(y_train1), y=y_train1)
scale_pos_weight1 = class_weights1[1] / class_weights1[0]

# XGBoost Model 1
model1 = xgb.XGBClassifier(
    n_estimators=500,
    max_depth=10,
    learning_rate=0.03,
    scale_pos_weight=scale_pos_weight1,
    random_state=42,
    eval_metric='logloss',
    early_stopping_rounds=50
)

# Fit with evaluation set
model1.fit(
    X_train1_smote, y_train1_smote,
    eval_set=[(X_train1_smote, y_train1_smote), (X_test1, y_test1)],
    verbose=False
)

# Predictions
probs1 = model1.predict_proba(X_test1)[:, 1]
thresh1 = 0.25  # ×”×•×¨×“× ×• ××ª ×”×¡×£ ×›×“×™ ×œ×©×¤×¨ Recall ×œ×¤×¦×™×¢×•×ª
y_pred1_test = (probs1 >= thresh1).astype(int)

# Train predictions for comparison
y_pred1_train = model1.predict(X_train1_smote)

# Print metrics
print_metrics(y_train1_smote, y_pred1_train, "Stage 1", "Train")
print_metrics(y_test1, y_pred1_test, "Stage 1", "Test")

# Plot confusion matrices
plot_confusion_matrix(y_train1_smote, y_pred1_train,
                     "Stage 1: Train Set Confusion Matrix",
                     labels=['No Injury', 'Injury'])
plot_confusion_matrix(y_test1, y_pred1_test,
                     "Stage 1: Test Set Confusion Matrix",
                     labels=['No Injury', 'Injury'])


# ğŸ”¸ ×©×œ×‘ ×©× ×™ â€“ ×¤×¦×™×¢×” ×§×˜×œ× ×™×ª / ×œ× ×§×˜×œ× ×™×ª
print("\n" + "="*60)
print("STAGE 2: FATAL / NON-FATAL INJURY CLASSIFICATION")
print("="*60)

df_stage2 = df[df['Injury Binary'] == 1].copy()
df_stage2['Fatal Binary'] = df_stage2['Injury Severity'].apply(
    lambda x: 1 if x == 'fatal injury' else 0
)

print(f"Stage 2 dataset shape: {df_stage2.shape}")
print(f"Fatal injury distribution: {df_stage2['Fatal Binary'].value_counts()}")

X2 = df_stage2.drop(['Injury Severity', 'Injury Binary', 'Fatal Binary'], axis=1)
y2 = df_stage2['Fatal Binary']

# Preprocessing
X2 = pd.get_dummies(X2, drop_first=True)
X2.columns = X2.columns.str.replace('[^A-Za-z0-9_]+', '_', regex=True)
X2 = X2.drop(columns=X2.select_dtypes(include=['datetime64']).columns)
X2 = X2.astype(float)

# Train-test split
X_train2, X_test2, y_train2, y_test2 = train_test_split(
    X2, y2, test_size=0.3, random_state=42, stratify=y2
)

# Apply SMOTE with custom sampling strategy
fatal_count = y_train2.value_counts()[1] if 1 in y_train2.values else 0
smote2 = SMOTE(sampling_strategy={1: fatal_count * 4}, random_state=42)
X_train2_smote, y_train2_smote = smote2.fit_resample(X_train2, y_train2)
print(f"Class distribution after SMOTE: {pd.Series(y_train2_smote).value_counts()}")

# Calculate class weights
class_weights2 = compute_class_weight('balanced', classes=np.unique(y_train2), y=y_train2)
scale_pos_weight2 = class_weights2[1] / class_weights2[0] if len(class_weights2) > 1 else 1

# XGBoost Model 2
model2 = xgb.XGBClassifier(
    n_estimators=500,
    max_depth=10,
    learning_rate=0.03,
    scale_pos_weight=scale_pos_weight2,
    random_state=42,
    eval_metric='logloss',
    early_stopping_rounds=50
)

# Fit with evaluation set
model2.fit(
    X_train2_smote, y_train2_smote,
    eval_set=[(X_train2_smote, y_train2_smote), (X_test2, y_test2)],
    verbose=False
)

# Predictions
probs2 = model2.predict_proba(X_test2)[:, 1]
thresh2 = 0.3
y_pred2_test = (probs2 >= thresh2).astype(int)
y_pred2_train = model2.predict(X_train2_smote)

# Print metrics
print_metrics(y_train2_smote, y_pred2_train, "Stage 2", "Train")
print_metrics(y_test2, y_pred2_test, "Stage 2", "Test")

# Plot confusion matrices
plot_confusion_matrix(y_train2_smote, y_pred2_train,
                     "Stage 2: Train Set Confusion Matrix",
                     labels=['Non-Fatal', 'Fatal'])
plot_confusion_matrix(y_test2, y_pred2_test,
                     "Stage 2: Test Set Confusion Matrix",
                     labels=['Non-Fatal', 'Fatal'])

# ğŸ”» ×©×œ×‘ ×©×œ×™×©×™ â€“ ×§×œ×” / ×—××•×¨×”
print("\n" + "="*60)
print("STAGE 3: LIGHT / SERIOUS INJURY CLASSIFICATION")
print("="*60)

stage3_df = df_stage2[df_stage2['Injury Severity'].isin([
    'possible injury', 'suspected minor injury', 'suspected serious injury'
])].copy()

stage3_df['Severity'] = stage3_df['Injury Severity'].apply(
    lambda x: 'serious' if x == 'suspected serious injury' else 'light'
)

print(f"Stage 3 dataset shape: {stage3_df.shape}")
print(f"Severity distribution: {stage3_df['Severity'].value_counts()}")

X3 = stage3_df.drop(['Injury Severity', 'Injury Binary', 'Fatal Binary', 'Severity'], axis=1)
y3 = stage3_df['Severity']

# Label encoding
le3 = LabelEncoder()
y3_encoded = le3.fit_transform(y3)
print(f"Label mapping: {dict(zip(le3.classes_, le3.transform(le3.classes_)))}")

# Preprocessing
X3 = pd.get_dummies(X3, drop_first=True)
X3.columns = X3.columns.str.replace('[^A-Za-z0-9_]+', '_', regex=True)
X3 = X3.drop(columns=X3.select_dtypes(include=['datetime64']).columns)
X3 = X3.astype(float)

# Train-test split
X_train3, X_test3, y_train3, y_test3 = train_test_split(
    X3, y3_encoded, test_size=0.3, random_state=42, stratify=y3_encoded
)

# Apply SMOTE
serious_count = y_train3.tolist().count(1)
smote3 = SMOTE(sampling_strategy={1: serious_count * 3}, random_state=42)
X_train3_smote, y_train3_smote = smote3.fit_resample(X_train3, y_train3)
print(f"Class distribution after SMOTE: {pd.Series(y_train3_smote).value_counts()}")

# Calculate class weights
class_weights3 = compute_class_weight('balanced', classes=np.unique(y_train3), y=y_train3)
scale_pos_weight3 = class_weights3[1] / class_weights3[0] if len(class_weights3) > 1 else 1

# XGBoost Model 3
model3 = xgb.XGBClassifier(
    n_estimators=500,
    max_depth=10,
    learning_rate=0.03,
    scale_pos_weight=scale_pos_weight3,
    random_state=42,
    eval_metric='logloss',
    early_stopping_rounds=50
)

# Fit with evaluation set
model3.fit(
    X_train3_smote, y_train3_smote,
    eval_set=[(X_train3_smote, y_train3_smote), (X_test3, y_test3)],
    verbose=False
)

# Predictions
y_pred3_test = model3.predict(X_test3)
y_pred3_train = model3.predict(X_train3_smote)

# Convert back to original labels for reporting
y_train3_labels = le3.inverse_transform(y_train3_smote)
y_pred3_train_labels = le3.inverse_transform(y_pred3_train)
y_test3_labels = le3.inverse_transform(y_test3)
y_pred3_test_labels = le3.inverse_transform(y_pred3_test)

# Print metrics
print_metrics(y_train3_labels, y_pred3_train_labels, "Stage 3", "Train")
print_metrics(y_test3_labels, y_pred3_test_labels, "Stage 3", "Test")

# Plot confusion matrices
plot_confusion_matrix(y_train3_labels, y_pred3_train_labels,
                     "Stage 3: Train Set Confusion Matrix",
                     labels=['Light', 'Serious'])
plot_confusion_matrix(y_test3_labels, y_pred3_test_labels,
                     "Stage 3: Test Set Confusion Matrix",
                     labels=['Light', 'Serious'])

# ğŸ“Š Feature Importance Analysis
print("\n" + "="*60)
print("FEATURE IMPORTANCE ANALYSIS")
print("="*60)

def plot_feature_importance(model, feature_names, title, top_n=15):
    importance = model.feature_importances_
    indices = np.argsort(importance)[::-1][:top_n]

    plt.figure(figsize=(10, 8))
    plt.title(title)
    plt.bar(range(top_n), importance[indices])
    plt.xticks(range(top_n), [feature_names[i] for i in indices], rotation=45, ha='right')
    plt.tight_layout()
    plt.show()

    # Print top features
    print(f"\nTop {top_n} features for {title}:")
    for i, idx in enumerate(indices):
        print(f"{i+1:2d}. {feature_names[idx]}: {importance[idx]:.4f}")

# Plot feature importance for each stage
plot_feature_importance(model1, X.columns, "Stage 1: Injury/No Injury")
plot_feature_importance(model2, X2.columns, "Stage 2: Fatal/Non-Fatal")
plot_feature_importance(model3, X3.columns, "Stage 3: Light/Serious")

print("\n" + "="*60)
print("TRAINING COMPLETED SUCCESSFULLY!")
print("="*60)

"""Random Forest
---


"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, f1_score
from imblearn.over_sampling import SMOTE

""">**××•×“×œ ×™×© ×¤×¦×™×¢×”/××™×Ÿ ×¤×¦×™×¢×”**"""

# âœ¨ ×™×¦×™×¨×ª ×¢××•×“×ª ×¤×¦×™×¢×” / ××™×Ÿ ×¤×¦×™×¢×”
df['Injury Binary'] = df['Injury Severity'].apply(
    lambda x: 0 if x == 'no apparent injury' else 1
)

# ğŸ› ï¸ ×‘× ×™×™×ª X ×•-y ×œ×©×œ×‘ ×”×¨××©×•×Ÿ
X = df.drop(['Injury Severity', 'Injury Binary'], axis=1)
y_stage1 = df['Injury Binary']


# ğŸ¯ One-Hot Encoding
cat_cols = X.select_dtypes(include='object').columns.tolist()  # ğŸ’¡ ×”×•×¡×¤×” ×—×©×•×‘×”!
X = pd.get_dummies(X, columns=cat_cols, drop_first=True)


# âœ… × ×™×§×•×™ ×©××•×ª ×¢××•×“×•×ª
X.columns = X.columns.str.replace('[^A-Za-z0-9_]+', '_', regex=True)

# ×¤×™×¦×•×œ ×œ-Train/Test
X_train1, X_test1, y_train1, y_test1 = train_test_split(
    X, y_stage1, test_size=0.3, random_state=42, stratify=y_stage1
)

# SMOTE
smote = SMOTE(random_state=42)
X_res1, y_res1 = smote.fit_resample(X_train1, y_train1)

# ğŸš€ ××™××•×Ÿ ××•×“×œ Random Forest ×œ×©×œ×‘ ×”×¨××©×•×Ÿ
rf_model_stage1 = RandomForestClassifier(
    n_estimators=300,
    max_depth=15,
    random_state=42,
    class_weight='balanced'
)
rf_model_stage1.fit(X_res1, y_res1)

# ğŸ¯ ×—×™×–×•×™ ×¢× ×¡×£ ××•×ª××
y_probs1 = rf_model_stage1.predict_proba(X_test1)[:, 1]
threshold = 0.48
y_pred1_threshold = (y_probs1 >= threshold).astype(int)

# ğŸ“ˆ ×“×•"×— ×‘×™×¦×•×¢×™× ×©×œ×‘ ×¨××©×•×Ÿ
print("\nğŸ“ˆ ×“×•\"×— ×¡×™×•×•×’ - ×©×œ×‘ ×¨××©×•×Ÿ (×¤×¦×™×¢×” / ××™×Ÿ ×¤×¦×™×¢×” ×¢× Random Forest):")
print(classification_report(y_test1, y_pred1_threshold))

# ğŸ“Š ××˜×¨×™×¦×ª ×‘×œ×‘×•×œ - ×©×œ×‘ ×¨××©×•×Ÿ
plt.figure(figsize=(8,6))
sns.heatmap(confusion_matrix(y_test1, y_pred1_threshold), annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix - Stage 1 (Injury/No Injury with Random Forest)')
plt.show()

# ğŸ¯ ×—×™×–×•×™ ×¢× ×¡×£ ××•×ª××
y_probs1 = rf_model_stage1.predict_proba(X_test1)[:, 1]
threshold = 0.3
y_pred1_threshold = (y_probs1 >= threshold).astype(int)

# ğŸ“ˆ ×“×•"×— ×‘×™×¦×•×¢×™× ×©×œ×‘ ×¨××©×•×Ÿ
print("\nğŸ“ˆ ×“×•\"×— ×¡×™×•×•×’ - ×©×œ×‘ ×¨××©×•×Ÿ (×¤×¦×™×¢×” / ××™×Ÿ ×¤×¦×™×¢×” ×¢× Random Forest):")
print(classification_report(y_test1, y_pred1_threshold))

# ğŸ“Š ××˜×¨×™×¦×ª ×‘×œ×‘×•×œ - ×©×œ×‘ ×¨××©×•×Ÿ
plt.figure(figsize=(8,6))
sns.heatmap(confusion_matrix(y_test1, y_pred1_threshold), annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix - Stage 1 (Injury/No Injury with Random Forest)')
plt.show()

""">**××•×“×œ 5 ×§×˜×’×¨×•×™×•×ª**"""

# ===============================
# 1. ×”×›× ×ª ×¢××•×“×ª ××˜×¨×” ×•×¤×™×¦'×¨×™×
# ===============================
target_column = 'Injury Severity'
X = df.drop(columns=[target_column])
y = df[target_column]


# ×”××¨×ª ×¢××•×“×•×ª ×§×˜×’×•×¨×™××œ×™×•×ª
for col in X.columns:
    if X[col].dtype == 'object':
        X[col] = X[col].astype('category')

# ×§×™×“×•×“ ×¢××•×“×ª ××˜×¨×”
le = LabelEncoder()
y = le.fit_transform(y)

# ===============================
# 2. ×¤×™×¦×•×œ ×¨××©×•× ×™ ×œ-Train/Test
# ===============================
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.3,
    random_state=42,
    stratify=y
)

# ×§×™×“×•×“ ×œ×§×˜×’×•×¨×™×•×ª ×›-int
X_train_encoded = X_train.copy()
for col in X_train_encoded.select_dtypes(['category']).columns:
    X_train_encoded[col] = X_train_encoded[col].cat.codes

X_test_encoded = X_test.copy()
for col in X_test_encoded.select_dtypes(['category']).columns:
    X_test_encoded[col] = X_test_encoded[col].cat.codes


# ===============================
# 3. SMOTE ×œ××™×–×•×Ÿ ×¡×˜ ×”××™××•×Ÿ
# ===============================
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train_encoded, y_train)

# ===============================
# 4. ××™××•×Ÿ ××•×“×œ Random Forest ×‘×¡×™×¡×™
# ===============================
model = RandomForestClassifier(random_state=42, n_jobs=-1)
model.fit(X_train_resampled, y_train_resampled)

# ===============================
# 5. ××“×“×™ ×‘×™×¦×•×¢ ×œ×¤× ×™ ×¡×™× ×•×Ÿ
# ===============================
y_pred = model.predict(X_test_encoded)

print("\nğŸ”µ ×ª×•×¦××•×ª ×œ×¤× ×™ ×¡×™× ×•×Ÿ ×¤×™×¦'×¨×™×:\n")
print(classification_report(y_test, y_pred, target_names=le.classes_))

macro_f1_before = f1_score(y_test, y_pred, average='macro')
print(f"\nğŸ¯ Macro F1 ×œ×¤× ×™ ×¡×™× ×•×Ÿ: {macro_f1_before:.4f}")

# ===============================
# 6. Feature Importance
# ===============================
importances = model.feature_importances_
feature_names = X_train_encoded.columns

feature_importance_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': importances
}).sort_values(by='Importance', ascending=False)

print("\nğŸ”µ ×˜×‘×œ×ª ×—×©×™×‘×•×ª ×¤×™×¦'×¨×™× (××”×›×™ ×—×©×•×‘ ×œ×¤×—×•×ª ×—×©×•×‘):\n")
print(feature_importance_df.head(30))

# ===============================
# 7. ×¡×™× ×•×Ÿ ×¤×™×¦'×¨×™× ×—×œ×©×™× ×œ×¤×™ ×¡×£
# ===============================
threshold = 0.01

features_to_drop_df = feature_importance_df[feature_importance_df['Importance'] < threshold]
features_to_drop = features_to_drop_df['Feature'].tolist()

print(f"\nğŸš® × ××¦××• {len(features_to_drop)} ×¤×™×¦'×¨×™× ×œ×”×¡×¨×” (××ª×—×ª ×œ-{threshold}):\n")
print(features_to_drop_df)

# ===============================
# 8. ×¡×™× ×•×Ÿ ×•×™×¦×™×¨×ª ×“××˜×” ×—×“×©
# ===============================
X_filtered = X.drop(columns=features_to_drop)

# ===============================
# 9. ×¤×™×¦×•×œ ××—×“×© ×œ-Train/Test
# ===============================
X_train, X_test, y_train, y_test = train_test_split(
    X_filtered, y,
    test_size=0.3,
    random_state=42,
    stratify=y
)

# ×§×™×“×•×“ ××—×“×©
X_train_encoded = X_train.copy()
for col in X_train_encoded.select_dtypes(['category']).columns:
    X_train_encoded[col] = X_train_encoded[col].cat.codes


X_test_encoded = X_test.copy()
for col in X_test_encoded.select_dtypes(['category']).columns:
    X_test_encoded[col] = X_test_encoded[col].cat.codes


# ===============================
# 10. SMOTE ××—×“×© ××—×¨×™ ×¡×™× ×•×Ÿ
# ===============================
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train_encoded, y_train)

# ===============================
# 11. ××™××•×Ÿ ××•×“×œ Random Forest ××—×¨×™ ×¡×™× ×•×Ÿ
# ===============================
model_filtered = RandomForestClassifier(random_state=42, n_jobs=-1)
model_filtered.fit(X_train_resampled, y_train_resampled)

# ===============================
# 12. ×—×™×–×•×™ ×•××“×“×™× ××—×¨×™ ×¡×™× ×•×Ÿ
# ===============================
y_pred_filtered = model_filtered.predict(X_test_encoded)

print("\nğŸ”µ ×ª×•×¦××•×ª ××—×¨×™ ×¡×™× ×•×Ÿ ×¤×™×¦'×¨×™×:\n")
print(classification_report(y_test, y_pred_filtered, target_names=le.classes_))

macro_f1_after = f1_score(y_test, y_pred_filtered, average='macro')
print(f"\nğŸ¯ Macro F1 ××—×¨×™ ×¡×™× ×•×Ÿ: {macro_f1_after:.4f}")

# ===============================
# 13. ×”×©×•×•××ª Macro F1 ×¡×•×¤×™×ª
# ===============================
print("\nğŸ“Š ×”×©×•×•××ª Macro F1:\n")
print(f"ğŸ”µ ×œ×¤× ×™ ×¡×™× ×•×Ÿ: {macro_f1_before:.4f}")
print(f"ğŸŸ¢ ××—×¨×™ ×¡×™× ×•×Ÿ: {macro_f1_after:.4f}")

"""> **××•×“×œ: 3 ×§×˜×’×•×¨×™×•×ª - ××™×Ÿ ×¤×¦×™×¢×”/×¤×¦×™×¢×” ×§×œ×”/×¤×¦×™×¢×” ×—××•×¨×”**

"""

# ===============================
# 1. ××™×¤×•×™ Injury Severity ×œÖ¾3 ×§×˜×’×•×¨×™×•×ª
# ===============================
injury_mapping = {
    'no apparent injury': 'no injury',
    'possible injury': 'minor injury',
    'suspected minor injury': 'minor injury',
    'suspected serious injury': 'severe injury',
    'fatal injury': 'severe injury'
}

df = df[df['Injury Severity'].isin(injury_mapping.keys())]
df['Injury Severity'] = df['Injury Severity'].map(injury_mapping)


# ===============================
# 2. ×”×›× ×ª ×¢××•×“×ª ××˜×¨×” ×•×¤×™×¦'×¨×™×
# ===============================
target_column = 'Injury Severity'
X = df.drop(columns=[target_column])
y = df[target_column]



# ×”××¨×ª ×¢××•×“×•×ª ×§×˜×’×•×¨×™××œ×™×•×ª ×œÖ¾category
for col in X.columns:
    if X[col].dtype == 'object':
        X[col] = X[col].astype('category')

# ×§×™×“×•×“ ×¢××•×“×ª ××˜×¨×”
le = LabelEncoder()
y = le.fit_transform(y)

# ===============================
# 3. ×¤×™×¦×•×œ ×œÖ¾Train/Test
# ===============================
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.3,
    random_state=42,
    stratify=y
)

# ×§×™×“×•×“ ×§×˜×’×•×¨×™×•×ª ×›Ö¾int
X_train_encoded = X_train.copy()
for col in X_train_encoded.select_dtypes(['category']).columns:
    X_train_encoded[col] = X_train_encoded[col].cat.codes

X_test_encoded = X_test.copy()
for col in X_test_encoded.select_dtypes(['category']).columns:
    X_test_encoded[col] = X_test_encoded[col].cat.codes

# ===============================
# 4. SMOTE ×¢×œ ×¡×˜ ×”××™××•×Ÿ
# ===============================
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train_encoded, y_train)

# ===============================
# 5. ××™××•×Ÿ ××•×“×œ ×¢× class_weight='balanced'
# ===============================
model = RandomForestClassifier(
    random_state=42,
    n_jobs=-1,
    class_weight='balanced'
)
model.fit(X_train_resampled, y_train_resampled)

# ===============================
# 6. ×—×™×–×•×™ ×•×”×¢×¨×›×ª ×‘×™×¦×•×¢×™×
# ===============================
y_pred = model.predict(X_test_encoded)

print("\nğŸ”µ ×ª×•×¦××•×ª ×”××•×“×œ (SMOTE + class_weight=balanced):\n")
print(classification_report(y_test, y_pred, target_names=le.classes_))

macro_f1 = f1_score(y_test, y_pred, average='macro')
print(f"\nğŸ¯ Macro F1 (×©×™×œ×•×‘ SMOTE + class_weight): {macro_f1:.4f}")

"""> **××•×“×œ ×“×•-×©×œ×‘×™**

*   ×©×œ×‘ 1: ×™×© ×¤×¦×™×¢×”/××™×Ÿ ×¤×¦×™×¢×”
*   ×©×œ×‘ 2: ×¤×¦×™×¢×” ×§×˜×œ× ×™×ª/×§×œ×”/×§×©×”

"""

# âœ¨ ×™×¦×™×¨×ª ×¢××•×“×ª ×¤×¦×™×¢×” / ××™×Ÿ ×¤×¦×™×¢×”
df['Injury Binary'] = df['Injury Severity'].apply(
    lambda x: 0 if x == 'no apparent injury' else 1
)

# ğŸ› ï¸ ×‘× ×™×™×ª X ×•-y ×œ×©×œ×‘ ×”×¨××©×•×Ÿ
X = df.drop(['Injury Severity', 'Injury Binary'], axis=1)
y_stage1 = df['Injury Binary']


# ğŸ¯ One-Hot Encoding
cat_cols = X.select_dtypes(include='object').columns.tolist()  # ğŸ’¡ ×”×•×¡×¤×” ×—×©×•×‘×”!
X = pd.get_dummies(X, columns=cat_cols, drop_first=True)


# âœ… × ×™×§×•×™ ×©××•×ª ×¢××•×“×•×ª
X.columns = X.columns.str.replace('[^A-Za-z0-9_]+', '_', regex=True)

# ×¤×™×¦×•×œ ×œ-Train/Test
X_train1, X_test1, y_train1, y_test1 = train_test_split(
    X, y_stage1, test_size=0.3, random_state=42, stratify=y_stage1
)

# SMOTE
smote = SMOTE(random_state=42)
X_res1, y_res1 = smote.fit_resample(X_train1, y_train1)

# ğŸš€ ××™××•×Ÿ ××•×“×œ Random Forest ×œ×©×œ×‘ ×”×¨××©×•×Ÿ
rf_model_stage1 = RandomForestClassifier(
    n_estimators=300,
    max_depth=15,
    random_state=42,
    class_weight='balanced'
)
rf_model_stage1.fit(X_res1, y_res1)

# ğŸ¯ ×—×™×–×•×™ ×¢× ×¡×£ ××•×ª××
y_probs1 = rf_model_stage1.predict_proba(X_test1)[:, 1]
threshold = 0.3
y_pred1_threshold = (y_probs1 >= threshold).astype(int)

# ğŸ“ˆ ×“×•"×— ×‘×™×¦×•×¢×™× ×©×œ×‘ ×¨××©×•×Ÿ
print("\nğŸ“ˆ ×“×•\"×— ×¡×™×•×•×’ - ×©×œ×‘ ×¨××©×•×Ÿ (×¤×¦×™×¢×” / ××™×Ÿ ×¤×¦×™×¢×” ×¢× Random Forest):")
print(classification_report(y_test1, y_pred1_threshold))

# ğŸ“Š ××˜×¨×™×¦×ª ×‘×œ×‘×•×œ - ×©×œ×‘ ×¨××©×•×Ÿ
plt.figure(figsize=(8,6))
sns.heatmap(confusion_matrix(y_test1, y_pred1_threshold), annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix - Stage 1 (Injury/No Injury with Random Forest)')
plt.show()

# ğŸ§¹ ×”×›× ×” ×œ×©×œ×‘ ×”×©× ×™: ×ª×¦×¤×™×•×ª ×©×—×–×™× ×• ××¦×œ×Ÿ "×¤×¦×™×¢×”"
X_test_stage2 = X_test1[y_pred1_threshold == 1]
indexes_stage2 = X_test_stage2.index
y_true_stage2 = df.loc[indexes_stage2, 'Injury Severity']

# âœ¨ ×‘× ×™×™×ª ×“××˜×” ×—×“×© ×¢× ××™×¤×•×™ ×œ-3 ×§×˜×’×•×¨×™×•×ª
injury_mapping = {
    'possible injury': 'light injury',
    'suspected minor injury': 'light injury',
    'suspected serious injury': 'serious injury',
    'fatal injury': 'fatal injury'
}

df_stage2 = df[df['Injury Severity'].isin(injury_mapping.keys())].copy()
df_stage2['Injury Mapped'] = df_stage2['Injury Severity'].map(injury_mapping)

# ×‘× ×™×™×ª X ×•-y ×œ×©×œ×‘ ×©× ×™
X_stage2 = df_stage2.drop(['Injury Severity', 'Injury Binary', 'Injury Mapped'], axis=1)
y_stage2 = df_stage2['Injury Mapped']

# ×˜×™×¤×•×œ ×‘×¢×¨×›×™× ×—×¡×¨×™×
cat_cols_stage2 = X_stage2.select_dtypes(include=['object']).columns
num_cols_stage2 = X_stage2.select_dtypes(include=['float64', 'int64']).columns

for col in num_cols_stage2:
    X_stage2[col] = X_stage2[col].fillna(X_stage2[col].median())
for col in cat_cols_stage2:
    X_stage2[col] = X_stage2[col].fillna('Unknown')

# ğŸ¯ One-Hot Encoding
X_stage2 = pd.get_dummies(X_stage2, columns=cat_cols_stage2, drop_first=True)

# âœ… × ×™×§×•×™ ×©××•×ª ×¢××•×“×•×ª
X_stage2.columns = X_stage2.columns.str.replace('[^A-Za-z0-9_]+', '_', regex=True)

# ×§×™×“×•×“ y
le_y = LabelEncoder()
y_stage2_encoded = le_y.fit_transform(y_stage2)

# ×¤×™×¦×•×œ ×œ-Train/Test
X_train2, X_test2, y_train2, y_test2 = train_test_split(
    X_stage2, y_stage2_encoded, test_size=0.3, random_state=42, stratify=y_stage2_encoded
)

# SMOTE
smote2 = SMOTE(random_state=42)
X_res2, y_res2 = smote2.fit_resample(X_train2, y_train2)

# ğŸš€ ××™××•×Ÿ ××•×“×œ Random Forest ×œ×©×œ×‘ ×”×©× ×™
rf_model_stage2 = RandomForestClassifier(
    n_estimators=300,
    max_depth=15,
    random_state=42,
    class_weight='balanced'
)
rf_model_stage2.fit(X_res2, y_res2)

# ğŸ”® ×—×™×–×•×™
y_pred2_encoded = rf_model_stage2.predict(X_test2)

# ğŸ§  ×”×—×–×¨×ª ×”×ª×•×¦××•×ª ×œ×©××•×ª
y_pred2_labels = le_y.inverse_transform(y_pred2_encoded)
y_test2_labels = le_y.inverse_transform(y_test2)

# ğŸ“ˆ ×“×•"×— ×‘×™×¦×•×¢×™× ×©×œ×‘ ×©× ×™
print("\nğŸ“ˆ ×“×•\"×— ×¡×™×•×•×’ - ×©×œ×‘ ×©× ×™ (3 ×§×˜×’×•×¨×™×•×ª - light/serious/fatal ×¢× Random Forest):")
print(classification_report(y_test2_labels, y_pred2_labels))

# ğŸ“Š ××˜×¨×™×¦×ª ×‘×œ×‘×•×œ ×©×œ×‘ ×©× ×™
plt.figure(figsize=(8,6))
sns.heatmap(confusion_matrix(y_test2_labels, y_pred2_labels, labels=le_y.classes_),
            annot=True, fmt='d', cmap='Blues',
            xticklabels=le_y.classes_, yticklabels=le_y.classes_)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix - Stage 2 (Mapped Injury Type with Random Forest)')
plt.show()

"""**×¨×’×¨×¡×™×” ×œ×•×’×™×¡×˜×™×ª**
---


"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import classification_report, f1_score
from imblearn.over_sampling import SMOTE

""">**××•×“×œ 5 ×§×˜×’×¨×•×™×•×ª**"""

# ===============================
# 1. ×”×›× ×ª ×¢××•×“×ª ××˜×¨×” ×•×¤×™×¦'×¨×™×
# ===============================

target_column = 'Injury Severity'

X = df.drop(columns=[target_column])
y = df[target_column]


# ×”××¨×ª ×§×˜×’×•×¨×™×•×ª
for col in X.columns:
    if X[col].dtype == 'object':
        X[col] = X[col].astype('category')


# ×§×™×“×•×“ ×¢××•×“×ª ×”××˜×¨×”
le = LabelEncoder()
y = le.fit_transform(y)

# ===============================
# 2. ×¤×™×¦×•×œ ×œ-Train/Test
# ===============================

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

# ===============================
# 3. ×§×™×“×•×“ ×œ-SMOTE
# ===============================

X_train_encoded = X_train.copy()
for col in X_train_encoded.select_dtypes(['category']).columns:
    X_train_encoded[col] = X_train_encoded[col].cat.codes

# SMOTE
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train_encoded, y_train)

# ===============================
# 4. Standardization
# ===============================

scaler = StandardScaler()
X_train_resampled = scaler.fit_transform(X_train_resampled)

X_test_encoded = X_test.copy()
for col in X_test_encoded.select_dtypes(['category']).columns:
    X_test_encoded[col] = X_test_encoded[col].cat.codes

X_test_encoded = scaler.transform(X_test_encoded)

# ===============================
# 5. ×‘× ×™×™×ª ××•×“×œ Logistic Regression
# ===============================

model = LogisticRegression(
    max_iter=2000,         # ×œ× ××•×’×–× ×›×“×™ ×œ× ×œ×¨×•×¥ 100 ×©× ×”
    random_state=42,
    class_weight='balanced',
    solver='lbfgs'
)

model.fit(X_train_resampled, y_train_resampled)

# ===============================
# 6. ×—×™×–×•×™ ×•×‘×“×™×§×ª ×‘×™×¦×•×¢×™×
# ===============================

y_pred = model.predict(X_test_encoded)

# ×“×•×— ××¤×•×¨×˜ ×œ×›×œ ×”×§×˜×’×•×¨×™×•×ª
print("\nğŸ”µ ×“×•×— ×‘×™×¦×•×¢×™× ×¢×œ ×”×¡×˜:")
print(classification_report(
    y_test,
    y_pred,
    target_names=le.classes_
))

# Macro F1 ×›×œ×œ×™
macro_f1 = f1_score(
    y_test,
    y_pred,
    average='macro'
)

print(f"\nğŸ¯ Macro F1 Score ×›×•×œ×œ: {macro_f1:.4f}")

"""# ×’×¨×¤×™× ×œ×”×¢×¨×›×ª ×”×ª×•×¦××•×ª"""

# ğŸ¯ Create performance DataFrame
df = pd.DataFrame({
    'Model': ['LightGBM', 'XGBoost', 'CatBoost', 'Random Forest'],
    'Accuracy': [0.88, 0.86, 0.87, 0.87]
})

# ğŸ¨ Plotting
plt.figure(figsize=(8, 5))
sns.set_palette("Blues")
ax = sns.barplot(x='Model', y='Accuracy', data=df)

# â• Add values on top of bars
for i, row in df.iterrows():
    ax.text(i, row['Accuracy'] + 0.015, f"{row['Accuracy']:.2f}",
            ha='center', va='bottom', fontsize=10)

# ğŸ–¼ï¸ Formatting
plt.title('Accuracy per Model (Injury vs No Injury)', fontsize=14)
plt.ylabel("Accuracy", fontsize=12)
plt.xlabel("Model", fontsize=12)
plt.ylim(0, 1.05)
plt.grid(axis='y')
plt.tight_layout()

# ğŸ’¾ Optional save
plt.savefig("accuracy_by_model_labeled.png", dpi=300)

# ğŸ“Š Show plot
plt.show()

# ğŸ¯ Create DataFrame
df = pd.DataFrame({
    'Model': ['LightGBM', 'XGBoost', 'CatBoost', 'Random Forest'],
    'F1_Injury': [0.74, 0.72, 0.73, 0.71],
    'Recall_Injury': [0.95, 0.97, 0.96, 0.94],
    'Precision_Injury': [0.6, 0.57, 0.59, 0.58],
    'F1_No_Injury': [0.92, 0.91, 0.92, 0.91],
    'Recall_No_Injury': [0.86, 0.84, 0.85, 0.85],
    'Precision_No_Injury': [0.99, 0.99, 0.99, 0.98]
})

# ğŸŸ¦ Long format for each group
df_injury = df[['Model', 'F1_Injury', 'Recall_Injury', 'Precision_Injury']].melt(id_vars='Model', var_name='Metric', value_name='Score')
df_noinjury = df[['Model', 'F1_No_Injury', 'Recall_No_Injury', 'Precision_No_Injury']].melt(id_vars='Model', var_name='Metric', value_name='Score')

# ğŸ“Š Plot Injury Metrics
plt.figure(figsize=(8, 5))
sns.set_palette("Blues")
ax1 = sns.barplot(data=df_injury, x='Model', y='Score', hue='Metric')
plt.title('Injury Class - Model Performance')
plt.ylim(0, 1.05)
plt.ylabel("Score")
plt.xlabel("Model")
plt.legend(title="Metric", bbox_to_anchor=(1.02, 1), loc='upper left')
plt.grid(axis='y')

# â• Add text labels
for container in ax1.containers:
    ax1.bar_label(container, fmt='%.2f', label_type='edge', fontsize=9)

plt.tight_layout()
plt.savefig("injury_metrics_by_model_labeled.png", dpi=300)
plt.show()

# ğŸ“Š Plot No Injury Metrics
plt.figure(figsize=(8, 5))
sns.set_palette("Blues_d")
ax2 = sns.barplot(data=df_noinjury, x='Model', y='Score', hue='Metric')
plt.title('No Injury Class - Model Performance')
plt.ylim(0, 1.05)
plt.ylabel("Score")
plt.xlabel("Model")
plt.legend(title="Metric", bbox_to_anchor=(1.02, 1), loc='upper left')
plt.grid(axis='y')

# â• Add text labels
for container in ax2.containers:
    ax2.bar_label(container, fmt='%.2f', label_type='edge', fontsize=9)

plt.tight_layout()
plt.savefig("no_injury_metrics_by_model_labeled.png", dpi=300)
plt.show()