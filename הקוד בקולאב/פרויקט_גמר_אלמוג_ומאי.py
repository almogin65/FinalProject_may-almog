# -*- coding: utf-8 -*-
"""פרויקט גמר- אלמוג ומאי.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GGaWmwAiu0n8j2iQsnSmNIbNggac8Nvm

pip installs

---
"""

# # נעשה אזורים למיקום על ידי קו אורך וקו רוחב
# # שהופכת את הקו אורך והקו גובה לאזורים h3 לכן נשתמש  בספרייה
!pip install h3

# התקנת ספריית KeplerGL ליצירת מפות אינטראקטיביות
!pip install keplergl

# התקנת ספריית RapidFuzz לניקוי טקסטים והשוואות מחרוזות (Fuzzy Matching)
!pip install rapidfuzz

#עבור מודל catboost
!pip install catboost

"""imports

---


"""

# ניהול ועיבוד נתונים
import pandas as pd
import numpy as np
import re

# ויזואליזציה וניתוח גרפים
import matplotlib.pyplot as plt
import seaborn as sns
import folium
from folium.plugins import HeatMap
from keplergl import KeplerGl
from sklearn.metrics import ConfusionMatrixDisplay

# סטטיסטיקה ובדיקות
import scipy.stats as stats
from scipy.stats import chi2_contingency, f_oneway

# עיבוד טקסט (ניקוי שמות/מותגים וכו')
from rapidfuzz import fuzz, process

# עיבוד מיקום (Geo encoding)
import h3

"""טעינת הנתונים

---


"""

!file -i /content/Crash_Reporting_-_Drivers_Data.csv

# צייני את הנתיב לקובץ שלך
file_path = "/content/Crash_Reporting_-_Drivers_Data.csv"

# נסי לטעון את הקובץ עם הקידוד 'utf-8'
try:
    df = pd.read_csv(file_path, encoding='utf-8')
    print("טעינת הקובץ הצליחה עם utf-8")
except UnicodeDecodeError:
    print("שגיאת קידוד! מנסים קידוד אחר...")

    # נסי קידוד נוסף במידה ו-utf-8 לא עובד
    try:
        df = pd.read_csv(file_path, encoding='iso-8859-1')
        print("טעינת הקובץ הצליחה עם iso-8859-1")
    except Exception as e:
        print(f"שגיאה: {e}")

"""# ניתוח ראשוני של המשתנים
(Data understanding report)

---


"""

#שיכפול הדאטה המקורית עבור ניתוח נתונים ראשוני
df_analysis = df.copy()

df_analysis.head()

df_analysis.info()

#העתק של הדאטה עבור טבלה אחת בהמשך הניתוח
df_original = df.copy()

# ספירת הערכים הייחודיים בכל עמודה
unique_values = df_analysis.nunique()

# יצירת טבלה מסודרת
unique_summary = pd.DataFrame({
    'Unique Values': unique_values
}).sort_values(by='Unique Values', ascending=False)

print(unique_summary)

# חישוב מספר הערכים החסרים בכל עמודה לפני שינויים
missing_values = df_analysis.isnull().sum()

# חישוב אחוז הערכים החסרים בכל עמודה
missing_percentage = (missing_values / len(df_analysis)) * 100

# חדש DataFrame שילוב התוצאות ב-
missing_summary = pd.DataFrame({
    'Missing Values': missing_values,
    'Percentage Missing': missing_percentage
})

# סינון עמודות עם ערכים חסרים בלבד
missing_summary = missing_summary[missing_summary['Missing Values'] > 0]

# הצגת התוצאות
print(missing_summary)

#יצירת גרף של כמות הערכים החסרים באחוזים
plt.figure(figsize=(10, 6))
sns.barplot(x=missing_summary.index, y='Percentage Missing', data=missing_summary, palette='coolwarm')
plt.title('Percentage of Missing Values by Column', fontsize=16)
plt.xlabel('Columns', fontsize=14)
plt.ylabel('Percentage Missing', fontsize=14)
plt.xticks(rotation=45, ha='right')
plt.show()

# או ערכים ריקים, רווחים - לערכים חסרים "Unknown"/"NA" המרת
df_analysis.replace(["", " ", "Unknown", "NA"], np.nan, inplace=True)

# חישוב הערכים החסרים מחדש
missing_values = df_analysis.isnull().sum()
missing_percentage = (missing_values / len(df_analysis)) * 100

missing_summary = pd.DataFrame({
    'Missing Values': missing_values,
    'Percentage Missing': missing_percentage
})

# סינון ערכים חסרים
missing_summary = missing_summary[missing_summary['Missing Values'] > 0]
print(missing_summary)

"""הצגת כמות הופעות של ערכים ייחודים בעמודות שונות"""

# פונקציה לספירת ערכים ייחודיים בעמודה
def count_values(df_analysis, column_name, top_n=None):
    counts = df_analysis[column_name].value_counts()
    if top_n:
        counts = counts.head(top_n)
    return counts

#Injury Severity מספר ההופעת של הערכים  הייחודים בעמודה
count_values(df_analysis, "Injury Severity")

#Drivers License State מספר ההופעת של הערכים  הייחודים בעמודה
count_values(df_analysis, "Drivers License State")

#Vehicle First Impact Location מספר ההופעת של הערכים הייחודים בעמודה
count_values(df_analysis, "Vehicle First Impact Location")

#ACRS Report Type מספר ההופעת של הערכים  הייחודים בעמודה
count_values(df_analysis, "ACRS Report Type")

#Route Type מספר ההופעת של הערכים  הייחודים בעמודה
count_values(df_analysis, "Route Type")

#Road Name מספר ההופעת של הערכים  הייחודים בעמודה
count_values(df_analysis, "Road Name")

#Cross-Street Name מספר ההופעת של הערכים  הייחודים בעמודה
count_values(df_analysis, "Cross-Street Name")

#Off-Road Description מספר ההופעת של הערכים  הייחודים בעמודה
count_values(df_analysis, "Off-Road Description")

#Municipality מספר ההופעת של הערכים  הייחודים בעמודה
count_values(df_analysis, "Municipality")

#Related Non-Motorist מספר ההופעת של הערכים  הייחודים בעמודה
count_values(df_analysis, "Related Non-Motorist")

#Collision Type מספר ההופעת של הערכים  הייחודים בעמודה
count_values(df_analysis, "Collision Type")

#Weather מספר ההופעת של הערכים  הייחודים בעמודה
count_values(df_analysis, "Weather")

#Surface Condition מספר ההופעת של הערכים  הייחודים בעמודה
count_values(df_analysis, "Surface Condition")

#Light מספר ההופעת של הערכים  הייחודים בעמודה
count_values(df_analysis, "Light")

#Traffic Control מספר ההופעת של הערכים  הייחודים בעמודה
count_values(df_analysis, "Traffic Control")

#Driver Substance Abuse מספר ההופעת של הערכים  הייחודים בעמודה
count_values(df_analysis, "Driver Substance Abuse")

#Driver At Fault מספר ההופעת של הערכים  הייחודים בעמודה
count_values(df_analysis, "Driver At Fault")

#Circumstance מספר ההופעת של הערכים  הייחודים בעמודה
count_values(df_analysis, "Circumstance")

#Crash Date/Time מספר ההופעת של הערכים  הייחודים בעמודה
count_values(df_analysis, "Crash Date/Time")

df_analysis.info()

"""המרת כל העמודות הקטגוריאליות לאותיות קטנות"""

# המרת כל הערכים של העמודות הקטגוריאליות לאותיות קטנות כך שאם יש כפילויות יתאחדו

for col in df_analysis.columns:
  if df_analysis[col].dtype == 'object': # categorical columns
    df_analysis[col] = df_analysis[col].str.lower()

df_analysis['Injury Severity'].value_counts()

"""**ניתוח ראשוני כולל גרפים**

---


"""

# פונקציה לגרף המראה את התפלגות התאונות בעמודות השונות
def plot_countplot(df_analysis, column_name, title=None, show_counts=True, top_n=None):
    # 📝 הדפסת מספר הערכים הייחודיים לפני הגרף
    unique_vals = df_analysis[column_name].nunique()
    print(f"🔹 מספר ערכים ייחודיים בעמודה '{column_name}': {unique_vals}")
    print(" ")
    print(" ")

    # סדר הערכים תמיד מהגדול לקטן (ברירת מחדל של value_counts)
    order = df_analysis[column_name].value_counts().index
    if top_n:
        order = order[:top_n]

    # ציור הגרף
    plt.figure(figsize=(15, 9))
    sns.countplot(x=column_name, data=df_analysis, order=order, palette='viridis')

    # כותרות וצירים
    plt.title(title if title else f"{column_name} Distribution", fontsize=16)
    plt.xlabel(column_name, fontsize=14)
    plt.ylabel("Count", fontsize=14)
    plt.xticks(rotation=45, ha='right')

    # הוספת מספרים מעל העמודות
    if show_counts:
        for p in plt.gca().patches:
            plt.gca().annotate(f'{p.get_height()}',
                               (p.get_x() + p.get_width() / 2., p.get_height()),
                               ha='center', va='center', xytext=(0, 5),
                               textcoords='offset points', fontsize=10)

    plt.tight_layout()
    plt.show()

# גרף שכיחויות ל-Injury Severity
plot_countplot(df_analysis, "Injury Severity")

#Weather
#'raining' & 'rain' -איחוד קטגוריות בעלות משמעות דומה
df_analysis['Weather'] = df_analysis['Weather'].replace('raining', 'rain')

#Weather גרף שכיחויות ל
plot_countplot(df_analysis, "Weather")

#חישוב אחוז התאונות עבור כל תנאי מזג האוויר, שבהן הערך בעמודת חומרת הפציעה שונה מ־'ללא פציעה נראית לעין'.

weather_summary = df_analysis.groupby('Weather')['Injury Severity'].apply(lambda x: (x != 'no apparent injury').sum() / len(x) * 100)
row_counts = df_analysis["Weather"].value_counts()

#איחוד כל התוצאות לטבלה אחת (DataFrame)
result_df = pd.DataFrame({
    'Percentage Not "No Apparent Injury"': weather_summary,
    'Row Counts': row_counts
})

result_df

#Surface Condition גרף שכיחויות ל
plot_countplot(df_analysis, "Surface Condition")

#חישוב אחוז התאונות עבור כל ,מצב פני השטח שבהן הערך בעמודת חומרת הפציעה שונה מ־'ללא פציעה נראית לעין'.

surface_condition_summary = df_analysis.groupby('Surface Condition')['Injury Severity'].apply(lambda x: (x != 'no apparent injury').sum() / len(x) * 100)
row_counts = df_analysis.groupby('Surface Condition')['Injury Severity'].count()

#איחוד כל התוצאות לטבלה אחת (DataFrame)
result_df = pd.DataFrame({
    'Percentage Not "No Apparent Injury"': surface_condition_summary,
    'Row Counts': row_counts
})

result_df

#Collision Type גרף שכיחויות ל
plot_countplot(df_analysis, "Collision Type")

#Collision Type גרף שכיחויות ל
plot_countplot(df_analysis, "Collision Type")

#Vehicle Damage Extent גרף שכיחויות ל
plot_countplot(df_analysis, "Vehicle Damage Extent")

#חישוב אחוז התאונות עבור כל הקטגוריות בעמודת היקף הנזק לרכב, שבהן הערך בעמודת חומרת הפציעה שונה מ־'ללא פציעה נראית לעין'.
vehicle_damage_summary = df_analysis.groupby('Vehicle Damage Extent')['Injury Severity'].apply(lambda x: (x != 'no apparent injury').sum() / len(x) * 100)
row_counts = df_analysis.groupby('Vehicle Damage Extent')['Injury Severity'].count()

#איחוד כל התוצאות לטבלה אחת (DataFrame)
result_df = pd.DataFrame({
    'Percentage Not "No Apparent Injury"': vehicle_damage_summary,
    'Row Counts': row_counts
})

print(result_df)

# יצירת גרף
plt.figure(figsize=(12, 6))
ax = sns.barplot(x=result_df.index, y='Percentage Not "No Apparent Injury"', data=result_df, palette='viridis')
plt.title('Percentage of Non "No Apparent Injury" by Vehicle Damage Extent', fontsize=16)
plt.xlabel('Vehicle Damage Extent', fontsize=14)
plt.ylabel('Percentage', fontsize=14)
plt.xticks(rotation=45, ha='right')

# הוספת אחוזים מעל כל עמודה בגרף
for p in ax.patches:
    ax.annotate(f'{p.get_height():.2f}%', (p.get_x() + p.get_width() / 2., p.get_height()),
                ha='center', va='center', xytext=(0, 5), textcoords='offset points', fontsize=10)

# הוספת מספר השורות מתחת לכל עמודה בגרף
for i, count in enumerate(result_df['Row Counts']):
    ax.text(i, -35, f'Count: {count}', ha='center', va='top', fontsize=8)


plt.tight_layout()
plt.show()

# Speed Limit -היסטוגרמה ל
plt.figure(figsize=(8, 5))
sns.histplot(df_analysis["Speed Limit"], bins=20, kde=True)
plt.title("Distribution of Speed Limits", fontsize=14)
plt.xlabel("Speed Limit")
plt.ylabel("Count")
plt.show()

# גרף לניתוח קשר בין מגבבלות מהירות ממוצעת לפי רמת פציעה
plt.figure(figsize=(10, 6))
sns.boxplot(data=df_analysis, x='Injury Severity', y='Speed Limit', order=['no apparent injury', 'possible injury', 'suspected minor injury', 'suspected serious injury', 'fatal injury'])
plt.title("Speed Limit by Injury Severity")
plt.xlabel("Injury Severity")
plt.ylabel("Speed Limit")
plt.xticks(rotation=45)
plt.show()

df_analysis.isna().sum()

"""

# ניקוי נתונים- הכנת נתונים למידול"""

df.replace(["", " ", "Unknown", "NA", "N/A","n/a","na",'UNKNOWN','unknown','Unknown, Unknown',"-","UNK","UNKN","UMK","ZZKNOWN"], np.nan, inplace=True)
# חישוב הערכים החסרים מחדש
missing_values = df.isnull().sum()
missing_percentage = (missing_values / len(df)) * 100

missing_summary = pd.DataFrame({
    'Missing Values': missing_values,
    'Percentage Missing': missing_percentage
})

# סינון ערכים חסרים
missing_summary = missing_summary[missing_summary['Missing Values'] > 0]
print(missing_summary)

"""Injury Severity טיפול בעמודת המטרה"""

# הצגת הערכים הייחודיים בעמודת המטרה
print(" מספר מופעים לפני טיפול:", df['Injury Severity'].nunique())
print(df['Injury Severity'].value_counts())

print("-----------")

###אחרי תיקון###
# הפיכת כל הערכים בעמודת injury_severity לאותיות קטנות (lowercase)
df["Injury Severity"] = df["Injury Severity"].str.strip().str.lower()
print(" מספר מופעים לאחר טיפול:", df['Injury Severity'].nunique())
print(df['Injury Severity'].value_counts())

# מחיקת רשומות שיש להן ערך חסר בעמודת מטרה
before = len(df)
df = df[df['Injury Severity'].notna()]
after = len(df)
print(f" נמחקו {before - after} שורות עם ערך חסר בעמודת המטרה.")

#שמציגה היכן יש יותר נקודות Latitude ו־Longitude לפי (Heatmap) ליצור מפת חום

#לא לכלול שורות שבהן ערך חומרת הפציעה הוא 'ללא פציעה נראית לעין'.
df_filtered = df[df['Injury Severity'] != 'no apparent injury']

#יצירת מפה שמורכזת סביב ערכי הרוחב והאורך הממוצעים
map_center = [df_filtered['Latitude'].mean(), df_filtered['Longitude'].mean()]
m = folium.Map(location=map_center, zoom_start=10)

# ליצור רשימה של רשימות, כאשר כל תת-רשימה מייצגת נקודה על המפה
heat_data = [[row['Latitude'], row['Longitude']] for index, row in df_filtered.iterrows()]

# הוספת מפת החום
HeatMap(heat_data).add_to(m)

# הצגת המפה
m

#html שמירת המפה בקובץ
m.save("my_folium_map.html")

"""מחיקת עמודות שלא תורמות לחיזוי"""

# רשימת עמודות שמוגדרות כמזהים ויותר מ80 אחוז ערכים חסרים – לא תורמות לניתוח או חיזוי
columns_to_drop = [
    'Report Number',          # מספר הדוח של התאונה
    'Local Case Number',      # מספר תיק מקומי
    'Person ID',              # מזהה ייחודי לכל אדם בדוח
    'Vehicle ID',             # מזהה ייחודי לרכב בדוח
    'Location',                # מיקום כתוב, יש לנו כבר Latitude + Longitude
    'Off-Road Description',    # מעל 80 אחוז ערכים חסרים
    'Municipality',            # מעל 80 אחוז ערכים חסרים
    'Related Non-Motorist',    # מעל 80 אחוז ערכים חסרים
    'Non-Motorist Substance Abuse',   # מעל 80 אחוז ערכים חסרים
    'Circumstance',             # מעל 80 אחוז ערכים חסרים
    'Driverless Vehicle',       #יש רק no
    'Parked Vehicle',          # NO 184,777    ,3022 YES   ,1534 חסרים
    'Vehicle Model',            #לא רלוונטית לחיזוי- גם הרבה ערכים יחודיים 7000 בערך

]

# הסרה של העמודות האלו מה-DataFrame
df = df.drop(columns=columns_to_drop)

# הודעה למעקב
print("✅ העמודות הבאות נמחקו מהטבלה: ", columns_to_drop)

"""Agency Name"""

#####לפני תיקון#####

#לפני טיפול בה Agency Name הצגת הקטגוריות בעמודת
print(" מספר סוכנויות לפני האיחוד:", df['Agency Name'].nunique())
print(" התפלגות הסוכנויות המאוחדות:\n", df['Agency Name'].value_counts())

print("----------------")


######אחרי תיקון#####
#  מילון המרה לאיחוד ערכים בעמודת Agency Name
# איחוד שמות סוכנויות כתובים בצורות שונות (אותיות גדולות, קיצורים וכו')
agency_mapping = {
    'Montgomery County Police': 'Montgomery',
    'MONTGOMERY': 'Montgomery',

    'Rockville Police Departme': 'Rockville',
    'ROCKVILLE': 'Rockville',

    'Gaithersburg Police Depar': 'Gaithersburg',
    'GAITHERSBURG': 'Gaithersburg',

    'Takoma Park Police Depart': 'Takoma Park',
    'TAKOMA': 'Takoma Park',

    'Maryland-National Capital': 'MCPARK',  # הסוכנות המנהלת את הפארקים
}

# 🧼 המרה של הערכים בטבלה לפי המילון
df['Agency Name'] = df['Agency Name'].replace(agency_mapping)

# 🧾 בדיקה לאחר האיחוד – כמה ערכים ייחודיים נותרו
print(" מספר סוכנויות לאחר האיחוד:", df['Agency Name'].nunique())
print(" התפלגות הסוכנויות המאוחדות:\n", df['Agency Name'].value_counts())

"""ACRS Report Type"""

#עמודת ACRS Report Type

#ACRS Report Type עמודת
unique_values_before = df['ACRS Report Type'].nunique()
print(f"\n מספר קטגוריות  : {unique_values_before}")

print(df['ACRS Report Type'].value_counts())

"""Crash Date/Time"""

# Crash Date/Time סידור עמודת
# המרה לפורמט datetime
df['Crash Date/Time'] = pd.to_datetime(df['Crash Date/Time'], errors='coerce', infer_datetime_format=True)

# כמה ערכים לא הצלחנו להמיר?
invalid_dates = df['Crash Date/Time'].isna().sum()
print(f"❗ מספר תאריכים שלא הצלחנו להמיר (NaT): {invalid_dates}")

# חילוץ מאפיינים חדשים מהתאריך
df['Crash Hour'] = df['Crash Date/Time'].dt.hour
df['Crash Day'] = df['Crash Date/Time'].dt.dayofweek  # 0=Monday, 6=Sunday
df['Crash Month'] = df['Crash Date/Time'].dt.month

# הסרה של העמודה המקורית Crash Date/Time
df.drop(columns=['Crash Date/Time'], inplace=True)

# בדיקה
print("✅ סיימנו טיפול בתאריך!")
print(df[['Crash Hour', 'Crash Day', 'Crash Month']].head())

"""route type"""

#עמודת route type

#####לפני תיקון#####
#בדיקת ערכים יחודיים
print(" מספר מופעים לפני טיפול:", df['Route Type'].nunique())
print( df['Route Type'].value_counts())

#####אחרי תיקון#####
#טיפולה בעמודת route type
# מילון לאיחוד קטגוריות בעמודת Route Type
route_type_mapping = {
    'Maryland (State)': 'Maryland (State) Route',
    'County': 'County Route',
    'Municipality': 'Municipality Route',
    'Government': 'Government Route',
}
print("-------")
#  המרה של הערכים בטבלה לפי המילון
df['Route Type'] = df['Route Type'].replace(route_type_mapping)
print(" מספר מופעים לאחר טיפול:", df['Route Type'].nunique())
print(df['Route Type'].value_counts())

"""Collision Type"""

# Collision Type עמודת

#####לפני תיקון#####
# הצגת מספר הקטגוריות לפני התיקון
unique_values_before = df['Collision Type'].nunique()
print(f"\n מספר קטגוריות לפני תיקון: {unique_values_before}")

print(df['Collision Type'].value_counts())

#####אחרי תיקון#####
# המרת כל הערכים לאותיות גדולות והסרת רווחים מיותרים
df['Collision Type'] = df['Collision Type'].apply(lambda x: x.upper() if pd.notna(x) else x)

# תיקון והאחדת שמות דומים
df['Collision Type'] = df['Collision Type'].replace({
    'FRONT TO REAR': 'SAME DIRECTION REAR-END',
    'SAME DIR REAR END': 'SAME DIRECTION REAR-END',
    'SIDESWIPE, SAME DIRECTION': 'SAME DIRECTION SIDESWIPE',
    'SIDESWIPE, OPPOSITE DIRECTION': 'OPPOSITE DIRECTION SIDESWIPE',
    'FRONT TO FRONT': 'HEAD ON',
    'OTHER':'other'
})

#ערכים חסרים
missing_Collision_Type = df['Collision Type'].isna().sum()
print(f" מספר ערכים חסרים בעמודת CollisionType: {missing_Collision_Type}")

# הצגת מספר הקטגוריות אחרי התיקון
unique_values_after = df['Collision Type'].nunique()
print(f"\n מספר קטגוריות אחרי תיקון : {unique_values_after}")

print(df['Collision Type'].value_counts())

"""Weather"""

# Weather עמודת

###לפני תיקון
# הצגת מספר הקטגוריות
unique_values_before = df['Weather'].nunique()
print(f"\n מספר קטגוריות לפני תיקון: {unique_values_before}")

print(df['Weather'].value_counts())

#אחרי תיקון
#הפיכת כל הערכים לאותיות קטנות
df['Weather'] = df['Weather'].str.strip().str.lower()

#איחוד שמות דומים
df['Weather'] = df['Weather'].replace('raining', 'rain')

# הצגת מספר הקטגוריות אחרי התיקון
unique_values_after = df['Weather'].nunique()
print(f"\n מספר קטגוריות אחרי תיקון נוסף: {unique_values_after}")

print(df['Weather'].value_counts())

"""Surface Condition"""

# Surface Condition עמודת

####Kלפני תיקון####
# הצגת מספר הקטגוריות לפני התיקון
unique_values_before = df['Surface Condition'].nunique()
print(f"\n מספר קטגוריות לפני תיקון: {unique_values_before}")

print(df['Surface Condition'].value_counts())

print("---------")

####אחרי תיקון####
#הפיכת כל הערכים לאותיות קטנות
df['Surface Condition'] = df['Surface Condition'].str.strip().str.lower()

#איחוד שמות דומים
mapping_surface_condition = {
    'ice/frost': 'ice',
    'water (standing, moving)': 'water(standing/moving)'
}

df['Surface Condition'] = df['Surface Condition'].replace(mapping_surface_condition)

# הצגת מספר הקטגוריות אחרי התיקון
unique_values_after = df['Surface Condition'].nunique()
print(f"\n מספר קטגוריות אחרי תיקון : {unique_values_after}")

print(df['Surface Condition'].value_counts())

"""Light"""

# Light עמודת

#####לפני תיקון####
#ערכים חסרים
missing_Light= df['Light'].isna().sum()
print(f" מספר ערכים חסרים בעמודת Light: {missing_Light}")

# הצגת מספר הקטגוריות לפני התיקון
print(df['Light'].nunique())
print(df['Light'].value_counts())

#####אחרי תיקון####
#הפיכת כל הערכים לאותיות קטנות
df['Light'] = df['Light'].str.strip().str.lower()

#איחוד שמות דומים
df['Light'] = df['Light'].replace({
    'dark lights on': 'dark - lighted',
    'dark no lights': 'dark - not lighted',
    'dark -- unknown lighting': 'dark - unknown lighting'
})
# הצגת מספר הקטגוריות אחרי התיקון
unique_values_after = df['Light'].nunique()
print(f"\n מספר קטגוריות אחרי תיקון: {unique_values_after}")

print(df['Light'].value_counts())

"""Traffic Control"""

# Traffic Control עמודת

####לפני תיקון####
#ערכים חסרים
missing_Traffic_Control= df['Traffic Control'].isna().sum()
print(f" מספר ערכים חסרים בעמודת Traffic Control: {missing_Traffic_Control}")

# הצגת מספר הקטגוריות לפני התיקון
print(df['Traffic Control'].nunique())
print(df['Traffic Control'].value_counts())

####אחרי תיקון####
#הפיכת כל הערכים לאותיות קטנות
df['Traffic Control'] = df['Traffic Control'].str.strip().str.lower()

# #איחוד שמות דומים
df['Traffic Control'] = df['Traffic Control'].replace({
    # רמזורים
    'traffic control signal': 'traffic signal',
    'flashing traffic control signal': 'flashing traffic signal',
    # שלטים אחרים (אזהרה, הולכי רגל, עקומות, בית ספר)
    'other warning sign': 'warning sign',
    'school zone sign device': 'school zone sign',
    'school zone': 'school zone sign',
    # הולכי רגל/אנשים
    'pedestrian crossing': 'pedestrian crossing sign',
    'person': 'person (including flagger, law enforcement, crossing guard, etc.',

    'other signal' : 'other'
})

# הצגת מספר הקטגוריות אחרי התיקון
unique_values_after = df['Traffic Control'].nunique()
print(f"\n מספר קטגוריות אחרי תיקון : {unique_values_after}")

print(df['Traffic Control'].value_counts())

"""Driver Substance Abuse"""

# Driver Substance Abuse עמודת

####לפני תיקון####
#ערכים חסרים
missing_Driver_Substance_Abuse= df['Driver Substance Abuse'].isna().sum()
print(f" מספר ערכים חסרים בעמודת Driver Substance Abuse: {missing_Driver_Substance_Abuse}")

# הצגת מספר הקטגוריות לפני התיקון
print(df['Driver Substance Abuse'].nunique())
print(df['Driver Substance Abuse'].value_counts())

####אחרי ניקוי####
# המרה לאותיות קטנות והסרת רווחים
df['Driver Substance Abuse'] = df['Driver Substance Abuse'].str.strip().str.lower()

# מיפוי ערכים עם ניסוח לא ברור לניסוחים מדויקים יותר
mapping_substance_abuse = {
    'unknown, not suspect of drug use': 'alcohol unknown, drug not suspected',
    'unknown, suspect of drug use': 'alcohol unknown, drug suspected',
    'suspect of alcohol use, unknown': 'alcohol suspected, drug unknown',
    'not suspect of alcohol use, unknown': 'alcohol not suspected, drug unknown'
}

# החלת המיפוי
df['Driver Substance Abuse'] = df['Driver Substance Abuse'].replace(mapping_substance_abuse)

#הצגת סיכום
print(f" מספר ערכים חסרים: {df['Driver Substance Abuse'].isna().sum()}")
df['Driver Substance Abuse'].value_counts()

"""Driver At Fault"""

# Driver At Fault עמודת

#ערכים חסרים
missing_Driver_At_Fault= df['Driver At Fault'].isna().sum()
print(f"📌 מספר ערכים חסרים בעמודת Driver At Fault: {missing_Driver_At_Fault}")

# הצגת מספר הקטגוריות לפני התיקון
print(df['Driver At Fault'].nunique())
df['Driver At Fault'].value_counts()

"""Driver Distracted By"""

# Driver Distracted By עמודת

#ערכים חסרים
missing_Driver_Distracted_By= df['Driver Distracted By'].isna().sum()
print(f" מספר ערכים חסרים בעמודת Driver Distracted By: {missing_Driver_Distracted_By}")

##לפני תיקון
# הצגת מספר הקטגוריות לפני התיקון
print(df['Driver Distracted By'].nunique())
print(df['Driver Distracted By'].value_counts())

##אחרי תיקון
#הפיכת כל הערכים לאותיות קטנות
df['Driver Distracted By'] = df['Driver Distracted By'].str.strip().str.lower()

# הצגת מספר הקטגוריות אחרי התיקון
unique_values_after = df['Driver Distracted By'].nunique()
print(f"\n מספר קטגוריות אחרי תיקון : {unique_values_after}")

print(df['Driver Distracted By'].value_counts())

"""Drivers License State"""

#Drivers License State עמודת

####לפני תיקון####

#ערכים חסרים
missing_Drivers_License_State= df['Drivers License State'].isna().sum()
print(f" מספר ערכים חסרים בעמודת Drivers License State: {missing_Drivers_License_State}")

# הצגת מספר הקטגוריות לפני התיקון
print(df['Drivers License State'].nunique())
print(df['Drivers License State'].value_counts())

print("\n :אחרי טיפול בעמודת שם מדינה")
####אחרי תיקון####
# מילון קוד ➝ שם מדינה מלא
state_mapping = {
    'AB': 'Alberta',
    'AK': 'Alaska',
    'AL': 'Alabama',
    'AR': 'Arkansas',
    'AS': 'American Samoa',
    'AZ': 'Arizona',
    'BC': 'British Columbia',
    'CA': 'California',
    'CO': 'Colorado',
    'CT': 'Connecticut',
    'DC': 'District of Columbia',
    'DE': 'Delaware',
    'FL': 'Florida',
    'FM': 'Federated States of Micronesia',
    'GA': 'Georgia',
    'GU': 'Guam',
    'HI': 'Hawaii',
    'IA': 'Iowa',
    'ID': 'Idaho',
    'IL': 'Illinois',
    'IN': 'Indiana',
    'IT': 'Italy',
    'KS': 'Kansas',
    'KY': 'Kentucky',
    'LA': 'Louisiana',
    'MA': 'Massachusetts',
    'MB': 'Manitoba',
    'MD': 'Maryland',
    'ME': 'Maine',
    'MH': 'Marshall Islands',
    'MI': 'Michigan',
    'MN': 'Minnesota',
    'MO': 'Missouri',
    'MP': 'Northern Mariana Islands',
    'MS': 'Mississippi',
    'MT': 'Montana',  #
    'MX-BCN': 'Baja California',
    'MX-MEX': 'Estado de México',
    'MX-ROO': 'Quintana Roo',
    'NB': 'New Brunswick',
    'NC': 'North Carolina',
    'ND': 'North Dakota',
    'NE': 'Nebraska',
    'NF': 'Norfolk',  #
    'NH': 'New Hampshire',
    'NJ': 'New Jersey',
    'NL': 'Netherlands',  #
    'NM': 'New Mexico',
    'NS': 'Nova Scotia',   #
    'NT': 'Northwest Territories',
    'NV': 'Nevada',
    'NY': 'New York',
    'OH': 'Ohio',
    'OK': 'Oklahoma',
    'ON': 'Ontario',  #
    'OR': 'Oregon',
    'PA': 'Pennsylvania',
    'PR': 'Puerto Rico',
    'QC': 'Quebec',
    'RI': 'Rhode Island',
    'SC': 'South Carolina',
    'SD': 'South Dakota',
    'SK': 'Saskatchewan',
    'TN': 'Tennessee',
    'TX': 'Texas',
    'UM': 'U.S. Minor Outlying Islands',
    'US': 'United States',
    'UT': 'Utah',
    'VA': 'Virginia',
    'VI': 'U.S. Virgin Islands',
    'VT': 'Vermont',
    'WA': 'Washington',
    'WI': 'Wisconsin',
    'WV': 'West Virginia',
    'WY': 'Wyoming',
    'YT': 'Yukon'
}

print(df['Drivers License State'].nunique())

# עדכון ישיר של העמודה המקורית
def map_state(code):
    if code == 'XX':
        return np.nan
    elif code in state_mapping:
        return state_mapping[code]
    else:
        return code

df['Drivers License State'] = df['Drivers License State'].apply(map_state)
missing_Drivers_License_State= df['Drivers License State'].isna().sum()
print(missing_Drivers_License_State)

# drivers license state קוד לאיחוד קטגוריות נדירות (עד 50 מופעים ייחודים)  בעמודת
# סופרים את כמות ההופעות של כל מדינה
state_counts = df['Drivers License State'].value_counts()

# בוחרים מדינות עם לפחות 50 מקרים
threshold = 50
common_states = state_counts[state_counts >= threshold].index

# מחליפים ערכים נדירים ב־"Other" ישירות בעמודה המקורית
df['Drivers License State'] = df['Drivers License State'].apply(
    lambda x: x if x in common_states else 'other'
)

"""Vehicle Damage Extent"""

#Vehicle Damage Extent עמודת

#ערכים חסרים
missing_Vehicle_Damage_Extent= df['Vehicle Damage Extent'].isna().sum()
print(f" מספר ערכים חסרים בעמודת Vehicle Damage Extent: {missing_Vehicle_Damage_Extent}")

# הצגת מספר הקטגוריות לפני התיקון
print(f"\n מספר קטגוריות לפני תיקון:")
print(df['Vehicle Damage Extent'].nunique())
print(df['Vehicle Damage Extent'].value_counts())

#אחרי תיקון
#הפיכת כל הערכים לאותיות קטנות
df['Vehicle Damage Extent'] = df['Vehicle Damage Extent'].str.strip().str.lower()

# הצגת מספר הקטגוריות אחרי התיקון
unique_values_after = df['Vehicle Damage Extent'].nunique()
print(f"\n מספר קטגוריות אחרי תיקון נוסף: {unique_values_after}")

print(df['Vehicle Damage Extent'].value_counts())

"""Vehicle First Impact Location"""

#Vehicle First Impact Location עמודת

####לפני תיקון####
#ערכים חסרים
missing_Vehicle_First_Impact_Location= df['Vehicle First Impact Location'].isna().sum()
print(f" מספר ערכים חסרים בעמודת Vehicle First Impact Location: {missing_Vehicle_First_Impact_Location}")

# הצגת מספר הקטגוריות לפני התיקון
print(df['Vehicle First Impact Location'].nunique())
print(df['Vehicle First Impact Location'].value_counts())

####אחרי תיקון####
#איחוד ערכים דומים
mapping = {
    'Twelve O Clock': 'TWELVE OCLOCK',
    'Six O Clock': 'SIX OCLOCK',
    'One O Clock': 'ONE OCLOCK',
    'Two O Clock': 'TWO OCLOCK',
    'Three O Clock': 'THREE OCLOCK',
    'Four O Clock': 'FOUR OCLOCK',
    'Five O Clock': 'FIVE OCLOCK',
    'Six O Clock': 'SIX OCLOCK',
    'Seven O Clock': 'SEVEN OCLOCK',
    'Eight O Clock': 'EIGHT OCLOCK',
    'Nine O Clock': 'NINE OCLOCK',
    'Ten O Clock': 'TEN OCLOCK',
    'Eleven O Clock': 'ELEVEN OCLOCK',
    'Non-Collision': 'NON-COLLISION',
    'UNDERSIDE': 'Underside'
}
df['Vehicle First Impact Location'] = df['Vehicle First Impact Location'].replace(mapping)

# הצגת מספר הקטגוריות אחרי התיקון
unique_values_after = df['Vehicle First Impact Location'].nunique()
print(f"\n מספר קטגוריות אחרי תיקון : {unique_values_after}")

print(df['Vehicle First Impact Location'].value_counts())

"""Vehicle Body Type"""

#Vehicle Body Type עמודת

####לפני תיקון####
#ערכים חסרים
missing_Vehicle_Body_Type= df['Vehicle Body Type'].isna().sum()
print(f"📌 מספר ערכים חסרים בעמודת Vehicle Body Type: {missing_Vehicle_Body_Type}")

# הצגת מספר הקטגוריות לפני התיקון
print(df['Vehicle Body Type'].nunique())
print(df['Vehicle Body Type'].value_counts())

####אחרי תיקון####
#הפיכת הערכים לאותיות קטנות
df['Vehicle Body Type'] = df['Vehicle Body Type'].str.strip().str.lower()

#איחוד ערכים דומים
vehicle_type_mapping = {
    '(sport) utility vehicle': 'sport utility vehicle',
    'bus - transit': 'transit bus',
    'bus - school': 'school bus',
    'bus - other type': 'other bus',
    'bus - cross country': 'cross country bus',
    #'all-terrain vehicle/all-terrain cycle (atv/atc)': 'all terrain vehicle (atv)',
    #'recreational off-highway vehicles (rov)': 'recreational vehicle',
}

df['Vehicle Body Type'] = df['Vehicle Body Type'].replace(vehicle_type_mapping)

# הצגת מספר הקטגוריות אחרי התיקון
unique_values_after = df['Vehicle Body Type'].nunique()
print(f"\n מספר קטגוריות אחרי תיקון : {unique_values_after}")

print(df['Vehicle Body Type'].value_counts())

# קוד לאיחוד קטגוריות נדירות בעמודת Vehicle Body Type

# סופרים את כמות ההופעות של כל סוג רכב
body_type_counts = df['Vehicle Body Type'].value_counts()

# בוחרים סוגי רכב שמופיעים לפחות 20 פעמים
threshold = 20
common_body_types = body_type_counts[body_type_counts >= threshold].index

# מחליפים קטגוריות נדירות ב־"other" ישירות בעמודה המקורית
df['Vehicle Body Type'] = df['Vehicle Body Type'].apply(
    lambda x: x if x in common_body_types else 'other'
)

"""Vehicle Movement"""

#Vehicle Movement עמודת

####לפני תיקון####
#ערכים חסרים
missing_Vehicle_Movement= df['Vehicle Movement'].isna().sum()
print(f" מספר ערכים חסרים בעמודת Vehicle Movement: {missing_Vehicle_Movement}")

# הצגת מספר הקטגוריות לפני התיקון
print(df['Vehicle Movement'].nunique())
print(df['Vehicle Movement'].value_counts())

####אחרי תיקון####
#הפיכת הערכים לאותיות קטנות
df['Vehicle Movement'] = df['Vehicle Movement'].str.strip().str.lower()

#איחוד ערכים דומים
mapping = {
    'making u turn': 'making u-turn',
    'parked': 'parking',
    'turning right': 'making right turn',
    'turning left': 'making left turn',
    'overtaking/passing': 'passing'
}
df['Vehicle Movement'] = df['Vehicle Movement'].replace(mapping)

# הצגת מספר הקטגוריות אחרי התיקון
unique_values_after = df['Vehicle Movement'].nunique()
print(f"\n מספר קטגוריות אחרי תיקון : {unique_values_after}")

print(df['Vehicle Movement'].value_counts())

"""vehicle_year"""

#'vehicle_year'

# המרה בטוחה למספרים (למקרה שיש תווים חריגים)
df['Vehicle Year'] = pd.to_numeric(df['Vehicle Year'], errors='coerce')

# NaN- הפיכת ערכים לא תקינים (מחוץ לטווח 1980–2025) ל
df.loc[(df['Vehicle Year'] < 1980) | (df['Vehicle Year'] > 2025), 'Vehicle Year'] = pd.NA
#לפני שעשינו את הטווח לא היו ערכים חסרים בעמודה זאת

print(f"\n מספר ערכים חסרים: {df['Vehicle Year'].isna().sum()}")

"""Vehicle Make"""

#טיפול בעמודת vehicle make

####לפני טיפול####
#בfuzzy בדיקה של הערכים הנפוצים מתוך כל הערכים הייחודים בעמודת הייצרן כדי לטפל ידנית בערכים לפני שנשתמש
# המרה לאותיות גדולות והסרת רווחים
df["Vehicle Make"] = df["Vehicle Make"].str.upper().str.strip()

# ספירת ערכים ייחודיים והצגת 40 הראשונים
top_40 = df["Vehicle Make"].value_counts(dropna=False).head(40)

# הדפסה
print("📊 40 הערכים הנפוצים ביותר בעמודה Vehicle Make:")
print(top_40)
print("\n--------")

####אחרי טיפול####

# שלב 1️⃣: ניקוי ראשוני של עמודת Vehicle Make
# המרה לאותיות גדולות והסרת רווחים
df["Vehicle Make"] = df["Vehicle Make"].str.upper().str.strip()

# ✨ טיפול ידני לפני fuzzy – ערכים שכיחים עם קיצורים
df["Vehicle Make"] = df["Vehicle Make"].replace({
    "TOYT": "TOYOTA",
    "TOTA" : "TOYOTA",
    "TOY": "TOYOTA",
    "TOYO": "TOYOTA",
    "TOYOA": "TOYOTA",
    "TOYOT": "TOYOTA",
    "TOYOYA" : "TOYOTA",
    "TOYOTO" : "TOYOTA",
    "TOYOTS" :"TOYOTA",
    "TOYOVAL" : "TOYOTA",
    "TOYOYTA" : "TOYOTA",
    "TOYTA" : "TOYOTA",
    "TOYTOA" : "TOYOTA",
    "TOYTOTA" :"TOYOTA",
    "HOND": "HONDA",
    "CHEV": "CHEVROLET",
    "CHEVY": "CHEVROLET",
    "NISS": "NISSAN",
    "HYUN": "HYUNDAI",
    "MERZ": "MERCEDES-BENZ",
    "MERC": "MERCEDES-BENZ",
    "MERCEDES": "MERCEDES-BENZ",
    "VOLK": "VOLKSWAGEN",
    "VOLKS" : "VOLKSWAGEN",
    "VOLKS WAGON" : "VOLKSWAGEN",
    "VOLKSWAGON": "VOLKSWAGEN",
    "VOLV" : "VOLVO",
    "VW" : "VOLKSWAGEN",
    "ACUR": "ACURA",
    "SUBA": "SUBARU",
    "DODG": "DODGE",
    "MAZD": "MAZDA",
    "GILL": "GILLIG",
    "THOMAS": "THOMAS BUILT BUSES",
    "LINC" : "LINCOLN",
    "MITS" : "MITSUBISHI",
    "MITZ" : "MITSUBISHI",
    "NFLY" : "NEW FLYER",
    "PIERCE": "PIERCE MANUFACTURING",
    "PONT" : "PONTIAC",
    "PORS" : "PORSCHE",
    "SUBA" : "SUBARU",
    "SUB" : "SUBARU",
    "SUBU" : "SUBARU",
    "SUBURU" : "SUBARU",
    "SUZI" : "SUZUKI",
    "SUZ" : "SUZUKI",
    "SUSUKI" : "SUZUKI",
    "SUZU" : "SUZUKI",
    "SUZIKI": "SUZUKI",
    "TELSA" : "TESLA",
    "THMNS" : "THOMAS BUILT BUSES",
    "THOM" : "THOMAS BUILT BUSES",
    "US" : "USPS",
    "US POSTAL" : "USPS",
    "YAMA" : "YAMAHA"
})

# שלב 2️⃣: הגדרת רשימת מותגים תקניים (מותגים ידועים של רכבים)
known_brands = [
    'TOYOTA', 'HONDA', 'FORD', 'CHEVROLET', 'NISSAN', 'HYUNDAI', 'KIA',
    'MAZDA', 'MERCEDES-BENZ', 'BMW', 'VOLKSWAGEN', 'JEEP', 'ACURA', 'DODGE',
    'SUBARU', 'LEXUS', 'INFINITI', 'CHRYSLER', 'AUDI', 'BUICK', 'GMC',
    'THOMAS BUILT BUSES', 'GILLIG', 'FREIGHTLINER', 'BLUE BIRD', 'TESLA',
    'ISUZU', 'HARLEY DAVIDSON', 'PETERBILT', 'MITSUBISHI', 'MINI','CADILLAC',
    'VOLVO', 'NEW FLYER','JAGUAR','LINCOLN','MACK','ORION','PIERCE MANUFACTURING','PONTIAC',
    'PORSCHE','PTRB','RAM','RANGE ROVER','SATURN','SCION', 'SPARTAN','SUZUKI','TBU','TRANZIT',
    'TRIUMPH','TSMR','USPS','YAMAHA'
]

# שלב 3️⃣: fuzzy matching – התאמת ערכים ידניים לרשימת המותגים התקניים
# אם הדמיון בין ערך בעמודה לבין מותג תקני הוא >= 80%, נחליף למותג.
# אם לא – נכניס לקטגוריה "other"
mapping_dict = {}
for make in df["Vehicle Make"].dropna().unique():
    match, score, _ = process.extractOne(make, known_brands, scorer=fuzz.token_sort_ratio)
    if score >= 80:
        mapping_dict[make] = match
    else:
        mapping_dict[make] = "OTHER"

# שלב 4️⃣: החלת ההמרה על כל הערכים בעמודה – כולל NaN (יהפוך ל"OTHER")
df["Vehicle Make"] = df["Vehicle Make"].apply(lambda x: mapping_dict.get(x, "OTHER"))

# שלב 5️⃣: הצגת סיכום
print("✅ מספר ערכים ייחודיים לאחר ניקוי:", df["Vehicle Make"].nunique(dropna=True))
print("📄 מספר רשומות:", df.shape[0])
print("❌ מספר ערכים שהפכו ל-other:", (df["Vehicle Make"] == "OTHER").sum())
print("\n🔍 הערכים לאחר הניקוי:")
print(df["Vehicle Make"].value_counts())

"""h3_index

 \Latitude, Longitude נוצר באמצעות העמודות

"""

# # נעשה אזורים למיקום על ידי קו אורך וקו רוחב
# # שהופכת את הקו אורך והקו גובה לאזורים h3 לכן נשתמש  בספרייה

# H3 להגדיר פונקציה שממירה קווי רוחב/אורך לאינדקס
def lat_lng_to_h3(row, resolution=7):
    return h3.latlng_to_cell(row['Latitude'], row['Longitude'], resolution)

#למבנה הנתונים ברזולוציה 6 H3 מוסיפה
# ניתן לשנות את הרזולוציה (0–15) בהתאם לצורך
# רזולוציה גבוהה יותר = משושים קטנים יותר
resolution = 6
df['h3_index'] = df.apply(lat_lng_to_h3, resolution=resolution, axis=1)

#לשנות את הצבעים של המשושים בקפלר לפי כמות התאונות
df_agg = df.groupby('h3_index')['Latitude'].count().reset_index(name="Number Of Accidents")

#לכמה משושים השטח (התאונות) מתחלקות
df.h3_index.nunique()

## הקוד שמייצר את הקובץ  של הקפלר

from keplergl import KeplerGl
# Create a Kepler.gl map
config = {
    "mapState": {
        "latitude": 39.0458,     # Maryland center איפה המפה תפתח בתור התחלה
        "longitude": -76.6413,
        "zoom": 8,
        "bearing": 0,
        "pitch": 0
    }
}

map_1 = KeplerGl(height=600, config=config)

# Add data to the map
map_1.add_data(data=df_agg, name="H3 Points")

# Display the map
map_1

map_1.save_to_html(file_name='kepler_map.html')

# Display HTML map in Colab
from google.colab import files
files.download('kepler_map.html')  # Download the HTML file

# שהופכת את הקו אורך וגובה לאזורים ולכן אין שימוש בשאר עמודות המיקומים h3 עמודות אלו הוסרו לאחר שימוש בספריית
columns_to_drop = [
    'Road Name',
    'Cross-Street Name',
    'Latitude',
    'Longitude'
]

df = df.drop(columns=columns_to_drop)

# הודעה למעקב
print(" העמודות הבאות נמחקו מהטבלה: ", columns_to_drop)

"""טיפול בערכים חסרים"""

# (חציון/אחר) טיפול בערכים חסרים
for col in df.columns:
    if df[col].dtype == 'object' or str(df[col].dtype) == 'category':
        # most_common = df[col].mode()[0]  # שכיח
        # df[col].fillna(most_common, inplace=True)
        df[col] = df[col].fillna('other') # אחר
    else:
        mean_value = df[col].median()  # חציון
        df[col].fillna(mean_value, inplace=True)

#CSV שמירת הדאטה לאחר הניקוי בקובץ
df.to_csv("processed_data.csv")

"""# גרפים וטבלאות לניתוח הנתונים"""

# טבלה לראות את הכמות הערכים הייחודים לפני ואחרי כל הניקויים

#עשינו קוד למעלה להעתקת הנתונים לפני כל שינוי
unique_before = df_original.nunique()
unique_after = df.nunique()

# יצירת טבלה אחת שמשווה
unique_comparison = pd.DataFrame({
    'Unique Values (Before)': unique_before,
    'Unique Values (After)': unique_after
}).sort_values('Unique Values (Before)', ascending=False)

display(unique_comparison)

#ניתוח עמודת המטרה
# יצירת טבלת שכיחויות
target_counts = df['Injury Severity'].value_counts()

# סידור לפי סדר היררכי (אופציונלי)
order = ['no apparent injury', 'possible injury', 'suspected minor injury', 'suspected serious injury', 'fatal injury']

# הגרף
plt.figure(figsize=(10, 6))
sns.set_style("whitegrid")
sns.barplot(x=target_counts.index, y=target_counts.values, palette='Blues_d', order=order)

# כותרות ותיוגים
plt.title('Distribution of Injury Severity', fontsize=18, pad=20)
plt.xlabel('Injury Severity', fontsize=14)
plt.ylabel('Count', fontsize=14)
plt.xticks(rotation=45, ha='right', fontsize=12)
plt.yticks(fontsize=12)

# הצגת מספרים מעל כל עמודה
for i, v in enumerate(target_counts.loc[order]):
    plt.text(i, v + 2000, f'{v:,}', ha='center', va='bottom', fontsize=12)

plt.tight_layout()
plt.show()

# סטטסיטקות תיאוריות של משתנים מספריים
# בחירת משתנים מספריים
numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns

# יצירת טבלת סטטיסטיקה
df_numerical_stats = df[numerical_cols].describe().T

# עיצוב
styled_stats = df_numerical_stats.style\
    .background_gradient(cmap='Blues', subset=['mean', 'std', 'min', '25%', '50%', '75%', 'max'])\
    .format("{:.2f}")\
    .set_caption("סטטיסטיקות תיאוריות עבור משתנים מספריים")\
    .set_table_styles(
        [{'selector': 'th', 'props': [('font-size', '14pt'), ('text-align', 'center')]},
         {'selector': 'td', 'props': [('font-size', '12pt'), ('text-align', 'center')]}]
    )\
    .set_properties(**{'border': '1px solid black', 'padding': '6px'})

styled_stats

#גרף לשעה ביום
# טבלת שכיחויות לפי שעה
hour_counts = df['Crash Hour'].value_counts().sort_index()

# הדפסת טבלה
print("טבלת שכיחויות לפי שעה:")
print(hour_counts)

plt.figure(figsize=(12,6))
sns.set_style("whitegrid")
ax = sns.barplot(x=hour_counts.index, y=hour_counts.values, color="#4F81BD")
plt.title('Distribution of Crashes by Hour of Day', fontsize=18, pad=20)
plt.xlabel('Hour of Day', fontsize=14)
plt.ylabel('Number of Crashes', fontsize=14)
plt.xticks(range(0,24), fontsize=12)
plt.yticks(fontsize=12)
plt.grid(axis='y', linestyle='--', alpha=0.7)

# הוספת מספרים מעל כל עמודה
for container in ax.containers:
    ax.bar_label(container, fmt='%d', fontsize=10, label_type='edge', padding=2)

plt.tight_layout()
plt.show()

#גרף ליום בשבוע
# טבלת שכיחויות לפי יום
day_counts = df['Crash Day'].value_counts().sort_index()

# מיפוי מספר -> שם יום
day_names = {
    0: 'Monday', 1: 'Tuesday', 2: 'Wednesday', 3: 'Thursday',
    4: 'Friday', 5: 'Saturday', 6: 'Sunday'
}
day_counts.index = day_counts.index.map(day_names)

# הדפסת טבלה
print("טבלת שכיחויות לפי יום:")
print(day_counts)

plt.figure(figsize=(10,6))
sns.set_style("whitegrid")
ax = sns.barplot(x=day_counts.index, y=day_counts.values, color="#4F81BD")
plt.title('Distribution of Crashes by Day of Week', fontsize=18, pad=20)
plt.xlabel('Day of Week', fontsize=14)
plt.ylabel('Number of Crashes', fontsize=14)
plt.xticks(rotation=45, fontsize=12)
plt.yticks(fontsize=12)
plt.grid(axis='y', linestyle='--', alpha=0.7)

# הוספת מספרים מעל כל עמודה
for container in ax.containers:
    ax.bar_label(container, fmt='%d', fontsize=10, label_type='edge', padding=2)

plt.tight_layout()
plt.show()

#גרף לחודש בשנה
# טבלת שכיחויות לפי חודש
month_counts = df['Crash Month'].value_counts().sort_index()

# מיפוי מספר -> שם חודש
month_names = {1: 'January', 2: 'February', 3: 'March', 4: 'April',
               5: 'May', 6: 'June', 7: 'July', 8: 'August',
               9: 'September', 10: 'October', 11: 'November', 12: 'December'}
month_counts.index = month_counts.index.map(month_names)

# הדפסת טבלה
print("טבלת שכיחויות לפי חודש:")
print(month_counts)

plt.figure(figsize=(12,6))
sns.set_style("whitegrid")
ax = sns.barplot(x=month_counts.index, y=month_counts.values, color="#4F81BD")
plt.title('Distribution of Crashes by Month', fontsize=18, pad=20)
plt.xlabel('Month', fontsize=14)
plt.ylabel('Number of Crashes', fontsize=14)
plt.xticks(rotation=45, fontsize=12)
plt.yticks(fontsize=12)
plt.grid(axis='y', linestyle='--', alpha=0.7)

# הוספת מספרים מעל כל עמודה
for container in ax.containers:
    ax.bar_label(container, fmt='%d', fontsize=10, label_type='edge', padding=2)

plt.tight_layout()
plt.show()

#פונרציה להרצת גרפים לעמודות השונות
def plot_categorical_distribution(df, column, top_n=None, palette="pastel", rotation=45):

    # טבלת שכיחויות
    value_counts = df[column].value_counts(dropna=False)

    # אם המשתמש הגדיר top_n
    if top_n is not None:
        value_counts = value_counts.head(top_n)

    # הדפסת הטבלה
    print(f"\nטבלת שכיחויות עבור {column}:")
    print(value_counts)

    # קביעת גודל גרף אוטומטי לפי מספר קטגוריות
    num_categories = len(value_counts)
    width = max(12, num_categories * 0.6)   # לפחות 12, אחרת מתרחב ביחס לקטגוריות
    height = 6 if num_categories < 15 else 8

    # גרף
    plt.figure(figsize=(width, height))
    sns.set_style("whitegrid")
    ax = sns.barplot(
        x=value_counts.index.astype(str),
        y=value_counts.values,
    )

    plt.title(f"Distribution of {column}", fontsize=18, pad=20)
    plt.xlabel(column, fontsize=14)
    plt.ylabel("Count", fontsize=14)
    plt.xticks(rotation=rotation, ha='right', fontsize=12)
    plt.yticks(fontsize=12)
    plt.grid(axis='y', linestyle='--', alpha=0.7)

    # הוספת מספרים מעל העמודות
    for container in ax.containers:
        ax.bar_label(container, fmt='%d', fontsize=10, label_type='edge', padding=2)

    plt.tight_layout()
    plt.show()

plot_categorical_distribution(df, "Agency Name")

plot_categorical_distribution(df, "ACRS Report Type")

plot_categorical_distribution(df, "Collision Type")

plot_categorical_distribution(df, "Weather")

plot_categorical_distribution(df, "Surface Condition")

plot_categorical_distribution(df, "Light")

plot_categorical_distribution(df, "Traffic Control")

#driver at fault
import matplotlib.pyplot as plt

plt.rcParams['font.family'] = 'DejaVu Sans'

# טבלת שכיחויות
fault_counts = df['Driver At Fault'].value_counts()
print(fault_counts)

# הכנת תוויות משולבות (Label + אחוזים)
labels = [f'{label} ({percent:.1f}%)' for label, percent in zip(fault_counts.index, 100 * fault_counts / fault_counts.sum())]

# צבעים נאים
colors = ['#66B2FF', '#FF9933','#99CC66']  # כחול וכתום עדין

# גרף פאי
plt.figure(figsize=(8, 8))
plt.pie(fault_counts,
        labels=labels,
        autopct=None,
        startangle=90,
        counterclock=False,
        colors=colors)
plt.title('Distribution of Driver Fault in Accidents')
plt.show()

plot_categorical_distribution(df, "Driver Distracted By")

plot_categorical_distribution(df, "Drivers License State")

plot_categorical_distribution(df, "Vehicle Damage Extent")

plot_categorical_distribution(df, "Vehicle First Impact Location")

plot_categorical_distribution(df, "Vehicle Body Type")

plot_categorical_distribution(df, "Vehicle Movement")

plot_categorical_distribution(df, "Vehicle Going Dir")

plot_categorical_distribution(df, "Route Type")

plot_categorical_distribution(df, "Vehicle Make")

""" **ניתוח של העמודות עם עמודת המטרה**"""

# פונקציה כללית לחישוב הפיצרים השונים מול עמודת המטרה
def analyze_categorical_relationship(feature_column, target_column):
    # טבלת שכיחויות
    crosstab = pd.crosstab(df[feature_column], df[target_column], normalize='index')* 100
    display(crosstab)  # מציג את הטבלה

    # גרף
    crosstab.plot(kind='bar', stacked=True, figsize=(10,6))
    plt.legend(title='Injury Severity', bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.title(f'Distribution of {target_column} by {feature_column}')
    plt.ylabel('Percentage')
    plt.xlabel(feature_column)
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

    # חי-בריבוע
    contingency_table = pd.crosstab(df[feature_column], df[target_column])
    chi2, p, dof, expected = chi2_contingency(contingency_table)

    # חישוב Cramér's V
    n = contingency_table.sum().sum()
    min_dim = min(contingency_table.shape) - 1
    cramers_v = np.sqrt(chi2 / (n * min_dim))

    # הדפסת תוצאות
    print(f"Chi-Square Statistic: {chi2:.4f}")
    print(f"P-Value: {p:.4f}")
    print(f"Cramér's V: {cramers_v:.4f}")

# בדיקת קשר בין עמודת המטרה לעמודת ACRS Report Type
analyze_categorical_relationship('ACRS Report Type', 'Injury Severity')

# בדיקת קשר בין עמודת המטרה לעמודת Vehicle Damage Extent
analyze_categorical_relationship('Vehicle Damage Extent', 'Injury Severity')

#פונקציה להרצת גרפים לניתוח בין משתני הזמן למשתנה המטרה
def analyze_time_columns_vs_target(df):
    # 3 משתני הזמן
    time_columns = ['Crash Hour', 'Crash Day', 'Crash Month']
    target_column = 'Injury Severity'

    for col in time_columns:
        print(f"\n📊 ניתוח {col} מול {target_column}")
        print('-'*50)

        # Boxplot
        plt.figure(figsize=(10,6))
        sns.boxplot(x=target_column, y=col, data=df)
        plt.title(f'{col} by {target_column}')
        plt.ylabel(col)
        plt.xlabel(target_column)
        plt.xticks(rotation=45)
        plt.show()

        # ANOVA
        groups = [df[df[target_column] == g][col] for g in df[target_column].unique()]
        f_stat, p_val = f_oneway(*groups)

        print(f"ANOVA F-Statistic: {f_stat:.4f}")
        print(f"P-Value: {p_val:.4f}")

        if p_val < 0.05:
            print("✅ יש הבדל מובהק סטטיסטית בין הקבוצות (p < 0.05)")
        else:
            print("⚠️ אין הבדל מובהק סטטיסטית בין הקבוצות (p >= 0.05)")

# הרצת הפונקציה
analyze_time_columns_vs_target(df)

# טבלת שכיחויות: שעה מול חומרת פציעה
hour_injury_crosstab = pd.crosstab(df['Crash Hour'], df['Injury Severity'], normalize='index') * 100

plt.figure(figsize=(14,8))
sns.heatmap(hour_injury_crosstab, annot=True, fmt='.1f', cmap='coolwarm')
plt.title('Heatmap of Injury Severity by Crash Hour')
plt.ylabel('Crash Hour')
plt.xlabel('Injury Severity')
plt.show()

day_heatmap = pd.crosstab(df['Crash Day'], df['Injury Severity'])

plt.figure(figsize=(12, 6))
sns.heatmap(day_heatmap, annot=True, fmt='d', cmap='YlGnBu')
plt.title('Number of Crashes by Day of Week and Injury Severity')
plt.xlabel('Injury Severity')
plt.ylabel('Day of Week')
plt.tight_layout()
plt.show()

# טבלת שכיחויות
day_vs_severity = pd.crosstab(df['Crash Day'], df['Injury Severity'])

# גרף ברים מרובד
df_day_grouped = day_vs_severity.reset_index().melt(id_vars='Crash Day',
                                                    var_name='Injury Severity',
                                                    value_name='Count')
plt.figure(figsize=(10, 6))
sns.barplot(data=df_day_grouped, x='Crash Day', y='Count', hue='Injury Severity')
plt.title("Injury Severity by Day of Week")
plt.ylabel("Number of Crashes")
plt.xlabel("Day of Week")
plt.grid(axis='y')
plt.tight_layout()
plt.show()

# בדיקת קשר בין עמודת המטרה לעמודת Speed Limit

# Boxplot: קשר בין רמת הפציעה למהירות
plt.figure(figsize=(10, 6))
sns.boxplot(data=df, x='Injury Severity', y='Speed Limit')
plt.title('Speed Limit by Injury Severity')
plt.xlabel('Injury Severity')
plt.ylabel('Speed Limit')
plt.grid(True)
plt.show()

# סטטיסטיקות תיאוריות לפי קטגוריית הפציעה
summary_stats = df.groupby('Injury Severity')['Speed Limit'].describe()
display(summary_stats)

# בדיקת קשר בין עמודת המטרה לעמודת Speed Limit

# 🧹 ניפוי ערכים חסרים
df_speed = df[['Injury Severity', 'Speed Limit']].dropna()

# 🪜 הפיכת Speed Limit לקטגוריה (לדוגמה: bins של מהירות)
bins = [0, 30, 50, 70, 100, np.inf]
labels = ['0-30', '31-50', '51-70', '71+', '100+']
df_speed['Speed Category'] = pd.cut(df_speed['Speed Limit'], bins=bins, labels=labels)

# 📊 טבלת שכיחויות
contingency_table = pd.crosstab(df_speed['Speed Category'], df_speed['Injury Severity'])

# 🔥 heatmap
plt.figure(figsize=(10, 6))
sns.heatmap(contingency_table, annot=True, fmt='d', cmap='YlOrRd')
plt.title('Heatmap: Speed Category vs Injury Severity')
plt.xlabel('Injury Severity')
plt.ylabel('Speed Category')
plt.show()

# מבחן חי-בריבוע
chi2, p, dof, expected = stats.chi2_contingency(contingency_table)
n = contingency_table.sum().sum()
phi2 = chi2 / n
r, k = contingency_table.shape
cramers_v = np.sqrt(phi2 / min(k - 1, r - 1))


# הדפסה בסגנון שביקשת
print(f"Chi-Square Statistic: {chi2:.3f}")
print(f"P-Value: {p:.5e}")
print(f"Cramér's V: {cramers_v:.6f}")

# טבלת הצלבה עם אחוזים לכל שורת Route Type
route_vs_injury = pd.crosstab(df['Route Type'], df['Injury Severity'], normalize='index') * 100

plt.figure(figsize=(14, 10))
sns.heatmap(route_vs_injury, annot=True, fmt=".2f", cmap="Blues")
plt.title("Distribution (%) of Injury Severity by Route Type")
plt.ylabel("Route Type")
plt.xlabel("Injury Severity")
plt.tight_layout()
plt.show()

# חישוב מבחן חי בריבוע
chi2_stat, p_val, dof, expected = chi2_contingency(contingency_table)

# חישוב Cramér's V
n = contingency_table.sum().sum()
min_dim = min(contingency_table.shape) - 1
cramers_v = np.sqrt(chi2_stat / (n * min_dim))

# הדפסת תוצאות המבחן
print("Chi-Square Statistic:", chi2_stat)
print("P-Value:", p_val)
print("Cramér's V:", cramers_v)

#Driver Substance Abuse הקשר בין עמודת המטרה ל

def analyze_driver_substance_abuse(df, target_col='Injury Severity'):
    feature_col = 'Driver Substance Abuse'
    data = df[[feature_col, target_col]].dropna()

    # טבלת שכיחויות
    contingency = pd.crosstab(data[feature_col], data[target_col])

    # Heatmap
    plt.figure(figsize=(10, 6))
    sns.heatmap(contingency, annot=True, fmt='d', cmap='YlGnBu', linewidths=0.5, linecolor='gray')
    plt.title('Driver Substance Abuse vs Injury Severity')
    plt.xlabel('Injury Severity')
    plt.ylabel('Driver Substance Abuse')
    plt.tight_layout()
    plt.show()

    #עם אחוזים Heatmap
    # חישוב אחוזים לפי שורות
    row_percent = contingency.div(contingency.sum(axis=1), axis=0) * 100

    # ציור heatmap
    plt.figure(figsize=(14, 8))
    sns.heatmap(row_percent, annot=True, fmt=".1f", cmap="YlGnBu", linewidths=0.5, linecolor='gray', cbar_kws={'label': 'Percentage (%)'})

    plt.title(f"{feature_col} vs {target_col} (Row %)", fontsize=16)
    plt.xlabel(target_col, fontsize=12)
    plt.ylabel(feature_col, fontsize=12)
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

    # מבחן חי-בריבוע
    chi2, p, dof, expected = chi2_contingency(contingency)

    # Cramér's V
    n = contingency.sum().sum()
    phi2 = chi2 / n
    r, k = contingency.shape
    cramers_v = np.sqrt(phi2 / min(k - 1, r - 1))

    # תצוגת תוצאות בלבד
    print("\n🧪 תוצאות סטטיסטיות:")
    print(f"Chi-Square Statistic: {chi2:.3f}")
    print(f"P-Value: {p:.6e}")
    print(f"Cramér's V: {cramers_v:.6f}")
analyze_driver_substance_abuse(df)

#Vehicle Body Type הקשר בין עמודת המטרה ל

def analyze_vehicle_body_type(df, target_col='Injury Severity'):
    feature_col = 'Vehicle Body Type'
    data = df[[feature_col, target_col]].dropna()

    # צמצום לרכבים עם לפחות 100 מופעים (כדי לא להעמיס)
    top_types = data[feature_col].value_counts()[data[feature_col].value_counts() >= 100].index
    data = data[data[feature_col].isin(top_types)]

    # טבלת שכיחויות
    contingency = pd.crosstab(data[feature_col], data[target_col])

    # Heatmap
    plt.figure(figsize=(12, 7))
    sns.heatmap(contingency, annot=True, fmt='d', cmap='Oranges', linewidths=0.5, linecolor='gray')
    plt.title('Vehicle Body Type vs Injury Severity')
    plt.xlabel('Injury Severity')
    plt.ylabel('Vehicle Body Type')
    plt.tight_layout()
    plt.show()

    # סטטיסטיקות
    chi2, p, dof, expected = chi2_contingency(contingency)
    n = contingency.sum().sum()
    phi2 = chi2 / n
    r, k = contingency.shape
    cramers_v = np.sqrt(phi2 / min(k - 1, r - 1))

    print(f"\n🧪 תוצאות סטטיסטיות:")
    print(f"Chi-Square Statistic: {chi2:.3f}")
    print(f"P-Value: {p:.6e}")
    print(f"Cramér's V: {cramers_v:.6f}")

analyze_vehicle_body_type(df)

# ספירה כוללת של ערכים חסרים בכל העמודות
missing_summary = df.isna().sum()

# מציג רק עמודות שבהן יש לפחות ערך חסר אחד
missing_summary = missing_summary[missing_summary > 0].sort_values(ascending=False)

print("📊 עמודות עם ערכים חסרים:")
print(missing_summary)

# אחוזים – לראות יחסית לגודל הדאטה
missing_percent = (missing_summary / len(df)) * 100
print("\n📉 אחוזי חסרים:")
print(missing_percent)

# הצגת שמות כל העמודות
print(df.columns.tolist())

"""#CatBoost - המודל הנבחר
**חיזוי בינארי - יש פציעה/אין פציעה**
"""

from catboost import CatBoostClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, f1_score
from imblearn.over_sampling import SMOTENC  # גירסה של SMOTE שתומכת בעמודות קטגוריאליות
from sklearn.metrics import confusion_matrix
from sklearn.preprocessing import LabelEncoder
from collections import Counter #סופר כמה מופעים יש בכל קטגוריה בעמודת המטרה ומחזיר כמילון(עבור חישוב משקלים)
from dataclasses import dataclass
from typing import Dict, Any, List, Tuple
from catboost import CatBoostClassifier, Pool

# ===============================
# 1. ייבוא ספריות
# ===============================
import pandas as pd
import numpy as np
from catboost import CatBoostClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, f1_score
from imblearn.over_sampling import SMOTENC  # גירסה של SMOTE שתומכת בעמודות קטגוריאליות

# ===============================
# 2. יצירת עמודת מטרה בינארית
# ===============================
# סינון שורות שחסרה בהן חומרת פציעה
df = df[df['Injury Severity'].notna()].copy()

# המרה של עמודת הפציעה לערך בינארי: 0 = אין פציעה, 1 = יש פציעה
df['Injury Binary'] = df['Injury Severity'].apply(
    lambda x: 0 if x.strip().lower() == 'no apparent injury' else 1
)

# ===============================
# 3. הגדרת X ו-y
# ===============================
# הפרדת משתנים מסבירים (X) ועמודת המטרה (y)
X = df.drop(columns=['Injury Severity', 'Injury Binary'])
y = df['Injury Binary']

# זיהוי עמודות קטגוריאליות (object) והגדרתן כ־category – נוח לעבודה עם SMOTENC ו־CatBoost
cat_cols = X.select_dtypes(include='object').columns
for col in cat_cols:
    X[col] = X[col].astype('category')

# ===============================
# 4. פיצול ל־Train ו־Test
# ===============================
# פיצול הנתונים לסט אימון וסט בדיקה עם stratify לפי y לשמירה על פרופורציות
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.3,
    random_state=42,
    stratify=y
)

# ===============================
# 5. איזון הנתונים באמצעות SMOTENC
# ===============================
# ווידוא שעמודות קטגוריאליות מוגדרות כ־category גם ב־X_train
for col in cat_cols:
    X_train[col] = X_train[col].astype('category')

# יצירת רשימת אינדקסים של העמודות הקטגוריאליות עבור SMOTENC
cat_indices = [X_train.columns.get_loc(c) for c in cat_cols]

# יצירת מופע של SMOTENC עם העמודות הקטגוריאליות
smote_nc = SMOTENC(categorical_features=cat_indices, random_state=42)

# SMOTENC דורש numpy – המרה פנימית מתבצעת אוטומטית
X_train_resampled, y_train_resampled = smote_nc.fit_resample(X_train, y_train)

# החזרת הנתונים לתוך DataFrame (כמו המקור)
X_train_resampled = pd.DataFrame(X_train_resampled, columns=X_train.columns)

# ===============================
# 6. אימון מודל CatBoost
# ===============================
# יצירת מודל CatBoost עם פרמטרים מותאמים
model = CatBoostClassifier(
    iterations=500,
    learning_rate=0.05,
    depth=6,
    loss_function='Logloss',
    random_seed=42,
    verbose=0  # לא מציג תוצאות בכל איטרציה
)

# אימון המודל – העברת שמות העמודות הקטגוריאליות (ולא אינדקסים)
model.fit(
    X_train_resampled,
    y_train_resampled,
    cat_features=list(cat_cols)
)

from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# ===============================
# 7. חיזוי והערכת ביצועים
# ===============================
y_probs = model.predict_proba(X_test)[:, 1]


# סף החלטה של 0.25 – רגישות גבוהה יותר
threshold = 0.25
y_pred_thresh = (y_probs >= threshold).astype(int)

# דו"ח סיווג
print("\n📊 תוצאות CatBoost - פציעה / אין פציעה (סף = 0.25):")
print(classification_report(y_test, y_pred_thresh, digits=2))

# Macro F1
macro_f1 = f1_score(y_test, y_pred_thresh, average='macro')
print(f"\n🎯 Macro F1 Score (threshold = 0.25): {macro_f1:.4f}")

# ===============================
# מטריצת בלבול
# ===============================
cm = confusion_matrix(y_test, y_pred_thresh)
labels = ['No Injury', 'Injury']

print("\n📌 מטריצת בלבול:")
print(cm)

# גרף Heatmap
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
            xticklabels=labels, yticklabels=labels)
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix - CatBoost (Threshold=0.25)")
plt.tight_layout()
plt.show()

"""**בדיקה אם יש קשר בין הסתברויות חיזוי המודל לרמת חומרת הפציעה**"""

### רצינו לבדוק לפי הסתברויות במודל הנבחר(הבינארי) אם ההסתברות קרובה יותר ל 1 אז הסיכוי שבאמת חזינו נכון שיש פציעה יותר גבוה מאשר קצת פחות קרוב ל1 וככה אפשר גם לעזור לתעדף על ידי שנספק קודם לאלה שבטוח יש להם פציעה
results_bin_cat=df[['Injury Severity']].merge(y_test.to_frame(name='y_test_binary'),left_index=True,right_index=True)
results_bin_cat["y_prob"]=y_probs
results_bin_cat["y_pred"]=y_pred_thresh

# boxplot for each probability and injuery severity
results_bin_cat

plt.figure(figsize=(8, 5))
sns.boxplot(x='Injury Severity', y='y_prob', data=results_bin_cat, showfliers=False, boxprops=dict(facecolor='#4682B4'), medianprops=dict(color='red', linewidth=3))
plt.xticks(rotation=45, ha='right')
plt.xlabel('Injury Severity')
plt.ylabel('Predicted Probability of Injury')
plt.title('Predicted Probability by Injury Severity')
plt.tight_layout()
plt.show()

results_bin_cat

# נניח שיש לך DataFrame עם תחזיות בשם results_bin_cat
# הוא צריך לכלול לפחות את העמודות:
# 'Injury Severity' - חומרת הפציעה המקורית
# 'y_prob' - ההסתברות שחזתה המערכת לקטגוריית "יש פציעה"

# חישוב ממוצע, חציון וסטיית תקן לכל קטגוריה
summary = results_bin_cat.groupby('Injury Severity')['y_prob'].agg(
    mean='mean',
    median='median',
    std='std'
).reset_index()

print(summary)

#save the model - locally
import pickle
with open("catboost_model.pkl", "wb") as f:
    pickle.dump(model, f)

"""**ניסיון לשנות את הסף של המודל**"""

#ניסיון לשנות את הסף למספרים אחרים

# ===============================
# 7. חיזוי והערכת ביצועים
# ===============================
# חיזוי הסתברויות למחלקה "יש פציעה"
y_probs = model.predict_proba(X_test)[:, 1]

# סף החלטה של 0.3 – מאפשר רגישות גבוהה יותר לאיתור פציעות
threshold = 0.3
y_pred_thresh = (y_probs >= threshold).astype(int)

# הדפסת דו"ח סיווג
print("\n📊 תוצאות CatBoost - פציעה / אין פציעה (סף = 0.3):")
print(classification_report(y_test, y_pred_thresh, digits=2))

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# יצירת מטריצת בלבול
cm = confusion_matrix(y_test, y_pred_thresh)

# הגדרת תוויות הקטגוריות
labels = ['No Injury (0)', 'Injury (1)']

# ציור מטריצה
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix - Injury Prediction (Threshold = 0.3)')
plt.tight_layout()
plt.show()

"""# הרצת מודלים נוספים

---

CatBoost
---
"""

from catboost import CatBoostClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, f1_score
from imblearn.over_sampling import SMOTENC  # גירסה של SMOTE שתומכת בעמודות קטגוריאליות
from sklearn.metrics import confusion_matrix
from sklearn.preprocessing import LabelEncoder
from collections import Counter #סופר כמה מופעים יש בכל קטגוריה בעמודת המטרה ומחזיר כמילון(עבור חישוב משקלים)
from dataclasses import dataclass
from typing import Dict, Any, List, Tuple
from catboost import CatBoostClassifier, Pool

""">**מודל 5 קטגרויות**"""

# ========= שלב 1: ניקוי ותיוג =========
df = df[df['Injury Severity'].notna()].copy()
df['Injury Severity'] = df['Injury Severity'].str.lower().str.strip()

X = df.drop(columns=['Injury Severity'])
y_raw = df['Injury Severity']

# קידוד מטרות
le = LabelEncoder()
y = le.fit_transform(y_raw)

# ========= שלב 2: פיצול =========
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, stratify=y, random_state=42
)

# ========= שלב 3: הגדרת עמודות קטגוריאליות =========
cat_features = X.select_dtypes(include='object').columns.tolist()
for col in cat_features:
    X_train[col] = X_train[col].astype('category')
    X_test[col] = X_test[col].astype('category')

# ========= שלב 4: חישוב class_weights =========
# מחושבים לפי שכיחות הפוכה – ככל שקטגוריה נדירה יותר, היא תקבל משקל גבוה יותר
from collections import Counter

class_counts = Counter(y_train)
total = sum(class_counts.values())
class_weights = {
    k: total / (len(class_counts) * v)
    for k, v in class_counts.items()
}
print("Class Weights:", class_weights)

# ========= שלב 5: אימון CatBoost =========
model = CatBoostClassifier(
    iterations=300,
    learning_rate=0.05,
    depth=6,
    loss_function='MultiClass',
    eval_metric='TotalF1',  # שיפור Recall כללי
    class_weights=class_weights,
    cat_features=cat_features,
    random_state=42,
    verbose=100
)

train_pool = Pool(X_train, y_train, cat_features=cat_features)
test_pool = Pool(X_test, y_test, cat_features=cat_features)

model.fit(train_pool)

# ========= שלב 6: חיזוי =========
y_pred = model.predict(test_pool).flatten()
target_names = le.inverse_transform(np.unique(y))

print("\n📊 דו\"ח סיווג – TEST (5 קטגוריות):")
print(classification_report(y_test, y_pred, target_names=target_names))

# ========= שלב 7: מטריצת בלבול =========
plt.figure(figsize=(8,6))
sns.heatmap(confusion_matrix(y_test, y_pred),
            annot=True, fmt='d', cmap='Blues',
            xticklabels=target_names, yticklabels=target_names)
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("CatBoost – Confusion Matrix (5-Class Prediction)")
plt.tight_layout()
plt.show()

"""> **מודל: 3 קטגוריות - אין פציעה/פציעה קלה/פציעה חמורה**

"""

# ===============================
# 1. מיפוי ל־3 קטגוריות
# ===============================
injury_mapping = {
    'no apparent injury': 'no injury',
    'possible injury': 'minor injury',
    'suspected minor injury': 'minor injury',
    'suspected serious injury': 'severe injury',
    'fatal injury': 'severe injury'
}

df = df[df['Injury Severity'].isin(injury_mapping.keys())].copy()
df['Injury Severity'] = df['Injury Severity'].map(injury_mapping)

# ===============================
# 2. הכנת נתונים
# ===============================
X = df.drop(columns=['Injury Severity'])
y = df['Injury Severity']

# זיהוי עמודות קטגוריאליות
cat_cols = X.select_dtypes(include='object').columns
for col in cat_cols:
    X[col] = X[col].astype('category')

# קידוד y
le = LabelEncoder()
y_encoded = le.fit_transform(y)

# פיצול ל-Train/Test
X_train, X_test, y_train, y_test = train_test_split(
    X, y_encoded, test_size=0.3, random_state=42, stratify=y_encoded
)

# ===============================
# 3. חישוב class_weights
# ===============================
counter = Counter(y_train)
total = sum(counter.values())
class_weights = [total / counter[i] for i in range(len(counter))]

print("🎯 class_weights:", class_weights)

# ===============================
# 4. אימון CatBoost עם משקלים
# ===============================
cat_features_indices = [X.columns.get_loc(col) for col in cat_cols]

model = CatBoostClassifier(
    iterations=500,
    learning_rate=0.05,
    depth=6,
    loss_function='MultiClass',
    class_weights=class_weights,
    random_seed=42,
    verbose=0
)

model.fit(X_train, y_train, cat_features=cat_features_indices)

# ===============================
# 5. חיזוי והערכת ביצועים
# ===============================
y_pred = model.predict(X_test).flatten()

print("\n📊 תוצאות CatBoost (עם class_weights):")
print(classification_report(y_test, y_pred, target_names=le.classes_))

macro_f1 = f1_score(y_test, y_pred, average='macro')
print(f"\n🎯 Macro F1: {macro_f1:.4f}")

"""> **מודל דו-שלבי**

שלב 1: יש פציעה/אין פציעה
שלב 2: פציעה קטלנית/קלה/קשה

"""

# ---------------- הגדרות ----------------

@dataclass
class CatConfig:
    random_state: int = 42
    test_size: float = 0.30
    threshold: float = 0.25
    use_smotenc: bool = True
    cat_params_stage1: Dict[str, Any] = None
    cat_params_stage2: Dict[str, Any] = None
    smotenc_params: Dict[str, Any] = None

    def __post_init__(self):
        if self.cat_params_stage1 is None:
            self.cat_params_stage1 = dict(
                iterations=500,
                learning_rate=0.05,
                depth=6,
                loss_function='Logloss',  # 🔹 Binary classification
                random_seed=self.random_state,
                verbose=0
            )
        if self.cat_params_stage2 is None:
            self.cat_params_stage2 = dict(
                iterations=500,
                learning_rate=0.05,
                depth=6,
                loss_function='MultiClass',  # 🔹 Multiclass classification
                random_seed=self.random_state,
                verbose=0
            )
        if self.smotenc_params is None:
            self.smotenc_params = dict(random_state=self.random_state)


# ---------------- פונקציות עזר ----------------

def print_cm(y_true, y_pred, title):
    cm = confusion_matrix(y_true, y_pred)
    print(f"\n📊 {title}")
    print(classification_report(y_true, y_pred, digits=2))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.xlabel("Predicted"); plt.ylabel("True"); plt.title(title)
    plt.show()

def split_xy(X, y, cfg):
    return train_test_split(X, y, test_size=cfg.test_size,
                            random_state=cfg.random_state, stratify=y)

def apply_smotenc(X_train, y_train, cat_cols, cfg):
    for c in cat_cols:
        X_train[c] = X_train[c].astype('category')
    cat_indices = [X_train.columns.get_loc(c) for c in cat_cols]
    smote = SMOTENC(categorical_features=cat_indices, **cfg.smotenc_params)
    X_res, y_res = smote.fit_resample(X_train, y_train)
    return pd.DataFrame(X_res, columns=X_train.columns), y_res


# ---------------- שלב 1: יש פציעה / אין פציעה ----------------

def preprocess_stage1(df):
    df = df[df['Injury Severity'].notna()].copy()
    df['Injury Binary'] = df['Injury Severity'].str.lower().str.strip().apply(
        lambda x: 0 if x == 'no apparent injury' else 1
    )
    return df

def build_xy_binary(df):
    X = df.drop(columns=['Injury Severity', 'Injury Binary'])
    y = df['Injury Binary']
    cat_cols = X.select_dtypes(include='object').columns.tolist()
    for c in cat_cols:
        X[c] = X[c].astype('category')
    return X, y, cat_cols

def train_stage1(df, cfg):
    df = preprocess_stage1(df)
    X, y, cat_cols = build_xy_binary(df)
    X_train, X_test, y_train, y_test = split_xy(X, y, cfg)

    if cfg.use_smotenc:
        X_train, y_train = apply_smotenc(X_train.copy(), y_train.copy(), cat_cols, cfg)

    model = CatBoostClassifier(**cfg.cat_params_stage1)
    model.fit(Pool(X_train, y_train, cat_features=cat_cols))

    y_prob = model.predict_proba(Pool(X_test, cat_features=cat_cols))[:, 1]
    y_pred = (y_prob >= cfg.threshold).astype(int)

    print_cm(y_test, y_pred, "Stage 1: Injury / No Injury")

    return model, cat_cols


# ---------------- שלב 2: קלה / חמורה / קטלנית ----------------

injury_mapping = {
    'possible injury': 'light injury',
    'suspected minor injury': 'light injury',
    'suspected serious injury': 'serious injury',
    'fatal injury': 'fatal injury',
    'no apparent injury': None
}

def prepare_stage2_data(df):
    df = df[df['Injury Severity'].notna()].copy()
    df['Injury Mapped'] = df['Injury Severity'].str.lower().str.strip().map(injury_mapping)
    df = df[df['Injury Mapped'].notna()].copy()

    X = df.drop(columns=['Injury Severity', 'Injury Binary', 'Injury Mapped'], errors='ignore')
    y = df['Injury Mapped']
    cat_cols = X.select_dtypes(include='object').columns.tolist()
    for c in cat_cols:
        X[c] = X[c].astype('category')

    le = LabelEncoder()
    y_enc = le.fit_transform(y)

    return X, y_enc, cat_cols, le

def train_stage2(df, cfg):
    X, y, cat_cols, le = prepare_stage2_data(df)
    X_train, X_test, y_train, y_test = split_xy(X, y, cfg)

    if cfg.use_smotenc:
        X_train, y_train = apply_smotenc(X_train.copy(), pd.Series(y_train), cat_cols, cfg)

    model = CatBoostClassifier(**cfg.cat_params_stage2)
    model.fit(Pool(X_train, y_train, cat_features=cat_cols))

    y_pred = model.predict(Pool(X_test, cat_features=cat_cols)).astype(int)
    y_test_lbl = le.inverse_transform(y_test)
    y_pred_lbl = le.inverse_transform(y_pred)

    print_cm(y_test_lbl, y_pred_lbl, "Stage 2: Injury Type (Light / Serious / Fatal)")

    return model, le


# ---------------- Pipeline מלא ----------------

def run_catboost_2stage_pipeline(df, cfg):
    print("🔹 Stage 1: Injury / No Injury")
    model1, cat_cols1 = train_stage1(df.copy(), cfg)

    print("\n🔹 Stage 2: Injury Type (Only on Injured cases)")
    df2 = df[df['Injury Severity'].str.lower().str.strip() != 'no apparent injury'].copy()
    model2, label_encoder = train_stage2(df2, cfg)

    return model1, model2, label_encoder



cfg = CatConfig()
model1, model2, le = run_catboost_2stage_pipeline(df, cfg)

"""LightGBM
---


"""

import lightgbm as lgb
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix
from imblearn.over_sampling import SMOTE
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.utils.class_weight import compute_class_weight
from sklearn.model_selection import GridSearchCV

from dataclasses import dataclass
from typing import Dict, Any, Optional, Tuple, List
from sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve
from imblearn.over_sampling import SMOTE

""">**מודל יש פציעה/אין פציעה**

"""

# ✨ יצירת עמודת פציעה / אין פציעה
df = df.dropna(subset=['Injury Severity'])
df['Injury Binary'] = df['Injury Severity'].apply(lambda x: 0 if x == 'no apparent injury' else 1)

# 🔹 שלב ראשון – פציעה / אין פציעה
X = df.drop(['Injury Severity', 'Injury Binary'], axis=1)
y = df['Injury Binary']
X = pd.get_dummies(X, drop_first=True)
X.columns = X.columns.str.replace('[^A-Za-z0-9_]+', '_', regex=True)
X = X.drop(columns=X.select_dtypes(include=['datetime64']).columns)
X = X.astype(float)

X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)
X_train1, y_train1 = SMOTE(random_state=42).fit_resample(X_train1, y_train1)

model1 = lgb.LGBMClassifier(n_estimators=500, max_depth=10, learning_rate=0.03, class_weight='balanced', random_state=42)
model1.fit(X_train1, y_train1)

probs1 = model1.predict_proba(X_test1)[:,1]
thresh1 = 0.2473  # הורדנו את הסף כדי לשפר Recall לפציעות
y_pred1 = (probs1 >= thresh1).astype(int)
# 0.5
print("\nחיזוי  – פציעה / אין פציעה")
print(classification_report(y_test1, y_pred1))
sns.heatmap(confusion_matrix(y_test1, y_pred1), annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix - Stage 1")
plt.show()

"""ניסיון למציאת הסף האופטימלי(לא הצליח)"""

# from sklearn.metrics import precision_recall_curve

# # חישוב Precision ו-Recall עבור כל סף אפשרי
# precision, recall, thresholds = precision_recall_curve(y_test1, probs1)

# # חישוב ה-F1 לכל סף כדי למצוא את האופטימלי (אם רוצים)
# f1_scores = 2 * (precision * recall) / (precision + recall)
# best_idx = np.argmax(f1_scores)
# best_threshold = thresholds[best_idx]
# best_f1 = f1_scores[best_idx]

# # ציור הגרף
# plt.figure(figsize=(8,6))
# plt.plot(thresholds, precision[:-1], label="Precision", color='royalblue')
# plt.plot(thresholds, recall[:-1], label="Recall", color='lightseagreen')
# plt.plot(thresholds, f1_scores[:-1], label="F1 Score", color='gray', linestyle='--')

# # # סימון הנקודה של threshold=0.3
# # plt.axvline(x=0.3, color='red', linestyle='--')

# # סימון הנקודה הכי טובה לפי F1
# plt.axvline(x=best_threshold, color='green', linestyle='--', label=f'Best F1 (th={best_threshold:.2f})')

# plt.xlabel("Threshold")
# plt.ylabel("Score")
# plt.title("Precision / Recall / F1 by Threshold")
# plt.legend()
# plt.grid()
# plt.show()

# #ניסיון לשנות כל פעם את הסף thresh1 למספר אחר
# #ניסינו 0.5,0.38

# probs1 = model1.predict_proba(X_test1)[:,1]
# thresh1 = 0.38  # שינינו את הסף כדי לשפר Recall לפציעות
# y_pred1 = (probs1 >= thresh1).astype(int)

# print("\nשלב 1 – פציעה / אין פציעה")
# print(classification_report(y_test1, y_pred1))
# sns.heatmap(confusion_matrix(y_test1, y_pred1), annot=True, fmt='d', cmap='Blues')
# plt.title("Confusion Matrix - Stage 1")
# plt.show()

""">**מודל 5 קטגרויות**"""

# ✅ שלב 1: ניקוי עמודת המטרה
df = df.dropna(subset=['Injury Severity'])
df['Injury Severity'] = df['Injury Severity'].str.lower().str.strip()

# ✅ שלב 2: קידוד המטרה ל־Label
le = LabelEncoder()
df['Injury_Label'] = le.fit_transform(df['Injury Severity'])  # נשמר הסדר המקורי של 5 הקטגוריות

# ✅ שלב 3: הכנת פיצ'רים
X = df.drop(columns=['Injury Severity', 'Injury_Label'])
y = df['Injury_Label']

# קידוד קטגוריות
X = pd.get_dummies(X, drop_first=True)
X.columns = X.columns.str.replace('[^A-Za-z0-9_]+', '_', regex=True)
X = X.drop(columns=X.select_dtypes(include=['datetime64']).columns)
X = X.astype(float)

# ✅ חלוקה ל־train/test + SMOTE מותאם לריבוי קטגוריות
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.3, random_state=42)
smote = SMOTE(random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

# ✅ שלב 4: אימון מודל LightGBM לריבוי קטגוריות
model = lgb.LGBMClassifier(
    objective='multiclass',
    num_class=5,
    n_estimators=300,
    max_depth=6,
    learning_rate=0.05,
    class_weight='balanced',
    random_state=42
)

model.fit(X_train_smote, y_train_smote)

# ✅ שלב 5: תחזיות והערכת ביצועים
y_pred = model.predict(X_test)

# שחזור שמות הקטגוריות לדו"ח
label_names = le.classes_
print("🔍 ביצועים על סט הבדיקה – חיזוי חמש קטגוריות:")
print(classification_report(y_test, y_pred, target_names=label_names))

# ✅ מטריצת בלבול
plt.figure(figsize=(8, 6))
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues',
            xticklabels=label_names, yticklabels=label_names)
plt.title("Confusion Matrix – Multiclass Injury Severity")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.tight_layout()
plt.show()

"""> **מודל 3 קטגוריות**"""

#עם סף לערך severe injury

# ✨ קיבוץ ל-3 קטגוריות
def map_injury(severity):
    severity = severity.strip().lower()
    if severity == 'no apparent injury':
        return 'no injury'
    elif severity in ['possible injury', 'suspected minor injury']:
        return 'minor injury'
    else:
        return 'severe injury'

df['Injury Group'] = df['Injury Severity'].apply(map_injury)

# 🧪 קלט ופלט
X = df.drop(columns=['Injury Severity', 'Injury Group'])
y = df['Injury Group']

# המרת משתנים קטגוריאליים
X = pd.get_dummies(X, drop_first=True)
X.columns = X.columns.str.replace('[^A-Za-z0-9_]+', '_', regex=True)
X = X.drop(columns=X.select_dtypes(include=['datetime64']).columns)
X = X.astype(float)

# פיצול
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.3, random_state=42)

# SMOTE רק אם יש חוסר איזון גדול
X_train, y_train = SMOTE(random_state=42).fit_resample(X_train, y_train)

# משקלים לקטגוריות
weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)
class_weight_dict = dict(zip(np.unique(y_train), weights))

# מודל LightGBM
model = lgb.LGBMClassifier(
    n_estimators=700,
    max_depth=12,
    learning_rate=0.02,
    num_leaves=40,
    class_weight=class_weight_dict,
    random_state=42
)
model.fit(X_train, y_train)

# 🔹 שימוש ב-threshold רק לקטגוריה severe injury
threshold = 0.3  # אפשר לשחק עם הערך (0.25, 0.35 וכו')

probs = model.predict_proba(X_test)
severe_index = list(model.classes_).index('severe injury')
probs_severe = probs[:, severe_index]

# תחזית רגילה של המודל
y_pred_default = model.predict(X_test)

# אם ההסתברות ל-severe injury >= threshold → ננבא severe injury
y_pred = np.where(probs_severe >= threshold, 'severe injury', y_pred_default)

# 📈 תוצאות עם threshold מותאם ל-severe
print(f"\n📈 דו\"ח סיווג – עם threshold={threshold} ל-severe injury:")
print(classification_report(y_test, y_pred))

plt.figure(figsize=(8,6))
sns.heatmap(confusion_matrix(y_test, y_pred, labels=model.classes_),
            annot=True, fmt='d', cmap='Blues',
            xticklabels=model.classes_,
            yticklabels=model.classes_)
plt.title(f"Confusion Matrix – threshold={threshold} for severe injury")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.show()

"""
> **מודל דו שלבי**


*   שלב 1: פציעה/אין פציעה
*   שלב 2: פציעה קלה/קשה-קטלנית
"""

# ✨ יצירת עמודת פציעה / אין פציעה
df = df.dropna(subset=['Injury Severity'])
df['Injury Binary'] = df['Injury Severity'].apply(lambda x: 0 if x == 'no apparent injury' else 1)

# 🔹 שלב ראשון – פציעה / אין פציעה
X = df.drop(['Injury Severity', 'Injury Binary'], axis=1)
y = df['Injury Binary']
X = pd.get_dummies(X, drop_first=True)
X.columns = X.columns.str.replace('[^A-Za-z0-9_]+', '_', regex=True)
X = X.drop(columns=X.select_dtypes(include=['datetime64']).columns)
X = X.astype(float)

X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)
X_train1, y_train1 = SMOTE(random_state=42).fit_resample(X_train1, y_train1)

model1 = lgb.LGBMClassifier(n_estimators=500, max_depth=10, learning_rate=0.03, class_weight='balanced', random_state=42)
model1.fit(X_train1, y_train1)

probs1 = model1.predict_proba(X_test1)[:,1]
thresh1 = 0.3  # הורדנו את הסף כדי לשפר Recall לפציעות
y_pred1 = (probs1 >= thresh1).astype(int)

print("\nשלב 1 – פציעה / אין פציעה")
print(classification_report(y_test1, y_pred1))
sns.heatmap(confusion_matrix(y_test1, y_pred1), annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix - Stage 1")
plt.show()

# 🔸 שלב שני – קלה (0) מול קשה/קטלנית (1)
df_stage2 = df[df['Injury Binary'] == 1].copy()

# יצירת עמודת יעד חדשה
df_stage2['Severe Binary'] = df_stage2['Injury Severity'].apply(
    lambda x: 1 if x in ['fatal injury', 'suspected serious injury'] else 0
)

# הגדרת X ו-y
X2 = df_stage2.drop(['Injury Severity', 'Injury Binary', 'Severe Binary'], axis=1)
y2 = df_stage2['Severe Binary']

# הכנה – בדיוק כמו בשלב הראשון
X2 = pd.get_dummies(X2, drop_first=True)
X2.columns = X2.columns.str.replace('[^A-Za-z0-9_]+', '_', regex=True)
X2 = X2.drop(columns=X2.select_dtypes(include=['datetime64']).columns)
X2 = X2.astype(float)

# פיצול + SMOTE
X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, test_size=0.3, stratify=y2, random_state=42)

# חיזוק קטגוריית המיעוט (פציעה קשה)
strategy = {1: y_train2.value_counts()[1]*3}  # אפשר לכוונן את המכפלה
X_train2, y_train2 = SMOTE(sampling_strategy=strategy, random_state=42).fit_resample(X_train2, y_train2)

# מודל
model2 = lgb.LGBMClassifier(n_estimators=500, max_depth=10, learning_rate=0.03,
                            class_weight='balanced', random_state=42)
model2.fit(X_train2, y_train2)

# חיזוי
probs2 = model2.predict_proba(X_test2)[:,1]
thresh2 = 0.3
y_pred2 = (probs2 >= thresh2).astype(int)

# תצוגת ביצועים
print("\nשלב 2 – פציעה קלה (0) מול קשה/קטלנית (1)")
print(classification_report(y_test2, y_pred2))
sns.heatmap(confusion_matrix(y_test2, y_pred2), annot=True, fmt='d', cmap='Oranges')
plt.title("Confusion Matrix - Stage 2: Minor vs Severe")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.show()

"""

> **מודל דו שלבי**

*   שלב 1: יש פציעה/אין פציעה
*   שלב שני: 3 קטגוריות- פציעה קלה/קשה/קטלנית



---

"""

# 🧠 הורדת שורות ללא ערך בעמודת המטרה
df = df.dropna(subset=['Injury Severity'])
df['Injury Severity'] = df['Injury Severity'].str.lower().str.strip()

# ✨ יצירת עמודת פציעה / אין פציעה
df['Injury Binary'] = df['Injury Severity'].apply(lambda x: 0 if x == 'no apparent injury' else 1)

# 🟡 המרה לקטגוריות
for col in df.select_dtypes(include='object').columns:
    df[col] = df[col].astype('category')

# 🔹 שלב ראשון – חיזוי פציעה / אין פציעה
X1 = df.drop(columns=['Injury Severity', 'Injury Binary'])
y1 = df['Injury Binary']

X_train1, X_test1, y_train1, y_test1 = train_test_split(X1, y1, test_size=0.3, stratify=y1, random_state=42)

lgb_model1 = lgb.LGBMClassifier(
    n_estimators=300, max_depth=7, learning_rate=0.05,
    class_weight='balanced', random_state=42
)
lgb_model1.fit(X_train1, y_train1)

y_probs1 = lgb_model1.predict_proba(X_test1)[:, 1]
threshold = 0.3
y_pred1 = (y_probs1 >= threshold).astype(int)

print("\n📈 דו\"ח סיווג - שלב ראשון (פציעה / אין פציעה):")
print(classification_report(y_test1, y_pred1))

plt.figure(figsize=(8,6))
sns.heatmap(confusion_matrix(y_test1, y_pred1), annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix - Stage 1 (Injury/No Injury)')
plt.show()

# 🔸 שלב שני – חיזוי סוג הפציעה
injury_mapping = {
    'possible injury': 'light injury',
    'suspected minor injury': 'light injury',
    'suspected serious injury': 'serious injury',
    'fatal injury': 'fatal injury'
}
df_stage2 = df[df['Injury Severity'].isin(injury_mapping.keys())].copy()
df_stage2['Injury Mapped'] = df_stage2['Injury Severity'].map(injury_mapping)

X2 = df_stage2.drop(columns=['Injury Severity', 'Injury Binary', 'Injury Mapped'])
y2 = df_stage2['Injury Mapped'].astype('category')

X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, test_size=0.3, stratify=y2, random_state=42)

# ⚖️ חישוב משקלים לקטגוריות
class_weights = compute_class_weight(
    class_weight='balanced',
    classes=np.unique(y_train2),
    y=y_train2
)
class_weight_dict = dict(zip(np.unique(y_train2), class_weights))

# 🚀 מודל שלב שני
lgb_model2 = lgb.LGBMClassifier(
    n_estimators=300, max_depth=7, learning_rate=0.05,
    class_weight=class_weight_dict, random_state=42
)
lgb_model2.fit(X_train2, y_train2)

y_pred2 = lgb_model2.predict(X_test2)

print("\n📈 דו\"ח סיווג - שלב שני (3 קטגוריות):")
print(classification_report(y_test2, y_pred2))

plt.figure(figsize=(8,6))
sns.heatmap(confusion_matrix(y_test2, y_pred2, labels=lgb_model2.classes_),
            annot=True, fmt='d', cmap='Blues',
            xticklabels=lgb_model2.classes_, yticklabels=lgb_model2.classes_)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title("Confusion Matrix - Stage 2 (Injury Type)")
plt.show()

"""
> **מודל תלת שלבי**


*   מודל 1: יש פציעה/אין פציעה
*   מודל שני: פציעה קטלנית/לא קטלנית
*   מודל 3: פציעה חמורה/ קלה

---

"""

# ✅ שיפור כולל למודל LightGBM בשלושה שלבים: פציעה / קטלנית / רמת חומרה

# ✨ יצירת עמודת פציעה / אין פציעה
df = df.dropna(subset=['Injury Severity'])
df['Injury Binary'] = df['Injury Severity'].apply(lambda x: 0 if x == 'no apparent injury' else 1)

# 🔹 שלב ראשון – פציעה / אין פציעה
X = df.drop(['Injury Severity', 'Injury Binary'], axis=1)
y = df['Injury Binary']
X = pd.get_dummies(X, drop_first=True)
X.columns = X.columns.str.replace('[^A-Za-z0-9_]+', '_', regex=True)
X = X.drop(columns=X.select_dtypes(include=['datetime64']).columns)
X = X.astype(float)

X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)
X_train1, y_train1 = SMOTE(random_state=42).fit_resample(X_train1, y_train1)

model1 = lgb.LGBMClassifier(n_estimators=500, max_depth=10, learning_rate=0.03, class_weight='balanced', random_state=42)
model1.fit(X_train1, y_train1)

probs1 = model1.predict_proba(X_test1)[:,1]
thresh1 = 0.3  # הורדנו את הסף כדי לשפר Recall לפציעות
y_pred1 = (probs1 >= thresh1).astype(int)
# 0.5
print("\nשלב 1 – פציעה / אין פציעה")
print(classification_report(y_test1, y_pred1))
sns.heatmap(confusion_matrix(y_test1, y_pred1), annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix - Stage 1")
plt.show()

# 🔸 שלב שני – פציעה קטלנית / לא קטלנית
df_stage2 = df[df['Injury Binary'] == 1].copy()
df_stage2['Fatal Binary'] = df_stage2['Injury Severity'].apply(lambda x: 1 if x == 'fatal injury' else 0)

X2 = df_stage2.drop(['Injury Severity', 'Injury Binary', 'Fatal Binary'], axis=1)
y2 = df_stage2['Fatal Binary']
X2 = pd.get_dummies(X2, drop_first=True)
X2.columns = X2.columns.str.replace('[^A-Za-z0-9_]+', '_', regex=True)
X2 = X2.drop(columns=X2.select_dtypes(include=['datetime64']).columns)
X2 = X2.astype(float)

X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, test_size=0.3, random_state=42, stratify=y2)
X_train2, y_train2 = SMOTE(sampling_strategy={1: y_train2.value_counts()[1]*4}, random_state=42).fit_resample(X_train2, y_train2)

model2 = lgb.LGBMClassifier(n_estimators=500, max_depth=10, learning_rate=0.03, class_weight='balanced', random_state=42)
model2.fit(X_train2, y_train2)

probs2 = model2.predict_proba(X_test2)[:,1]
thresh2 = 0.3
y_pred2 = (probs2 >= thresh2).astype(int)

print("\nשלב 2 – פציעה קטלנית / לא קטלנית")
print(classification_report(y_test2, y_pred2))
sns.heatmap(confusion_matrix(y_test2, y_pred2), annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix - Stage 2")
plt.show()

# 🔻 שלב שלישי – קלה / חמורה
stage3_df = df_stage2[df_stage2['Injury Severity'].isin(['possible injury', 'suspected minor injury', 'suspected serious injury'])].copy()
stage3_df['Severity'] = stage3_df['Injury Severity'].apply(lambda x: 'serious' if x == 'suspected serious injury' else 'light')

X3 = stage3_df.drop(['Injury Severity', 'Injury Binary', 'Fatal Binary', 'Severity'], axis=1)
y3 = stage3_df['Severity']
le3 = LabelEncoder()
y3_encoded = le3.fit_transform(y3)

X3 = pd.get_dummies(X3, drop_first=True)
X3.columns = X3.columns.str.replace('[^A-Za-z0-9_]+', '_', regex=True)
X3 = X3.drop(columns=X3.select_dtypes(include=['datetime64']).columns)
X3 = X3.astype(float)

X_train3, X_test3, y_train3, y_test3 = train_test_split(X3, y3_encoded, test_size=0.3, random_state=42, stratify=y3_encoded)
X_train3, y_train3 = SMOTE(sampling_strategy={1: y_train3.tolist().count(1)*3}, random_state=42).fit_resample(X_train3, y_train3)

model3 = lgb.LGBMClassifier(n_estimators=500, max_depth=10, learning_rate=0.03, class_weight='balanced', random_state=42)
model3.fit(X_train3, y_train3)
y_pred3 = model3.predict(X_test3)

print("\nשלב 3 – חומרת פציעה: קלה / חמורה")
print(classification_report(le3.inverse_transform(y_test3), le3.inverse_transform(y_pred3)))
sns.heatmap(confusion_matrix(le3.inverse_transform(y_test3), le3.inverse_transform(y_pred3)), annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix - Stage 3")
plt.show()

"""
> **(מודולרי)מודל תלת שלבי**


*   מודל 1: יש פציעה/אין פציעה
*   מודל שני: פציעה קטלנית/לא קטלנית
*   מודל 3: פציעה חמורה/ קלה

---

"""

# # פייפליין מודולרי של LightGBM (3 שלבים) + דוחות אימון/בדיקה + הדפסת סף
# # ---------------- Config ----------------

## @dataclass
# class LGBConfig:
#     random_state: int = 42
#     test_size: float = 0.30
#     use_smote: bool = True

#     stage1_threshold_strategy: str = "f2"
#     stage1_fixed_threshold: float = 0.30
#     stage1_recall_floor: float = 0.95

#     lgb_stage1: Dict[str, Any] = None
#     lgb_stage2: Dict[str, Any] = None
#     lgb_stage3: Dict[str, Any] = None

#     smote_stage1: Dict[str, Any] = None
#     smote_stage2: Dict[str, Any] = None
#     smote_stage3: Dict[str, Any] = None

#     def __post_init__(self):
#         if self.lgb_stage1 is None:
#             self.lgb_stage1 = dict(n_estimators=500, max_depth=10, learning_rate=0.03,
#                                    class_weight='balanced', random_state=self.random_state)
#         if self.lgb_stage2 is None:
#             self.lgb_stage2 = dict(n_estimators=500, max_depth=10, learning_rate=0.03,
#                                    class_weight='balanced', random_state=self.random_state)
#         if self.lgb_stage3 is None:
#             self.lgb_stage3 = dict(n_estimators=500, max_depth=10, learning_rate=0.03,
#                                    class_weight='balanced', random_state=self.random_state)

#         if self.smote_stage1 is None:
#             self.smote_stage1 = dict(random_state=self.random_state)
#         if self.smote_stage2 is None:
#             self.smote_stage2 = dict(random_state=self.random_state)
#         if self.smote_stage3 is None:
#             self.smote_stage3 = dict(random_state=self.random_state)


# # ---------------- Utils ----------------

# def sanitize_features(X: pd.DataFrame) -> pd.DataFrame:
#     X = X.copy()
#     dt_cols = X.select_dtypes(include=['datetime64[ns]', 'datetime64[ns, UTC]']).columns
#     X.drop(columns=dt_cols, inplace=True)
#     for c in X.columns:
#         if X[c].dtype == 'O':
#             X[c] = X[c].astype(str)
#     X.columns = X.columns.str.replace('[^A-Za-z0-9_]+', '_', regex=True)
#     return X

# def one_hot_all(X: pd.DataFrame) -> pd.DataFrame:
#     X = sanitize_features(X)
#     X = pd.get_dummies(X, drop_first=True)
#     X.columns = X.columns.str.replace('[^A-Za-z0-9_]+', '_', regex=True)
#     X = X.astype(float)
#     return X

# def check_bad_feature_names(X: pd.DataFrame):
#     bad_cols = [c for c in X.columns if any(ch in c for ch in ['"', "'", "{", "}", "[", "]", ":", ",", " ", "/", "\\", "#", "?", "&", "="])]
#     if bad_cols:
#         print("⚠ נמצאו עמודות בעייתיות:", bad_cols)
#     else:
#         print("✅ אין עמודות בעייתיות")

# def split_xy(X: pd.DataFrame, y: np.ndarray, cfg: LGBConfig):
#     return train_test_split(X, y, test_size=cfg.test_size, random_state=cfg.random_state, stratify=y)

# def print_report(title: str, y_true, y_pred):
#     print(f"\n=== {title} ===")
#     print(classification_report(y_true, y_pred, digits=2))
#     print("Confusion Matrix:")
#     print(confusion_matrix(y_true, y_pred))

# def plot_confusion_matrix(y_true, y_pred, title="Confusion Matrix", normalize=False):
#     labels = np.unique(np.concatenate([np.asarray(y_true), np.asarray(y_pred)]))
#     cm = confusion_matrix(y_true, y_pred, labels=labels)
#     fmt = "d"
#     if normalize:
#         cm = cm.astype(float) / cm.sum(axis=1, keepdims=True)
#         fmt = ".2f"
#     plt.figure(figsize=(7, 6))
#     sns.heatmap(cm, annot=True, fmt=fmt, cmap="Blues",
#                 xticklabels=labels, yticklabels=labels, cbar=True)
#     plt.xlabel("Predicted"); plt.ylabel("True"); plt.title(title); plt.tight_layout(); plt.show()

# def choose_threshold_f2(y_true: np.ndarray, y_prob: np.ndarray, default: float = 0.3) -> float:
#     prec, rec, thr = precision_recall_curve(y_true, y_prob)
#     beta = 2
#     f2 = (1 + beta**2) * (prec * rec) / np.clip(beta**2 * prec + rec, 1e-9, None)
#     if len(thr) == 0: return float(default)
#     best = int(np.nanargmax(f2))
#     return float(thr[max(best-1, 0)])

# def choose_threshold_min_recall(y_true: np.ndarray, y_prob: np.ndarray, recall_floor=0.95, default=0.3) -> float:
#     prec, rec, thr = precision_recall_curve(y_true, y_prob)
#     idx = np.where(rec >= recall_floor)[0]
#     if len(idx) == 0 or len(thr) == 0:
#         return float(default)
#     best = idx[np.argmax(prec[idx])]
#     return float(thr[max(best-1, 0)])


# # ---------------- Stage 1 ----------------

# def preprocess_stage1(df: pd.DataFrame) -> pd.DataFrame:
#     out = df.dropna(subset=['Injury Severity']).copy()
#     out['Injury Severity'] = out['Injury Severity'].str.lower()
#     out['Injury Binary'] = (out['Injury Severity'] != 'no apparent injury').astype(int)
#     return out

# def build_xy_stage1(df1: pd.DataFrame) -> Tuple[pd.DataFrame, np.ndarray]:
#     X = df1.drop(columns=['Injury Severity', 'Injury Binary'])
#     X = one_hot_all(X)
#     y = df1['Injury Binary'].to_numpy()
#     return X, y

# def train_stage1_lgb(df: pd.DataFrame, cfg: LGBConfig) -> Dict[str, Any]:
#     df1 = preprocess_stage1(df)
#     X_all, y_all = build_xy_stage1(df1)
#     X_tr, X_te, y_tr, y_te = split_xy(X_all, y_all, cfg)
#     X_tr_orig, y_tr_orig = X_tr.copy(), y_tr.copy()

#     if cfg.use_smote:
#         sm = SMOTE(**cfg.smote_stage1)
#         X_tr, y_tr = sm.fit_resample(X_tr, y_tr)

#     model = lgb.LGBMClassifier(**cfg.lgb_stage1)
#     model.fit(X_tr, y_tr)

#     y_prob_te = model.predict_proba(X_te)[:, 1]
#     thr = choose_threshold_min_recall(y_te, y_prob_te, recall_floor=cfg.stage1_recall_floor) \
#           if cfg.stage1_threshold_strategy == "min_recall" else \
#           choose_threshold_f2(y_te, y_prob_te) if cfg.stage1_threshold_strategy == "f2" \
#           else cfg.stage1_fixed_threshold

#     print(f"[Info] Stage 1 threshold used: {thr:.4f}")  # ✅ הדפסת הסף

#     y_pred_te = (y_prob_te >= thr).astype(int)
#     print_report("Stage 1 – TEST", y_te, y_pred_te)
#     plot_confusion_matrix(y_te, y_pred_te, title="Stage 1 – TEST")

#     y_pred_tr_fit = (model.predict_proba(X_tr)[:, 1] >= thr).astype(int)
#     print_report("Stage 1 – TRAIN-FIT", y_tr, y_pred_tr_fit)

#     y_pred_tr_orig = (model.predict_proba(X_tr_orig)[:, 1] >= thr).astype(int)
#     print_report("Stage 1 – TRAIN-ORIG", y_tr_orig, y_pred_tr_orig)

#     return {"model": model, "threshold": thr}


# # ---------------- Stage 2 ----------------

# def build_xy_stage2(df1: pd.DataFrame) -> Tuple[pd.DataFrame, np.ndarray]:
#     df2 = df1.copy()
#     df2['Fatal Binary'] = (df2['Injury Severity'].str.lower() == 'fatal injury').astype(int)
#     df2 = df2[df2['Injury Binary'] == 1].copy()
#     X = df2.drop(columns=['Injury Severity', 'Injury Binary', 'Fatal Binary'])
#     X = one_hot_all(X)
#     y = df2['Fatal Binary'].to_numpy()
#     return X, y

# def train_stage2_lgb(df: pd.DataFrame, cfg: LGBConfig) -> Optional[Dict[str, Any]]:
#     df1 = preprocess_stage1(df)
#     X_all, y_all = build_xy_stage2(df1)
#     if len(np.unique(y_all)) < 2:
#         return None

#     X_tr, X_te, y_tr, y_te = split_xy(X_all, y_all, cfg)
#     X_tr_orig, y_tr_orig = X_tr.copy(), y_tr.copy()

#     if cfg.use_smote:
#         sm = SMOTE(**cfg.smote_stage2)
#         X_tr, y_tr = sm.fit_resample(X_tr, y_tr)

#     model = lgb.LGBMClassifier(**cfg.lgb_stage2)
#     model.fit(X_tr, y_tr)

#     thr = 0.5
#     print(f"[Info] Stage 2 threshold used: {thr:.4f}")  # ✅ הדפסת הסף

#     y_pred_te = (model.predict_proba(X_te)[:, 1] >= thr).astype(int)
#     print_report("Stage 2 – TEST", y_te, y_pred_te)
#     plot_confusion_matrix(y_te, y_pred_te, title="Stage 2 – TEST")

#     y_pred_tr_fit = (model.predict_proba(X_tr)[:, 1] >= thr).astype(int)
#     print_report("Stage 2 – TRAIN-FIT", y_tr, y_pred_tr_fit)

#     y_pred_tr_orig = (model.predict_proba(X_tr_orig)[:, 1] >= thr).astype(int)
#     print_report("Stage 2 – TRAIN-ORIG", y_tr_orig, y_pred_tr_orig)

#     return {"model": model, "threshold": thr}


# # ---------------- Stage 3 ----------------

# def build_xy_stage3(df1: pd.DataFrame) -> Tuple[pd.DataFrame, np.ndarray, LabelEncoder]:
#     subset = ['possible injury', 'suspected minor injury', 'suspected serious injury']
#     df3 = df1[df1['Injury Severity'].str.lower().isin(subset)].copy()
#     df3['Severity'] = np.where(df3['Injury Severity'].str.lower() == 'suspected serious injury',
#                                'serious', 'light')
#     X = df3.drop(columns=['Injury Severity', 'Injury Binary', 'Severity'])
#     X = one_hot_all(X)
#     le = LabelEncoder()
#     y = le.fit_transform(df3['Severity'].astype(str).values)
#     return X, y, le

# def train_stage3_lgb(df: pd.DataFrame, cfg: LGBConfig) -> Optional[Dict[str, Any]]:
#     df1 = preprocess_stage1(df)
#     try:
#         X_all, y_all, le = build_xy_stage3(df1)
#     except ValueError:
#         return None
#     if len(np.unique(y_all)) < 2:
#         return None

#     X_tr, X_te, y_tr, y_te = split_xy(X_all, y_all, cfg)
#     X_tr_orig, y_tr_orig = X_tr.copy(), y_tr.copy()

#     if cfg.use_smote:
#         sm = SMOTE(**cfg.smote_stage3)
#         X_tr, y_tr = sm.fit_resample(X_tr, y_tr)

#     model = lgb.LGBMClassifier(**cfg.lgb_stage3)
#     model.fit(X_tr, y_tr)

#     thr = 0.5
#     print(f"[Info] Stage 3 threshold used: {thr:.4f}")  # ✅ הדפסת הסף

#     y_pred_te = model.predict(X_te)
#     print_report("Stage 3 – TEST", le.inverse_transform(y_te), le.inverse_transform(y_pred_te))
#     plot_confusion_matrix(le.inverse_transform(y_te), le.inverse_transform(y_pred_te), title="Stage 3 – TEST")

#     y_pred_tr_fit = model.predict(X_tr)
#     print_report("Stage 3 – TRAIN-FIT", le.inverse_transform(y_tr), le.inverse_transform(y_pred_tr_fit))

#     y_pred_tr_orig = model.predict(X_tr_orig)
#     print_report("Stage 3 – TRAIN-ORIG", le.inverse_transform(y_tr_orig), le.inverse_transform(y_pred_tr_orig))

#     return {"model": model, "label_encoder": le, "threshold": thr}


# # ---------------- Run All ----------------

# def run_pipeline_lgb(df: pd.DataFrame, cfg: LGBConfig) -> Dict[str, Any]:
#     stage1 = train_stage1_lgb(df, cfg)
#     stage2 = train_stage2_lgb(df, cfg)
#     stage3 = train_stage3_lgb(df, cfg)
#     return {"stage1": stage1, "stage2": stage2, "stage3": stage3}


# # ---------------- Example Run ----------------

# cfg = LGBConfig(
#     random_state=42,
#     test_size=0.30,
#     use_smote=True,
#     stage1_threshold_strategy="min_recall",
#     stage1_fixed_threshold=0.30,
#     stage1_recall_floor=0.95
# )

# results = run_pipeline_lgb(df, cfg)

"""Grid Search (זמן הרצה ארוך)

```
תוצאות פחות טובות
```


"""

# from sklearn.metrics import make_scorer, f1_score
# import lightgbm as lgb
# import matplotlib.pyplot as plt
# import seaborn as sns
# from sklearn.model_selection import train_test_split, GridSearchCV
# from imblearn.over_sampling import SMOTE
# from sklearn.preprocessing import LabelEncoder, StandardScaler
# from sklearn.metrics import (classification_report, confusion_matrix,accuracy_score,precision_score,recall_score,f1_score,make_scorer)

# # ✨ יצירת עמודת פציעה / אין פציעה
# df = df.dropna(subset=['Injury Severity'])
# df['Injury Binary'] = df['Injury Severity'].apply(lambda x: 0 if x == 'no apparent injury' else 1)

# # 🔹 שלב ראשון – פציעה / אין פציעה
# X = df.drop(['Injury Severity', 'Injury Binary'], axis=1)
# y = df['Injury Binary']
# X = pd.get_dummies(X, drop_first=True)
# X.columns = X.columns.str.replace('[^A-Za-z0-9_]+', '_', regex=True)
# X = X.drop(columns=X.select_dtypes(include=['datetime64']).columns)
# X = X.astype(float)

# # Split + SMOTE
# X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)
# X_train1, y_train1 = SMOTE(random_state=42).fit_resample(X_train1, y_train1)

# # ✅ הגדרת פרמטרים לחיפוש
# param_grid = {
#     'n_estimators': [200, 500],
#     'max_depth': [5, 10],
#     'learning_rate': [0.01, 0.03, 0.1],
#     'num_leaves': [15, 31, 63]
# }

# # מודל בסיס
# base_model = lgb.LGBMClassifier(class_weight='balanced', random_state=42)

# # GridSearchCV
# grid_search = GridSearchCV(
#     estimator=base_model,
#     param_grid=param_grid,
#     scoring=make_scorer(recall_score),
#     cv=3,
#     verbose=1,
#     n_jobs=-1
# )

# # הרצת החיפוש
# grid_search.fit(X_train1, y_train1)

# # הדפסת התוצאה הטובה ביותר
# print("Best Parameters:", grid_search.best_params_)
# print("Best F1 Score (train, CV):", grid_search.best_score_)

# # שימוש במודל הטוב ביותר
# best_model1 = grid_search.best_estimator_

# # תחזיות
# probs1 = best_model1.predict_proba(X_test1)[:, 1]
# thresh1 = 0.3
# y_pred1 = (probs1 >= thresh1).astype(int)

# # דו"ח
# print("\nשלב 1 – פציעה / אין פציעה")
# print(classification_report(y_test1, y_pred1))
# sns.heatmap(confusion_matrix(y_test1, y_pred1), annot=True, fmt='d', cmap='Blues')
# plt.title("Confusion Matrix - Stage 1")
# plt.show()

"""Cross Validation"""

# from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
# from sklearn.metrics import classification_report, confusion_matrix, make_scorer, f1_score
# # ✅ שלב ראשון – מודל LightGBM עם Cross Validation (כולל SMOTE)

# # 🎯 הכנת עמודת המטרה הבינארית
# df = df.dropna(subset=['Injury Severity'])
# df['Injury Binary'] = df['Injury Severity'].apply(lambda x: 0 if x == 'no apparent injury' else 1)

# # 🧠 הכנת תכונות
# X = df.drop(columns=['Injury Severity', 'Injury Binary'])
# y = df['Injury Binary']

# # קידוד One-Hot לעמודות קטגוריאליות
# X = pd.get_dummies(X, drop_first=True)
# X.columns = X.columns.str.replace('[^A-Za-z0-9_]+', '_', regex=True)

# # הסרת עמודות מסוג datetime
# X = X.drop(columns=X.select_dtypes(include=['datetime64']).columns)
# X = X.astype(float)

# # 🔄 חלוקת הנתונים
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)

# # ⚖️ איזון הנתונים עם SMOTE
# smote = SMOTE(random_state=42)
# X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

# # 🌟 הגדרת המודל
# model = lgb.LGBMClassifier(n_estimators=500, max_depth=10, learning_rate=0.03,
#                            class_weight='balanced', random_state=42)

# # 🧪 Cross Validation
# cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
# f1_weighted = make_scorer(f1_score, average='weighted')
# scores = cross_val_score(model, X_resampled, y_resampled, cv=cv, scoring=f1_weighted)

# # 📊 תוצאה
# print(f"Weighted F1 CV Mean: {scores.mean():.4f}")
# print(f"Weighted F1 CV Std: {scores.std():.4f}")

"""XGBoost
---


"""

import xgboost as xgb
from catboost import CatBoostClassifier
from sklearn.model_selection import train_test_split, cross_val_score
from imblearn.over_sampling import SMOTENC
from imblearn.over_sampling import SMOTE
from dataclasses import dataclass
from typing import Dict, Any, Optional
from sklearn.metrics import (
    classification_report, confusion_matrix, precision_recall_curve
)
from sklearn.metrics import classification_report, f1_score, confusion_matrix, precision_recall_fscore_support,accuracy_score, precision_score, recall_score
from sklearn.preprocessing import LabelEncoder
from sklearn.utils.class_weight import compute_class_weight
import warnings
warnings.filterwarnings('ignore')
from sklearn.model_selection import GridSearchCV

""">**מודל יש פציעה/אין פציעה ** *italicized text*"""

# ============================================
# XGBoost Pipeline (Stage 1 בלבד: Injury / No Injury)
# כולל ציור Confusion Matrix
# ============================================

from dataclasses import dataclass
from typing import Dict, Any
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    classification_report, confusion_matrix, precision_recall_curve
)
from imblearn.over_sampling import SMOTE
import xgboost as xgb

# --- לציור מטריצות ---
import matplotlib.pyplot as plt
import seaborn as sns


# ---------- Config (הגדרות) ----------

@dataclass
class TrainConfig:
    test_size: float = 0.30          # גודל סט הבדיקה (30%)
    random_state: int = 42           # זריעת רנדום לשחזוריות
    use_smote: bool = True           # להפעיל SMOTE לאיזון המחלקות ב-Train
    threshold_strategy: str = "f2"   # איך לבחור סף: "f2" (מעדיף Recall) או "fixed"
    fixed_threshold: float = 0.30    # אם threshold_strategy="fixed" – זה הסף
    # פרמטרי XGBoost
    xgb_stage1: Dict[str, Any] = None

    def __post_init__(self):
        if self.xgb_stage1 is None:
            self.xgb_stage1 = dict(
                n_estimators=350, max_depth=6, learning_rate=0.05,
                subsample=0.8, colsample_bytree=0.8,
                random_state=self.random_state, n_jobs=-1
            )


# ---------- Utilities (עזר) ----------

def _drop_existing(df: pd.DataFrame, cols: list) -> pd.DataFrame:
    return df.drop(columns=[c for c in cols if c in df.columns], errors='ignore')

def _get_dummies(X: pd.DataFrame) -> pd.DataFrame:
    X = X.copy()
    for c in X.columns:
        if X[c].dtype == 'O':
            X[c] = X[c].astype(str)
    return pd.get_dummies(X, drop_first=True)

def _split(X: pd.DataFrame, y: np.ndarray, cfg: TrainConfig):
    return train_test_split(X, y, test_size=cfg.test_size,
                            random_state=cfg.random_state, stratify=y)

def _choose_threshold_f2(y_true: np.ndarray, y_prob: np.ndarray, default: float = 0.3) -> float:
    prec, rec, thr = precision_recall_curve(y_true, y_prob)
    beta = 2
    denom = np.clip(beta**2 * prec + rec, 1e-9, None)
    f2 = (1 + beta**2) * (prec * rec) / denom
    if len(thr) == 0:
        return float(default)
    best = int(np.nanargmax(f2))
    return float(thr[max(best - 1, 0)])

def _print_report(title: str, y_true, y_pred):
    print(f"\n=== {title} ===")
    print(classification_report(y_true, y_pred, digits=3))
    print("Confusion Matrix:")
    print(confusion_matrix(y_true, y_pred))

def plot_confusion_matrix(y_true, y_pred, title="Confusion Matrix", normalize=False):
    """
    מצייר מטריצת בלבול בסגנון ברור
    """
    labels = np.unique(np.concatenate([np.asarray(y_true), np.asarray(y_pred)]))
    cm = confusion_matrix(y_true, y_pred, labels=labels)
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1, keepdims=True)
        fmt = ".2f"
    else:
        fmt = "d"

    plt.figure(figsize=(7, 6))
    ax = sns.heatmap(cm, annot=True, fmt=fmt, cmap="Blues",
                     xticklabels=labels, yticklabels=labels, cbar=True)
    ax.set_xlabel("Predicted", fontsize=11)
    ax.set_ylabel("True", fontsize=11)
    ax.set_title(title, fontsize=13, pad=12)
    plt.tight_layout()
    plt.show()


# ---------- Stage 1: Injury / No Injury ----------

def preprocess_stage1(df: pd.DataFrame) -> pd.DataFrame:
    if 'Injury Severity' not in df.columns:
        raise ValueError("עמודת 'Injury Severity' לא נמצאה ב-df.")
    out = df.dropna(subset=['Injury Severity']).copy()
    out['Injury Severity'] = out['Injury Severity'].str.lower()
    out['Injury Binary'] = (out['Injury Severity'] != 'no apparent injury').astype(int)
    return out

def build_xy_stage1(df: pd.DataFrame) -> (pd.DataFrame, np.ndarray):
    X = _drop_existing(df, ['Injury Severity', 'Injury Binary'])
    X = _get_dummies(X)
    y = df['Injury Binary'].astype(int).values
    return X, y

def train_stage1(df: pd.DataFrame, cfg: TrainConfig) -> Dict[str, Any]:
    df1 = preprocess_stage1(df)
    X_all, y_all = build_xy_stage1(df1)
    X_tr, X_te, y_tr, y_te = _split(X_all, y_all, cfg)

    if cfg.use_smote:
        sm = SMOTE(random_state=cfg.random_state)
        X_tr, y_tr = sm.fit_resample(X_tr, y_tr)

    model1 = xgb.XGBClassifier(**cfg.xgb_stage1)
    model1.fit(X_tr, y_tr)

    y_prob = model1.predict_proba(X_te)[:, 1]
    thr = (_choose_threshold_f2(y_te, y_prob, cfg.fixed_threshold)
           if cfg.threshold_strategy == "f2" else float(cfg.fixed_threshold))
    y_pred = (y_prob >= thr).astype(int)

    _print_report("Stage 1: Injury / No Injury", y_te, y_pred)
    plot_confusion_matrix(y_te, y_pred, title="Confusion Matrix - Stage 1 (Injury/No Injury)", normalize=False)

    feature_columns = X_all.columns.tolist()
    return {
        "model": model1,
        "feature_columns": feature_columns,
        "X_test": X_te, "y_test": y_te,
        "y_prob": y_prob, "y_pred": y_pred,
        "threshold": thr
    }


# ---------- Predict on NEW data ----------

def transform_new_input(df_new: pd.DataFrame, feature_columns: list) -> pd.DataFrame:
    df_new = df_new.copy()
    df_new = df_new.drop(columns=[c for c in ['Injury Severity','Injury Binary'] if c in df_new.columns],
                         errors='ignore')
    for c in df_new.columns:
        if df_new[c].dtype == 'O':
            df_new[c] = df_new[c].astype(str)
    X_new = pd.get_dummies(df_new, drop_first=True)
    X_new = X_new.reindex(columns=feature_columns, fill_value=0)  # יישור לעמודות האימון
    return X_new

def predict_new(df_new: pd.DataFrame, stage1_outputs: dict):
    model = stage1_outputs["model"]
    cols = stage1_outputs["feature_columns"]
    thr = stage1_outputs["threshold"]
    X_new = transform_new_input(df_new, cols)
    prob = model.predict_proba(X_new)[:, 1]
    pred = (prob >= thr).astype(int)
    return pred, prob


# ---------- Run Pipeline (Stage 1 בלבד) ----------

def run_pipeline(df: pd.DataFrame, cfg: TrainConfig) -> Dict[str, Any]:
    stage1 = train_stage1(df, cfg)
    print(f"\n[Info] Stage 1 threshold used: {stage1['threshold']:.3f}")
    return {"stage1": stage1}


# ===== שימוש =====
cfg = TrainConfig(threshold_strategy="f2", fixed_threshold=0.30, use_smote=True, random_state=42)
results = run_pipeline(df, cfg)
# דוגמה להסיק על שורה חדשה:
# df_new = pd.DataFrame([{...}])
# pred, prob = predict_new(df_new, results["stage1"])
# print(pred, prob)

""">**מודל 5 קטגרויות**"""

# ===============================
# 1. הגדרת עמודת המטרה
# ===============================

target_column = 'Injury Severity'

# ===============================
# 2. הכנת פיצ'רים ומטרה
# ===============================

# הפרדת פיצ'רים ועמודת מטרה
X = df.drop(columns=[target_column])
y = df[target_column]


# קידוד קטגוריות
X = pd.get_dummies(X)

# קידוד עמודת מטרה
le = LabelEncoder()
y = le.fit_transform(y)

# ===============================
# 3. פיצול ל-Train ו-Test
# ===============================

X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.3,
    random_state=42,
    stratify=y
)

# ===============================
# 4. החלת SMOTE על סט האימון בלבד
# ===============================

smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

print(f"✅ סיימנו SMOTE: כעת יש {np.bincount(y_train_resampled)} דוגמאות לכל קטגוריה.")

# ===============================
# 5. בניית מודל XGBoost
# ===============================

model = xgb.XGBClassifier(
    objective='multi:softprob',
    eval_metric='mlogloss',
    num_class=len(np.unique(y)),
    random_state=42
)

# ===============================
# 6. אימון המודל
# ===============================

model.fit(X_train_resampled, y_train_resampled)

# ===============================
# 7. חיזוי
# ===============================

y_pred = model.predict(X_test)

# ===============================
# 8. דוח ביצועים
# ===============================

print("\n🔵 תוצאות המודל אחרי SMOTE:\n")
print(classification_report(
    y_test,
    y_pred,
    target_names=le.classes_
))

# ===============================
# 9. חישוב Macro F1
# ===============================

macro_f1 = f1_score(
    y_test,
    y_pred,
    average='macro'
)

print(f"\n🎯 Macro F1 Score אחרי SMOTE: {macro_f1:.4f}")



# ===============================
# גרף חשיבות תכונות (Feature Importance)
# ===============================

import matplotlib.pyplot as plt

# ניקח את החשיבויות מתוך המודל
importances = model.feature_importances_
feature_names = X.columns

# סידור לפי סדר יורד
indices = np.argsort(importances)[::-1]

# נבחר את 20 הפיצ'רים הכי חשובים
top_n = 30
top_features = feature_names[indices][:top_n]
top_importances = importances[indices][:top_n]

# ציור גרף
plt.figure(figsize=(12, 8))
plt.barh(top_features[::-1], top_importances[::-1])  # הפוך כדי שהפיצ'ר הכי חשוב יהיה למעלה
plt.xlabel('Feature Importance')
plt.title('Top 20 Most Important Features')
plt.tight_layout()
plt.show()

# # ===============================
# # 1. הכנת עמודת מטרה ופיצ'רים
# # ===============================

# target_column = 'Injury Severity'

# X = df.drop(columns=[target_column])
# y = df[target_column]


# # המרת כל העמודות הקטגוריאליות לסוג (כי ככה סמוט עובד גם ללא דמיז)-'category'
# for col in X.columns:
#     if X[col].dtype == 'object':
#         X[col] = X[col].astype('category')

# # קידוד עמודת המטרה
# le = LabelEncoder()
# y = le.fit_transform(y)

# # ===============================
# # 2. פיצול ל-Train/Test
# # ===============================

# X_train, X_test, y_train, y_test = train_test_split(
#     X, y,
#     test_size=0.3,
#     random_state=42,
#     stratify=y
# )

# # ===============================
# # 3. SMOTE לאיזון סט האימון
# # ===============================

# # שימי לב - SMOTE עובד רק על נתונים נומריים, אז נמיר קטגוריות ל-int זמני
# X_train_encoded = X_train.copy()

# for col in X_train_encoded.select_dtypes(['category']).columns:
#     X_train_encoded[col] = X_train_encoded[col].cat.codes

# smote = SMOTE(random_state=42)
# X_train_resampled, y_train_resampled = smote.fit_resample(X_train_encoded, y_train)

# print(f"✅ סיימנו SMOTE: {np.bincount(y_train_resampled)}")

# # ===============================
# # 4. בניית מודל XGBoost
# # ===============================

# model = xgb.XGBClassifier(
#     objective='multi:softprob',
#     eval_metric='mlogloss',
#     tree_method='hist',
#     enable_categorical=True,
#     num_class=len(np.unique(y)),
#     random_state=42
# )

# # ===============================
# # 5. אימון המודל
# # ===============================

# model.fit(X_train_resampled, y_train_resampled)

# # ===============================
# # 6. חיזוי
# # ===============================

# # גם על ה-X_test נצטרך לקודד זמנית כדי לחזות
# X_test_encoded = X_test.copy()
# for col in X_test_encoded.select_dtypes(['category']).columns:
#     X_test_encoded[col] = X_test_encoded[col].cat.codes

# y_pred = model.predict(X_test_encoded)

# # ===============================
# # 7. דוח ביצועים
# # ===============================

# print("\n🔵 תוצאות המודל אחרי SMOTE ואימון חכם:\n")
# print(classification_report(
#     y_test,
#     y_pred,
#     target_names=le.classes_
# ))

# # ===============================
# # 8.  חישוב מדד Macro F1
# # ===============================

# macro_f1 = f1_score(
#     y_test,
#     y_pred,
#     average='macro'
# )

# print(f"\n🎯 Macro F1 Score אחרי SMOTE: {macro_f1:.4f}")

# # ===============================
# # 9. Feature Importance
# # ===============================

# import matplotlib.pyplot as plt

# # ניקח את החשיבויות מתוך המודל
# importances = model.feature_importances_
# feature_names = X_train_encoded.columns

# # סידור לפי סדר יורד
# indices = np.argsort(importances)[::-1]

# # נבחר את 30 הפיצ'רים הכי חשובים
# top_n = 30
# top_features = feature_names[indices][:top_n]
# top_importances = importances[indices][:top_n]

# # ציור גרף
# plt.figure(figsize=(12, 8))
# plt.barh(top_features[::-1], top_importances[::-1])
# plt.xlabel('Feature Importance')
# plt.title('Top 20 Most Important Features')
# plt.tight_layout()
# plt.show()



# # ===============================
# # 10.סינון פיצ'רים חלשים לפי Feature Importance
# # ===============================

# # קודם כל - נעשה DataFrame של החשיבויות
# feature_importance_df = pd.DataFrame({
#     'Feature': feature_names,
#     'Importance': importances
# })

# # מיון עולה לפי Importance (מהכי פחות חשוב)
# feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

# # הצגת הטבלה - למשל 30 הפיצ'רים הכי פחות חשובים
# print("\n🔵 30 הפיצ'רים הכי פחות חשובים:\n")
# print(feature_importance_df.head(50))

# # אפשרות אוטומטית לסנן - למשל, להסיר את כל הפיצ'רים עם Importance קטן מ-0.001
# threshold = 0.0038

# # מציאת הפיצ'רים המועמדים להסרה
# features_to_drop = feature_importance_df[feature_importance_df['Importance'] < threshold]['Feature'].tolist()

# print(f"\n🚮 נמצאו {len(features_to_drop)} פיצ'רים עם חשיבות נמוכה מ-{threshold}.")
# print(features_to_drop)

# # ===============================
# # 11.אופציונלי: למחוק אותם מה-DataFrame
# # ===============================

# X_filtered = X.drop(columns=features_to_drop)

# print(f"\n✅ הדאטה החדש אחרי סינון מכיל {X_filtered.shape[1]} פיצ'רים במקום {X.shape[1]}.")



# # ===============================
# # 12.פיצול חדש ל-Train/Test על הדאטה המסונן
# # ===============================

# X_train, X_test, y_train, y_test = train_test_split(
#     X_filtered, y,
#     test_size=0.3,
#     random_state=42,
#     stratify=y
# )

# # ===============================
# # 13.טיפול ב-SMOTE שוב על הדאטה החדש
# # ===============================

# X_train_encoded = X_train.copy()

# for col in X_train_encoded.select_dtypes(['category']).columns:
#     X_train_encoded[col] = X_train_encoded[col].cat.codes

# smote = SMOTE(random_state=42)
# X_train_resampled, y_train_resampled = smote.fit_resample(X_train_encoded, y_train)

# # ===============================
# # 14.בניית מודל חדש ואימון
# # ===============================

# model = xgb.XGBClassifier(
#     objective='multi:softprob',
#     eval_metric='mlogloss',
#     tree_method='hist',
#     enable_categorical=True,
#     num_class=len(np.unique(y)),
#     random_state=42
# )

# model.fit(X_train_resampled, y_train_resampled)

# # ===============================
# # 15.חיזוי מחדש
# # ===============================

# X_test_encoded = X_test.copy()
# for col in X_test_encoded.select_dtypes(['category']).columns:
#     X_test_encoded[col] = X_test_encoded[col].cat.codes

# y_pred = model.predict(X_test_encoded)

# # ===============================
# # 16.דוח ביצועים מחדש
# # ===============================

# print("\n🔵 תוצאות המודל אחרי מחיקת פיצ'רים:\n")
# print(classification_report(
#     y_test,
#     y_pred,
#     target_names=le.classes_
# ))

# macro_f1 = f1_score(
#     y_test,
#     y_pred,
#     average='macro'
# )

# print(f"\n🎯 Macro F1 Score אחרי מחיקת פיצ'רים: {macro_f1:.4f}")

# # ========================================
# # XGBoost עם SMOTE ממוקד, משקלים מותאמים ו־Threshold חכם
# # ========================================

# # ========================================
# # 1. הכנת הנתונים
# # ========================================
# target_col = 'Injury Severity'
# X = df.drop(columns=[target_col], errors='ignore').copy()
# y = df[target_col].copy()

# # המרת עמודות אובייקט לקטגוריה
# for col in X.select_dtypes(include='object').columns:
#     X[col] = X[col].astype('category')

# # קידוד y
# label_encoder = LabelEncoder()
# y_encoded = label_encoder.fit_transform(y)
# classes = label_encoder.classes_

# # פיצול ל־Train/Test
# X_train, X_test, y_train, y_test = train_test_split(
#     X, y_encoded, test_size=0.3, stratify=y_encoded, random_state=42
# )

# # קידוד קטגוריות מספרי עבור SMOTE
# X_train_enc = X_train.copy()
# for col in X_train_enc.select_dtypes(include='category').columns:
#     X_train_enc[col] = X_train_enc[col].cat.codes

# # ========================================
# # 2. SMOTE ממוקד לקטגוריות חלשות כולל קטלני
# # ========================================
# target_classes = ['possible injury', 'suspected minor injury', 'suspected serious injury', 'fatal injury']
# target_indices = [np.where(classes == cls)[0][0] for cls in target_classes]

# _, counts = np.unique(y_train, return_counts=True)
# max_count = max(counts)
# sampling_strategy = {cls_idx: max_count for cls_idx in target_indices}

# smote = SMOTE(sampling_strategy=sampling_strategy, random_state=42)
# X_resampled, y_resampled = smote.fit_resample(X_train_enc, y_train)

# # ========================================
# # 3. משקלים מחוזקים לקטגוריות החשובות
# # ========================================
# weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_resampled), y=y_resampled)
# for idx in target_indices:
#     weights[idx] *= 1.5
# sample_weights = np.array([weights[i] for i in y_resampled])

# # ========================================
# # 4. אימון מודל XGBoost
# # ========================================
# model = xgb.XGBClassifier(
#     objective='multi:softprob',
#     num_class=len(np.unique(y_encoded)),
#     eval_metric='mlogloss',
#     tree_method='hist',
#     enable_categorical=True,
#     n_estimators=300,
#     max_depth=8,
#     learning_rate=0.05,
#     subsample=0.8,
#     random_state=42
# )

# model.fit(X_resampled, y_resampled, sample_weight=sample_weights)

# # ========================================
# # 5. חיזוי עם threshold מותאם ל־serious ו־fatal
# # ========================================
# X_test_enc = X_test.copy()
# for col in X_test_enc.select_dtypes(include='category').columns:
#     X_test_enc[col] = X_test_enc[col].cat.codes

# y_proba = model.predict_proba(X_test_enc)
# serious_index = np.where(classes == 'suspected serious injury')[0][0]
# fatal_index = np.where(classes == 'fatal injury')[0][0]

# threshold_serious = 0.25
# threshold_fatal = 0.25

# y_pred = []
# for probs in y_proba:
#     if probs[fatal_index] >= threshold_fatal:
#         y_pred.append(fatal_index)
#     elif probs[serious_index] >= threshold_serious:
#         y_pred.append(serious_index)
#     else:
#         y_pred.append(np.argmax(probs))

# # ========================================
# # 6. דוח ביצועים + מטריצת בלבול
# # ========================================
# print("\n\U0001F4CA דוח ביצועים לפי קטגוריות:")
# print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))

# macro_f1 = f1_score(y_test, y_pred, average='macro')
# print(f"\n\U0001F3AF Macro F1 Score: {macro_f1:.4f}")

# cm = confusion_matrix(y_test, y_pred)
# plt.figure(figsize=(8, 6))
# sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
#             xticklabels=label_encoder.classes_,
#             yticklabels=label_encoder.classes_)
# plt.xlabel('Predicted')
# plt.ylabel('Actual')
# plt.title('Confusion Matrix')
# plt.tight_layout()
# plt.show()

# # ========================================
# # 7. ניתוח threshold ל־serious ו־fatal
# # ========================================
# print("\n\U0001F4C8 ניתוח threshold ל־'suspected serious injury' ו־'fatal injury':")
# thresholds = np.linspace(0.1, 0.5, 9)
# for t in thresholds:
#     y_pred_tmp = []
#     for probs in y_proba:
#         if probs[fatal_index] >= t:
#             y_pred_tmp.append(fatal_index)
#         elif probs[serious_index] >= t:
#             y_pred_tmp.append(serious_index)
#         else:
#             y_pred_tmp.append(np.argmax(probs))
#     prec, rec, f1, _ = precision_recall_fscore_support(y_test, y_pred_tmp, labels=[serious_index, fatal_index], average='macro')
#     print(f"Threshold={t:.2f} | Recall: {rec:.2f} | Precision: {prec:.2f} | F1: {f1:.2f}")

"""> **מודל: 3 קטגוריות - אין פציעה/פציעה קלה/פציעה חמורה**

"""

def train_best_balanced_model(df, target_column='Injury Severity'):

    # 🎯 מיפוי ל־3 קטגוריות
    injury_mapping = {
        'no apparent injury': 'no injury',
        'possible injury': 'minor injury',
        'suspected minor injury': 'minor injury',
        'suspected serious injury': 'severe injury',
        'fatal injury': 'severe injury'
    }
    df = df[df[target_column].isin(injury_mapping.keys())].copy()
    df[target_column] = df[target_column].map(injury_mapping)

    # 🧪 הכנה
    X = df.drop(columns=[target_column])
    y = df[target_column]

    for col in X.select_dtypes(include='object').columns:
        X[col] = X[col].astype('category')

    le = LabelEncoder()
    y_encoded = le.fit_transform(y)

    X_train, X_test, y_train, y_test = train_test_split(
        X, y_encoded, stratify=y_encoded, test_size=0.3, random_state=42
    )

    # 🔣 קידוד קטגוריות במספרים
    X_train_encoded = X_train.copy()
    for col in X_train_encoded.select_dtypes(['category']).columns:
        X_train_encoded[col] = X_train_encoded[col].cat.codes

    # 🧬 SMOTE ממוקד לפציעות חמורות
    label_map = dict(zip(le.classes_, le.transform(le.classes_)))
    severe_label = label_map['severe injury']
    severe_count = np.sum(y_train == severe_label)
    target_count = max(severe_count + 1000, 3000)

    smote = SMOTE(sampling_strategy={severe_label: target_count}, random_state=42)
    X_train_resampled, y_train_resampled = smote.fit_resample(X_train_encoded, y_train)

    # ⚖️ חישוב משקלים
    class_weights = compute_class_weight('balanced', classes=np.unique(y_train_resampled), y=y_train_resampled)
    weight_dict = dict(zip(np.unique(y_train_resampled), class_weights))
    sample_weights = np.array([weight_dict[i] for i in y_train_resampled])

    # 🚀 אימון מודל XGBoost
    model = xgb.XGBClassifier(
        objective='multi:softprob',
        eval_metric='mlogloss',
        tree_method='hist',
        enable_categorical=True,
        num_class=len(np.unique(y_encoded)),
        learning_rate=0.05,
        max_depth=5,
        min_child_weight=10,
        n_estimators=300,
        subsample=0.8,
        colsample_bytree=0.8,
        reg_alpha=0.5,
        reg_lambda=1.0,
        random_state=42
    )
    model.fit(X_train_resampled, y_train_resampled, sample_weight=sample_weights)

    # 🔍 חיזוי
    X_test_encoded = X_test.copy()
    for col in X_test_encoded.select_dtypes(['category']).columns:
        X_test_encoded[col] = X_test_encoded[col].cat.codes

    y_pred = model.predict(X_test_encoded)

    # 📊 חישוב מדדים
    macro_f1 = f1_score(y_test, y_pred, average='macro')
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='macro')
    recall = recall_score(y_test, y_pred, average='macro')

    # 🔁 Cross-Validation
    cv_score = cross_val_score(
        model, X_train_resampled, y_train_resampled,
        scoring='f1_macro', cv=5
    ).mean()

    # 📋 דוח סיווג מפורט
    print("\n📈 דו\"ח סיווג (3 קטגוריות - XGBoost):")
    print(classification_report(y_test, y_pred, target_names=le.classes_))

    # 📉 מטריצת בלבול
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=le.classes_, yticklabels=le.classes_)
    plt.title("Confusion Matrix - XGBoost (3 Classes)")
    plt.xlabel("Predicted")
    plt.ylabel("True")
    plt.tight_layout()
    plt.show()

    # 🧾 מדדים מסכמים
    print("\n📋 Evaluation Metrics:")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"Precision (macro): {precision:.4f}")
    print(f"Recall (macro): {recall:.4f}")
    print(f"F1 Score (macro): {macro_f1:.4f}")

    # 🧠 החזרת תוצאות
    return {
        'model': model,
        'label_encoder': le,
        'macro_f1_test': macro_f1,
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'macro_f1_train_cv': cv_score,
        'classification_report': classification_report(y_test, y_pred, target_names=le.classes_, output_dict=True),
        'confusion_matrix': cm
    }

# 📤 הפעלת הפונקציה על הדאטה שלך
results = train_best_balanced_model(df)

# 🖨️ הדפסת תוצאות מסכמות
print("\n🔍 תוצאות כלליות:")
print("🎯 Macro F1 (Test):", results['macro_f1_test'])
print("📚 Macro F1 (Train CV):", results['macro_f1_train_cv'])
print("✅ Accuracy:", results['accuracy'])
print("✅ Precision (macro):", results['precision'])
print("✅ Recall (macro):", results['recall'])

"""> **מודל דו-שלבי**

*   שלב 1: יש פציעה/אין פציעה
*   שלב 2: פציעה קטלנית/קלה/קשה

"""

# ============================================
# Modular 2-Stage XGBoost Pipeline (SMOTE + F2)
# כולל ציור Confusion Matrix לשני השלבים
# ============================================

# ---------- Config (הגדרות) ----------

@dataclass
class TrainConfig:
    test_size: float = 0.30          # גודל סט הבדיקה (30%)
    random_state: int = 42           # זריעת רנדום לשחזוריות
    use_smote: bool = True           # להפעיל SMOTE לאיזון המחלקות ב-Train
    threshold_strategy: str = "f2"   # איך לבחור סף: "f2" (מעדיף Recall) או "fixed"
    fixed_threshold: float = 0.30    # אם threshold_strategy="fixed" – זה הסף
    # פרמטרי XGBoost
    xgb_stage1: Dict[str, Any] = None
    xgb_stage2: Dict[str, Any] = None

    def __post_init__(self):
        if self.xgb_stage1 is None:
            self.xgb_stage1 = dict(
                n_estimators=350, max_depth=6, learning_rate=0.05,
                subsample=0.8, colsample_bytree=0.8,
                random_state=self.random_state, n_jobs=-1
            )
        if self.xgb_stage2 is None:
            self.xgb_stage2 = dict(
                objective='multi:softprob',
                n_estimators=350, max_depth=6, learning_rate=0.05,
                subsample=0.8, colsample_bytree=0.8,
                random_state=self.random_state, n_jobs=-1
            )


# ---------- Utilities (עזר) ----------

def _drop_existing(df: pd.DataFrame, cols: list) -> pd.DataFrame:
    return df.drop(columns=[c for c in cols if c in df.columns], errors='ignore')

def _get_dummies(X: pd.DataFrame) -> pd.DataFrame:
    X = X.copy()
    for c in X.columns:
        if X[c].dtype == 'O':
            X[c] = X[c].astype(str)
    return pd.get_dummies(X, drop_first=True)

def _split(X: pd.DataFrame, y: np.ndarray, cfg: TrainConfig):
    return train_test_split(X, y, test_size=cfg.test_size,
                            random_state=cfg.random_state, stratify=y)

def _choose_threshold_f2(y_true: np.ndarray, y_prob: np.ndarray, default: float = 0.3) -> float:
    prec, rec, thr = precision_recall_curve(y_true, y_prob)
    beta = 2
    denom = np.clip(beta**2 * prec + rec, 1e-9, None)
    f2 = (1 + beta**2) * (prec * rec) / denom
    if len(thr) == 0:
        return float(default)
    best = int(np.nanargmax(f2))
    return float(thr[max(best - 1, 0)])

def _print_report(title: str, y_true, y_pred):
    print(f"\n=== {title} ===")
    print(classification_report(y_true, y_pred, digits=3))
    print("Confusion Matrix:")
    print(confusion_matrix(y_true, y_pred))

def plot_confusion_matrix(y_true, y_pred, title="Confusion Matrix", normalize=False):
    """
    מצייר מטריצת בלבול בסגנון ברור:
    - ציר Y = True, ציר X = Predicted
    - כותרת למעלה
    - אפשר לנרמל לאחוזים בשורות (normalize=True)
    """
    # בחירת סדר תוויות עקבי (כל הערכים שהופיעו באמת או בתחזית)
    labels = np.unique(np.concatenate([np.asarray(y_true), np.asarray(y_pred)]))
    cm = confusion_matrix(y_true, y_pred, labels=labels)
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1, keepdims=True)
        fmt = ".2f"
    else:
        fmt = "d"

    plt.figure(figsize=(7, 6))
    ax = sns.heatmap(cm, annot=True, fmt=fmt, cmap="Blues",
                     xticklabels=labels, yticklabels=labels, cbar=True)
    ax.set_xlabel("Predicted", fontsize=11)
    ax.set_ylabel("True", fontsize=11)
    ax.set_title(title, fontsize=13, pad=12)
    plt.tight_layout()
    plt.show()


# ---------- Stage 1: Injury / No Injury ----------

def preprocess_stage1(df: pd.DataFrame) -> pd.DataFrame:
    if 'Injury Severity' not in df.columns:
        raise ValueError("עמודת 'Injury Severity' לא נמצאה ב-df.")
    out = df.dropna(subset=['Injury Severity']).copy()
    out['Injury Severity'] = out['Injury Severity'].str.lower()
    out['Injury Binary'] = (out['Injury Severity'] != 'no apparent injury').astype(int)
    return out

def build_xy_stage1(df: pd.DataFrame) -> (pd.DataFrame, np.ndarray):
    X = _drop_existing(df, ['Injury Severity', 'Injury Binary'])
    X = _get_dummies(X)
    y = df['Injury Binary'].astype(int).values
    return X, y

def train_stage1(df: pd.DataFrame, cfg: TrainConfig) -> Dict[str, Any]:
    df1 = preprocess_stage1(df)
    X_all, y_all = build_xy_stage1(df1)
    X_tr, X_te, y_tr, y_te = _split(X_all, y_all, cfg)

    if cfg.use_smote:
        sm = SMOTE(random_state=cfg.random_state)
        X_tr, y_tr = sm.fit_resample(X_tr, y_tr)

    model1 = xgb.XGBClassifier(**cfg.xgb_stage1)
    model1.fit(X_tr, y_tr)

    y_prob = model1.predict_proba(X_te)[:, 1]
    thr = (_choose_threshold_f2(y_te, y_prob, cfg.fixed_threshold)
           if cfg.threshold_strategy == "f2" else float(cfg.fixed_threshold))
    y_pred = (y_prob >= thr).astype(int)

    _print_report("Stage 1: Injury / No Injury", y_te, y_pred)
    plot_confusion_matrix(y_te, y_pred, title="Confusion Matrix - Stage 1 (Injury/No Injury)", normalize=False)

    feature_columns = X_all.columns.tolist()
    return {
        "model": model1,
        "feature_columns": feature_columns,
        "X_test": X_te, "y_test": y_te,
        "y_prob": y_prob, "y_pred": y_pred,
        "threshold": thr
    }


# ---------- Stage 2: light / serious / fatal (לא חובה כרגע) ----------

def map_injury(df: pd.DataFrame, mapping: Dict[str, str]) -> pd.DataFrame:
    out = df.copy()
    out['Injury Severity'] = out['Injury Severity'].str.lower()
    out['Injury Mapped'] = out['Injury Severity'].map(mapping)
    return out

def build_xy_stage2(df_mapped: pd.DataFrame) -> (pd.DataFrame, np.ndarray, LabelEncoder):
    df2 = df_mapped[df_mapped['Injury Mapped'].notna()].copy()
    if df2.empty:
        raise ValueError("אין דגימות מתויגות ל-Injury Mapped עבור שלב 2.")
    X = _drop_existing(df2, ['Injury Severity', 'Injury Binary', 'Injury Mapped'])
    X = _get_dummies(X)
    le = LabelEncoder()
    y = le.fit_transform(df2['Injury Mapped'].astype(str).values)
    return X, y, le

def train_stage2(df: pd.DataFrame, cfg: TrainConfig, mapping: Dict[str, str]) -> Optional[Dict[str, Any]]:
    df_mapped = map_injury(df, mapping)
    try:
        X_all, y_all, le = build_xy_stage2(df_mapped)
    except ValueError as e:
        print(f"\n[Stage 2] {e}")
        return None

    if len(np.unique(y_all)) < 2:
        print("\n[Stage 2] רק מחלקה אחת לאחר המיפוי – לא ניתן לאמן מסווג רב־מחלקתי.")
        return None

    X_tr, X_te, y_tr, y_te = _split(X_all, y_all, cfg)

    if cfg.use_smote:
        sm = SMOTE(random_state=cfg.random_state)
        X_tr, y_tr = sm.fit_resample(X_tr, y_tr)

    model2 = xgb.XGBClassifier(**cfg.xgb_stage2)
    model2.fit(X_tr, y_tr)

    y_pred_enc = model2.predict(X_te)
    y_true_lbl = le.inverse_transform(y_te)
    y_pred_lbl = le.inverse_transform(y_pred_enc)

    _print_report("Stage 2: light / serious / fatal", y_true_lbl, y_pred_lbl)
    plot_confusion_matrix(y_true_lbl, y_pred_lbl, title="Confusion Matrix - Stage 2 (light/serious/fatal)", normalize=False)

    return {
        "model": model2,
        "label_encoder": le,
        "X_test": X_te, "y_test_labels": y_true_lbl,
        "y_pred_labels": y_pred_lbl
    }


# ---------- Predict on NEW data (הסקה על נתונים חדשים) ----------

def transform_new_input(df_new: pd.DataFrame, feature_columns: list) -> pd.DataFrame:
    df_new = df_new.copy()
    df_new = df_new.drop(columns=[c for c in ['Injury Severity','Injury Binary','Injury Mapped'] if c in df_new.columns],
                         errors='ignore')
    for c in df_new.columns:
        if df_new[c].dtype == 'O':
            df_new[c] = df_new[c].astype(str)
    X_new = pd.get_dummies(df_new, drop_first=True)
    X_new = X_new.reindex(columns=feature_columns, fill_value=0)  # יישור לעמודות האימון
    return X_new

def predict_new(df_new: pd.DataFrame, stage1_outputs: dict):
    model = stage1_outputs["model"]
    cols = stage1_outputs["feature_columns"]
    thr = stage1_outputs["threshold"]
    X_new = transform_new_input(df_new, cols)
    prob = model.predict_proba(X_new)[:, 1]
    pred = (prob >= thr).astype(int)
    return pred, prob


# ---------- Run Pipeline (הרצה) ----------

def run_pipeline(df: pd.DataFrame, cfg: TrainConfig, mapping: Dict[str, str]) -> Dict[str, Any]:
    stage1 = train_stage1(df, cfg)
    stage2 = train_stage2(df, cfg, mapping)  # אפשר להעריך גם שלב 2; אם לא צריך – אפשר להשבית
    print(f"\n[Info] Stage 1 threshold used: {stage1['threshold']:.3f}")
    return {"stage1": stage1, "stage2": stage2}


# ===== שימוש =====
mapping = {
    'possible injury': 'light injury',
    'suspected minor injury': 'light injury',
    'suspected serious injury': 'serious injury',
    'fatal injury': 'fatal injury'
}
cfg = TrainConfig(threshold_strategy="f2", fixed_threshold=0.30, use_smote=True, random_state=42)
results = run_pipeline(df, cfg, mapping)
# דוגמה להסיק על שורה חדשה:
# df_new = pd.DataFrame([{...}])
# pred, prob = predict_new(df_new, results["stage1"])
# print(pred, prob)

# 🛠️ שלב 0: מחיקת ערכים חסרים בעמודת המטרה
df = df.dropna(subset=['Injury Severity'])

# ✨ שלב 1: יצירת עמודת פציעה / אין פציעה
df['Injury Binary'] = df['Injury Severity'].apply(
    lambda x: 0 if x == 'no apparent injury' else 1
)

# 🛠️ בניית X ו-y לשלב הראשון
X = df.drop(['Injury Severity', 'Injury Binary'], axis=1)
y_stage1 = df['Injury Binary']


# קידוד טקסטים
le_features = LabelEncoder()
for col in cat_cols:
    X[col] = le_features.fit_transform(X[col])

# פיצול ל-Train/Test
X_train1, X_test1, y_train1, y_test1 = train_test_split(
    X, y_stage1, test_size=0.3, random_state=42, stratify=y_stage1
)

# SMOTE על קבוצת האימון
smote = SMOTE(random_state=42)
X_res1, y_res1 = smote.fit_resample(X_train1, y_train1)

# 🚀 אימון מודל XGBoost לשלב הראשון
xgb_model_stage1 = xgb.XGBClassifier(
    n_estimators=300,
    max_depth=6,
    learning_rate=0.05,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42
)
xgb_model_stage1.fit(X_res1, y_res1)

# 🎯 חיזוי עם סף מותאם
y_probs1 = xgb_model_stage1.predict_proba(X_test1)[:, 1]
threshold = 0.3
y_pred1_threshold = (y_probs1 >= threshold).astype(int)

# 📈 דו"ח ביצועים שלב ראשון
print("\n📈 דו\"ח סיווג - שלב ראשון (פציעה / אין פציעה):")
print(classification_report(y_test1, y_pred1_threshold))

# 📊 מטריצת בלבול - שלב ראשון
plt.figure(figsize=(8,6))
sns.heatmap(confusion_matrix(y_test1, y_pred1_threshold), annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix - Stage 1 (Injury/No Injury)')
plt.show()

# 🧹 שלב מעבר לשלב שני: תצפיות שחזינו אצלם "פציעה"
X_test_stage2 = X_test1[y_pred1_threshold == 1]
indexes_stage2 = X_test_stage2.index
y_true_stage2 = df.loc[indexes_stage2, 'Injury Severity']

# ✨ שלב 2: בניית דאטה חדש עם מיפוי ל-3 קטגוריות
injury_mapping = {
    'possible injury': 'light injury',
    'suspected minor injury': 'light injury',
    'suspected serious injury': 'serious injury',
    'fatal injury': 'fatal injury'
}

df_stage2 = df[df['Injury Severity'].isin(injury_mapping.keys())].copy()
df_stage2['Injury Mapped'] = df_stage2['Injury Severity'].map(injury_mapping)

# בניית X ו-y
X_stage2 = df_stage2.drop(['Injury Severity', 'Injury Binary', 'Injury Mapped'], axis=1)
y_stage2 = df_stage2['Injury Mapped']


# 🎯 One-Hot Encoding לעמודות טקסט
X_stage2 = pd.get_dummies(X_stage2, columns=cat_cols, drop_first=True)

# קידוד y (התוצאה) בעזרת LabelEncoder
le_y = LabelEncoder()
y_stage2_encoded = le_y.fit_transform(y_stage2)

# פיצול ל-Train/Test
X_train2, X_test2, y_train2, y_test2 = train_test_split(
    X_stage2, y_stage2_encoded, test_size=0.3, random_state=42, stratify=y_stage2_encoded
)

# SMOTE
smote2 = SMOTE(random_state=42)
X_res2, y_res2 = smote2.fit_resample(X_train2, y_train2)

# 🚀 אימון מודל XGBoost לשלב השני
xgb_model_stage2 = xgb.XGBClassifier(
    n_estimators=300,
    max_depth=6,
    learning_rate=0.05,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42
)
xgb_model_stage2.fit(X_res2, y_res2)

# 🔮 חיזוי
y_pred2_encoded = xgb_model_stage2.predict(X_test2)

# 🧠 החזרת התוצאות לשמות
y_pred2_labels = le_y.inverse_transform(y_pred2_encoded)
y_test2_labels = le_y.inverse_transform(y_test2)

# 📈 דו"ח ביצועים שלב שני
print("\n📈 דו\"ח סיווג - שלב שני (3 קטגוריות - light/serious/fatal עם One-Hot):")
print(classification_report(y_test2_labels, y_pred2_labels))

# 📊 מטריצת בלבול
plt.figure(figsize=(8,6))
sns.heatmap(confusion_matrix(y_test2_labels, y_pred2_labels, labels=le_y.classes_),
            annot=True, fmt='d', cmap='Blues',
            xticklabels=le_y.classes_, yticklabels=le_y.classes_)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix - Stage 2 (Mapped Injury Type with One-Hot Encoding)')
plt.show()

"""
> **מודל תלת שלבי**

*   מודל 1: יש פציעה/אין פציעה
*   מודל שני: פציעה קטלנית/לא קטלנית
*   מודל 3: פציעה חמורה/ קלה
"""

# ✅ שיפור כולל למודל XGBoost בשלושה שלבים: פציעה / קטלנית / רמת חומרה

# Function to print comprehensive metrics
def print_metrics(y_true, y_pred, stage_name, set_type="Test"):
    print(f"\n{'='*50}")
    print(f"{stage_name} - {set_type} Set Metrics")
    print(f"{'='*50}")
    print(f"Accuracy: {accuracy_score(y_true, y_pred):.4f}")
    print(f"Precision: {precision_score(y_true, y_pred, average='weighted'):.4f}")
    print(f"Recall: {recall_score(y_true, y_pred, average='weighted'):.4f}")
    print(f"F1-Score: {f1_score(y_true, y_pred, average='weighted'):.4f}")
    print("\nDetailed Classification Report:")
    print(classification_report(y_true, y_pred))

# Function to plot confusion matrix
def plot_confusion_matrix(y_true, y_pred, title, labels=None):
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(8, 6))
    if labels:
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                   xticklabels=labels, yticklabels=labels)
    else:
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title(title)
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.tight_layout()
    plt.show()

# ✨ יצירת עמודת פציעה / אין פציעה
df = df.dropna(subset=['Injury Severity'])
df['Injury Binary'] = df['Injury Severity'].apply(lambda x: 0 if x == 'no apparent injury' else 1)

print(f"Dataset shape: {df.shape}")
print(f"Injury distribution: {df['Injury Binary'].value_counts()}")

# 🔹 שלב ראשון – פציעה / אין פציעה
print("\n" + "="*60)
print("STAGE 1: INJURY / NO INJURY CLASSIFICATION")
print("="*60)

X = df.drop(['Injury Severity', 'Injury Binary'], axis=1)
y = df['Injury Binary']

# Preprocessing
X = pd.get_dummies(X, drop_first=True)
X.columns = X.columns.str.replace('[^A-Za-z0-9_]+', '_', regex=True)
X = X.drop(columns=X.select_dtypes(include=['datetime64']).columns)
X = X.astype(float)

print(f"Features shape: {X.shape}")
print(f"Class distribution before SMOTE: {y.value_counts()}")

# Train-test split
X_train1, X_test1, y_train1, y_test1 = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

# Apply SMOTE
smote1 = SMOTE(random_state=42)
X_train1_smote, y_train1_smote = smote1.fit_resample(X_train1, y_train1)
print(f"Class distribution after SMOTE: {pd.Series(y_train1_smote).value_counts()}")

# Calculate class weights
class_weights1 = compute_class_weight('balanced', classes=np.unique(y_train1), y=y_train1)
scale_pos_weight1 = class_weights1[1] / class_weights1[0]

# XGBoost Model 1
model1 = xgb.XGBClassifier(
    n_estimators=500,
    max_depth=10,
    learning_rate=0.03,
    scale_pos_weight=scale_pos_weight1,
    random_state=42,
    eval_metric='logloss',
    early_stopping_rounds=50
)

# Fit with evaluation set
model1.fit(
    X_train1_smote, y_train1_smote,
    eval_set=[(X_train1_smote, y_train1_smote), (X_test1, y_test1)],
    verbose=False
)

# Predictions
probs1 = model1.predict_proba(X_test1)[:, 1]
thresh1 = 0.25  # הורדנו את הסף כדי לשפר Recall לפציעות
y_pred1_test = (probs1 >= thresh1).astype(int)

# Train predictions for comparison
y_pred1_train = model1.predict(X_train1_smote)

# Print metrics
print_metrics(y_train1_smote, y_pred1_train, "Stage 1", "Train")
print_metrics(y_test1, y_pred1_test, "Stage 1", "Test")

# Plot confusion matrices
plot_confusion_matrix(y_train1_smote, y_pred1_train,
                     "Stage 1: Train Set Confusion Matrix",
                     labels=['No Injury', 'Injury'])
plot_confusion_matrix(y_test1, y_pred1_test,
                     "Stage 1: Test Set Confusion Matrix",
                     labels=['No Injury', 'Injury'])


# 🔸 שלב שני – פציעה קטלנית / לא קטלנית
print("\n" + "="*60)
print("STAGE 2: FATAL / NON-FATAL INJURY CLASSIFICATION")
print("="*60)

df_stage2 = df[df['Injury Binary'] == 1].copy()
df_stage2['Fatal Binary'] = df_stage2['Injury Severity'].apply(
    lambda x: 1 if x == 'fatal injury' else 0
)

print(f"Stage 2 dataset shape: {df_stage2.shape}")
print(f"Fatal injury distribution: {df_stage2['Fatal Binary'].value_counts()}")

X2 = df_stage2.drop(['Injury Severity', 'Injury Binary', 'Fatal Binary'], axis=1)
y2 = df_stage2['Fatal Binary']

# Preprocessing
X2 = pd.get_dummies(X2, drop_first=True)
X2.columns = X2.columns.str.replace('[^A-Za-z0-9_]+', '_', regex=True)
X2 = X2.drop(columns=X2.select_dtypes(include=['datetime64']).columns)
X2 = X2.astype(float)

# Train-test split
X_train2, X_test2, y_train2, y_test2 = train_test_split(
    X2, y2, test_size=0.3, random_state=42, stratify=y2
)

# Apply SMOTE with custom sampling strategy
fatal_count = y_train2.value_counts()[1] if 1 in y_train2.values else 0
smote2 = SMOTE(sampling_strategy={1: fatal_count * 4}, random_state=42)
X_train2_smote, y_train2_smote = smote2.fit_resample(X_train2, y_train2)
print(f"Class distribution after SMOTE: {pd.Series(y_train2_smote).value_counts()}")

# Calculate class weights
class_weights2 = compute_class_weight('balanced', classes=np.unique(y_train2), y=y_train2)
scale_pos_weight2 = class_weights2[1] / class_weights2[0] if len(class_weights2) > 1 else 1

# XGBoost Model 2
model2 = xgb.XGBClassifier(
    n_estimators=500,
    max_depth=10,
    learning_rate=0.03,
    scale_pos_weight=scale_pos_weight2,
    random_state=42,
    eval_metric='logloss',
    early_stopping_rounds=50
)

# Fit with evaluation set
model2.fit(
    X_train2_smote, y_train2_smote,
    eval_set=[(X_train2_smote, y_train2_smote), (X_test2, y_test2)],
    verbose=False
)

# Predictions
probs2 = model2.predict_proba(X_test2)[:, 1]
thresh2 = 0.3
y_pred2_test = (probs2 >= thresh2).astype(int)
y_pred2_train = model2.predict(X_train2_smote)

# Print metrics
print_metrics(y_train2_smote, y_pred2_train, "Stage 2", "Train")
print_metrics(y_test2, y_pred2_test, "Stage 2", "Test")

# Plot confusion matrices
plot_confusion_matrix(y_train2_smote, y_pred2_train,
                     "Stage 2: Train Set Confusion Matrix",
                     labels=['Non-Fatal', 'Fatal'])
plot_confusion_matrix(y_test2, y_pred2_test,
                     "Stage 2: Test Set Confusion Matrix",
                     labels=['Non-Fatal', 'Fatal'])

# 🔻 שלב שלישי – קלה / חמורה
print("\n" + "="*60)
print("STAGE 3: LIGHT / SERIOUS INJURY CLASSIFICATION")
print("="*60)

stage3_df = df_stage2[df_stage2['Injury Severity'].isin([
    'possible injury', 'suspected minor injury', 'suspected serious injury'
])].copy()

stage3_df['Severity'] = stage3_df['Injury Severity'].apply(
    lambda x: 'serious' if x == 'suspected serious injury' else 'light'
)

print(f"Stage 3 dataset shape: {stage3_df.shape}")
print(f"Severity distribution: {stage3_df['Severity'].value_counts()}")

X3 = stage3_df.drop(['Injury Severity', 'Injury Binary', 'Fatal Binary', 'Severity'], axis=1)
y3 = stage3_df['Severity']

# Label encoding
le3 = LabelEncoder()
y3_encoded = le3.fit_transform(y3)
print(f"Label mapping: {dict(zip(le3.classes_, le3.transform(le3.classes_)))}")

# Preprocessing
X3 = pd.get_dummies(X3, drop_first=True)
X3.columns = X3.columns.str.replace('[^A-Za-z0-9_]+', '_', regex=True)
X3 = X3.drop(columns=X3.select_dtypes(include=['datetime64']).columns)
X3 = X3.astype(float)

# Train-test split
X_train3, X_test3, y_train3, y_test3 = train_test_split(
    X3, y3_encoded, test_size=0.3, random_state=42, stratify=y3_encoded
)

# Apply SMOTE
serious_count = y_train3.tolist().count(1)
smote3 = SMOTE(sampling_strategy={1: serious_count * 3}, random_state=42)
X_train3_smote, y_train3_smote = smote3.fit_resample(X_train3, y_train3)
print(f"Class distribution after SMOTE: {pd.Series(y_train3_smote).value_counts()}")

# Calculate class weights
class_weights3 = compute_class_weight('balanced', classes=np.unique(y_train3), y=y_train3)
scale_pos_weight3 = class_weights3[1] / class_weights3[0] if len(class_weights3) > 1 else 1

# XGBoost Model 3
model3 = xgb.XGBClassifier(
    n_estimators=500,
    max_depth=10,
    learning_rate=0.03,
    scale_pos_weight=scale_pos_weight3,
    random_state=42,
    eval_metric='logloss',
    early_stopping_rounds=50
)

# Fit with evaluation set
model3.fit(
    X_train3_smote, y_train3_smote,
    eval_set=[(X_train3_smote, y_train3_smote), (X_test3, y_test3)],
    verbose=False
)

# Predictions
y_pred3_test = model3.predict(X_test3)
y_pred3_train = model3.predict(X_train3_smote)

# Convert back to original labels for reporting
y_train3_labels = le3.inverse_transform(y_train3_smote)
y_pred3_train_labels = le3.inverse_transform(y_pred3_train)
y_test3_labels = le3.inverse_transform(y_test3)
y_pred3_test_labels = le3.inverse_transform(y_pred3_test)

# Print metrics
print_metrics(y_train3_labels, y_pred3_train_labels, "Stage 3", "Train")
print_metrics(y_test3_labels, y_pred3_test_labels, "Stage 3", "Test")

# Plot confusion matrices
plot_confusion_matrix(y_train3_labels, y_pred3_train_labels,
                     "Stage 3: Train Set Confusion Matrix",
                     labels=['Light', 'Serious'])
plot_confusion_matrix(y_test3_labels, y_pred3_test_labels,
                     "Stage 3: Test Set Confusion Matrix",
                     labels=['Light', 'Serious'])

# 📊 Feature Importance Analysis
print("\n" + "="*60)
print("FEATURE IMPORTANCE ANALYSIS")
print("="*60)

def plot_feature_importance(model, feature_names, title, top_n=15):
    importance = model.feature_importances_
    indices = np.argsort(importance)[::-1][:top_n]

    plt.figure(figsize=(10, 8))
    plt.title(title)
    plt.bar(range(top_n), importance[indices])
    plt.xticks(range(top_n), [feature_names[i] for i in indices], rotation=45, ha='right')
    plt.tight_layout()
    plt.show()

    # Print top features
    print(f"\nTop {top_n} features for {title}:")
    for i, idx in enumerate(indices):
        print(f"{i+1:2d}. {feature_names[idx]}: {importance[idx]:.4f}")

# Plot feature importance for each stage
plot_feature_importance(model1, X.columns, "Stage 1: Injury/No Injury")
plot_feature_importance(model2, X2.columns, "Stage 2: Fatal/Non-Fatal")
plot_feature_importance(model3, X3.columns, "Stage 3: Light/Serious")

print("\n" + "="*60)
print("TRAINING COMPLETED SUCCESSFULLY!")
print("="*60)

"""Random Forest
---


"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, f1_score
from imblearn.over_sampling import SMOTE

""">**מודל יש פציעה/אין פציעה**"""

# ✨ יצירת עמודת פציעה / אין פציעה
df['Injury Binary'] = df['Injury Severity'].apply(
    lambda x: 0 if x == 'no apparent injury' else 1
)

# 🛠️ בניית X ו-y לשלב הראשון
X = df.drop(['Injury Severity', 'Injury Binary'], axis=1)
y_stage1 = df['Injury Binary']


# 🎯 One-Hot Encoding
cat_cols = X.select_dtypes(include='object').columns.tolist()  # 💡 הוספה חשובה!
X = pd.get_dummies(X, columns=cat_cols, drop_first=True)


# ✅ ניקוי שמות עמודות
X.columns = X.columns.str.replace('[^A-Za-z0-9_]+', '_', regex=True)

# פיצול ל-Train/Test
X_train1, X_test1, y_train1, y_test1 = train_test_split(
    X, y_stage1, test_size=0.3, random_state=42, stratify=y_stage1
)

# SMOTE
smote = SMOTE(random_state=42)
X_res1, y_res1 = smote.fit_resample(X_train1, y_train1)

# 🚀 אימון מודל Random Forest לשלב הראשון
rf_model_stage1 = RandomForestClassifier(
    n_estimators=300,
    max_depth=15,
    random_state=42,
    class_weight='balanced'
)
rf_model_stage1.fit(X_res1, y_res1)

# 🎯 חיזוי עם סף מותאם
y_probs1 = rf_model_stage1.predict_proba(X_test1)[:, 1]
threshold = 0.48
y_pred1_threshold = (y_probs1 >= threshold).astype(int)

# 📈 דו"ח ביצועים שלב ראשון
print("\n📈 דו\"ח סיווג - שלב ראשון (פציעה / אין פציעה עם Random Forest):")
print(classification_report(y_test1, y_pred1_threshold))

# 📊 מטריצת בלבול - שלב ראשון
plt.figure(figsize=(8,6))
sns.heatmap(confusion_matrix(y_test1, y_pred1_threshold), annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix - Stage 1 (Injury/No Injury with Random Forest)')
plt.show()

# 🎯 חיזוי עם סף מותאם
y_probs1 = rf_model_stage1.predict_proba(X_test1)[:, 1]
threshold = 0.3
y_pred1_threshold = (y_probs1 >= threshold).astype(int)

# 📈 דו"ח ביצועים שלב ראשון
print("\n📈 דו\"ח סיווג - שלב ראשון (פציעה / אין פציעה עם Random Forest):")
print(classification_report(y_test1, y_pred1_threshold))

# 📊 מטריצת בלבול - שלב ראשון
plt.figure(figsize=(8,6))
sns.heatmap(confusion_matrix(y_test1, y_pred1_threshold), annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix - Stage 1 (Injury/No Injury with Random Forest)')
plt.show()

""">**מודל 5 קטגרויות**"""

# ===============================
# 1. הכנת עמודת מטרה ופיצ'רים
# ===============================
target_column = 'Injury Severity'
X = df.drop(columns=[target_column])
y = df[target_column]


# המרת עמודות קטגוריאליות
for col in X.columns:
    if X[col].dtype == 'object':
        X[col] = X[col].astype('category')

# קידוד עמודת מטרה
le = LabelEncoder()
y = le.fit_transform(y)

# ===============================
# 2. פיצול ראשוני ל-Train/Test
# ===============================
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.3,
    random_state=42,
    stratify=y
)

# קידוד לקטגוריות כ-int
X_train_encoded = X_train.copy()
for col in X_train_encoded.select_dtypes(['category']).columns:
    X_train_encoded[col] = X_train_encoded[col].cat.codes

X_test_encoded = X_test.copy()
for col in X_test_encoded.select_dtypes(['category']).columns:
    X_test_encoded[col] = X_test_encoded[col].cat.codes


# ===============================
# 3. SMOTE לאיזון סט האימון
# ===============================
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train_encoded, y_train)

# ===============================
# 4. אימון מודל Random Forest בסיסי
# ===============================
model = RandomForestClassifier(random_state=42, n_jobs=-1)
model.fit(X_train_resampled, y_train_resampled)

# ===============================
# 5. מדדי ביצוע לפני סינון
# ===============================
y_pred = model.predict(X_test_encoded)

print("\n🔵 תוצאות לפני סינון פיצ'רים:\n")
print(classification_report(y_test, y_pred, target_names=le.classes_))

macro_f1_before = f1_score(y_test, y_pred, average='macro')
print(f"\n🎯 Macro F1 לפני סינון: {macro_f1_before:.4f}")

# ===============================
# 6. Feature Importance
# ===============================
importances = model.feature_importances_
feature_names = X_train_encoded.columns

feature_importance_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': importances
}).sort_values(by='Importance', ascending=False)

print("\n🔵 טבלת חשיבות פיצ'רים (מהכי חשוב לפחות חשוב):\n")
print(feature_importance_df.head(30))

# ===============================
# 7. סינון פיצ'רים חלשים לפי סף
# ===============================
threshold = 0.01

features_to_drop_df = feature_importance_df[feature_importance_df['Importance'] < threshold]
features_to_drop = features_to_drop_df['Feature'].tolist()

print(f"\n🚮 נמצאו {len(features_to_drop)} פיצ'רים להסרה (מתחת ל-{threshold}):\n")
print(features_to_drop_df)

# ===============================
# 8. סינון ויצירת דאטה חדש
# ===============================
X_filtered = X.drop(columns=features_to_drop)

# ===============================
# 9. פיצול מחדש ל-Train/Test
# ===============================
X_train, X_test, y_train, y_test = train_test_split(
    X_filtered, y,
    test_size=0.3,
    random_state=42,
    stratify=y
)

# קידוד מחדש
X_train_encoded = X_train.copy()
for col in X_train_encoded.select_dtypes(['category']).columns:
    X_train_encoded[col] = X_train_encoded[col].cat.codes


X_test_encoded = X_test.copy()
for col in X_test_encoded.select_dtypes(['category']).columns:
    X_test_encoded[col] = X_test_encoded[col].cat.codes


# ===============================
# 10. SMOTE מחדש אחרי סינון
# ===============================
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train_encoded, y_train)

# ===============================
# 11. אימון מודל Random Forest אחרי סינון
# ===============================
model_filtered = RandomForestClassifier(random_state=42, n_jobs=-1)
model_filtered.fit(X_train_resampled, y_train_resampled)

# ===============================
# 12. חיזוי ומדדים אחרי סינון
# ===============================
y_pred_filtered = model_filtered.predict(X_test_encoded)

print("\n🔵 תוצאות אחרי סינון פיצ'רים:\n")
print(classification_report(y_test, y_pred_filtered, target_names=le.classes_))

macro_f1_after = f1_score(y_test, y_pred_filtered, average='macro')
print(f"\n🎯 Macro F1 אחרי סינון: {macro_f1_after:.4f}")

# ===============================
# 13. השוואת Macro F1 סופית
# ===============================
print("\n📊 השוואת Macro F1:\n")
print(f"🔵 לפני סינון: {macro_f1_before:.4f}")
print(f"🟢 אחרי סינון: {macro_f1_after:.4f}")

"""> **מודל: 3 קטגוריות - אין פציעה/פציעה קלה/פציעה חמורה**

"""

# ===============================
# 1. מיפוי Injury Severity ל־3 קטגוריות
# ===============================
injury_mapping = {
    'no apparent injury': 'no injury',
    'possible injury': 'minor injury',
    'suspected minor injury': 'minor injury',
    'suspected serious injury': 'severe injury',
    'fatal injury': 'severe injury'
}

df = df[df['Injury Severity'].isin(injury_mapping.keys())]
df['Injury Severity'] = df['Injury Severity'].map(injury_mapping)


# ===============================
# 2. הכנת עמודת מטרה ופיצ'רים
# ===============================
target_column = 'Injury Severity'
X = df.drop(columns=[target_column])
y = df[target_column]



# המרת עמודות קטגוריאליות ל־category
for col in X.columns:
    if X[col].dtype == 'object':
        X[col] = X[col].astype('category')

# קידוד עמודת מטרה
le = LabelEncoder()
y = le.fit_transform(y)

# ===============================
# 3. פיצול ל־Train/Test
# ===============================
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.3,
    random_state=42,
    stratify=y
)

# קידוד קטגוריות כ־int
X_train_encoded = X_train.copy()
for col in X_train_encoded.select_dtypes(['category']).columns:
    X_train_encoded[col] = X_train_encoded[col].cat.codes

X_test_encoded = X_test.copy()
for col in X_test_encoded.select_dtypes(['category']).columns:
    X_test_encoded[col] = X_test_encoded[col].cat.codes

# ===============================
# 4. SMOTE על סט האימון
# ===============================
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train_encoded, y_train)

# ===============================
# 5. אימון מודל עם class_weight='balanced'
# ===============================
model = RandomForestClassifier(
    random_state=42,
    n_jobs=-1,
    class_weight='balanced'
)
model.fit(X_train_resampled, y_train_resampled)

# ===============================
# 6. חיזוי והערכת ביצועים
# ===============================
y_pred = model.predict(X_test_encoded)

print("\n🔵 תוצאות המודל (SMOTE + class_weight=balanced):\n")
print(classification_report(y_test, y_pred, target_names=le.classes_))

macro_f1 = f1_score(y_test, y_pred, average='macro')
print(f"\n🎯 Macro F1 (שילוב SMOTE + class_weight): {macro_f1:.4f}")

"""> **מודל דו-שלבי**

*   שלב 1: יש פציעה/אין פציעה
*   שלב 2: פציעה קטלנית/קלה/קשה

"""

# ✨ יצירת עמודת פציעה / אין פציעה
df['Injury Binary'] = df['Injury Severity'].apply(
    lambda x: 0 if x == 'no apparent injury' else 1
)

# 🛠️ בניית X ו-y לשלב הראשון
X = df.drop(['Injury Severity', 'Injury Binary'], axis=1)
y_stage1 = df['Injury Binary']


# 🎯 One-Hot Encoding
cat_cols = X.select_dtypes(include='object').columns.tolist()  # 💡 הוספה חשובה!
X = pd.get_dummies(X, columns=cat_cols, drop_first=True)


# ✅ ניקוי שמות עמודות
X.columns = X.columns.str.replace('[^A-Za-z0-9_]+', '_', regex=True)

# פיצול ל-Train/Test
X_train1, X_test1, y_train1, y_test1 = train_test_split(
    X, y_stage1, test_size=0.3, random_state=42, stratify=y_stage1
)

# SMOTE
smote = SMOTE(random_state=42)
X_res1, y_res1 = smote.fit_resample(X_train1, y_train1)

# 🚀 אימון מודל Random Forest לשלב הראשון
rf_model_stage1 = RandomForestClassifier(
    n_estimators=300,
    max_depth=15,
    random_state=42,
    class_weight='balanced'
)
rf_model_stage1.fit(X_res1, y_res1)

# 🎯 חיזוי עם סף מותאם
y_probs1 = rf_model_stage1.predict_proba(X_test1)[:, 1]
threshold = 0.3
y_pred1_threshold = (y_probs1 >= threshold).astype(int)

# 📈 דו"ח ביצועים שלב ראשון
print("\n📈 דו\"ח סיווג - שלב ראשון (פציעה / אין פציעה עם Random Forest):")
print(classification_report(y_test1, y_pred1_threshold))

# 📊 מטריצת בלבול - שלב ראשון
plt.figure(figsize=(8,6))
sns.heatmap(confusion_matrix(y_test1, y_pred1_threshold), annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix - Stage 1 (Injury/No Injury with Random Forest)')
plt.show()

# 🧹 הכנה לשלב השני: תצפיות שחזינו אצלן "פציעה"
X_test_stage2 = X_test1[y_pred1_threshold == 1]
indexes_stage2 = X_test_stage2.index
y_true_stage2 = df.loc[indexes_stage2, 'Injury Severity']

# ✨ בניית דאטה חדש עם מיפוי ל-3 קטגוריות
injury_mapping = {
    'possible injury': 'light injury',
    'suspected minor injury': 'light injury',
    'suspected serious injury': 'serious injury',
    'fatal injury': 'fatal injury'
}

df_stage2 = df[df['Injury Severity'].isin(injury_mapping.keys())].copy()
df_stage2['Injury Mapped'] = df_stage2['Injury Severity'].map(injury_mapping)

# בניית X ו-y לשלב שני
X_stage2 = df_stage2.drop(['Injury Severity', 'Injury Binary', 'Injury Mapped'], axis=1)
y_stage2 = df_stage2['Injury Mapped']

# טיפול בערכים חסרים
cat_cols_stage2 = X_stage2.select_dtypes(include=['object']).columns
num_cols_stage2 = X_stage2.select_dtypes(include=['float64', 'int64']).columns

for col in num_cols_stage2:
    X_stage2[col] = X_stage2[col].fillna(X_stage2[col].median())
for col in cat_cols_stage2:
    X_stage2[col] = X_stage2[col].fillna('Unknown')

# 🎯 One-Hot Encoding
X_stage2 = pd.get_dummies(X_stage2, columns=cat_cols_stage2, drop_first=True)

# ✅ ניקוי שמות עמודות
X_stage2.columns = X_stage2.columns.str.replace('[^A-Za-z0-9_]+', '_', regex=True)

# קידוד y
le_y = LabelEncoder()
y_stage2_encoded = le_y.fit_transform(y_stage2)

# פיצול ל-Train/Test
X_train2, X_test2, y_train2, y_test2 = train_test_split(
    X_stage2, y_stage2_encoded, test_size=0.3, random_state=42, stratify=y_stage2_encoded
)

# SMOTE
smote2 = SMOTE(random_state=42)
X_res2, y_res2 = smote2.fit_resample(X_train2, y_train2)

# 🚀 אימון מודל Random Forest לשלב השני
rf_model_stage2 = RandomForestClassifier(
    n_estimators=300,
    max_depth=15,
    random_state=42,
    class_weight='balanced'
)
rf_model_stage2.fit(X_res2, y_res2)

# 🔮 חיזוי
y_pred2_encoded = rf_model_stage2.predict(X_test2)

# 🧠 החזרת התוצאות לשמות
y_pred2_labels = le_y.inverse_transform(y_pred2_encoded)
y_test2_labels = le_y.inverse_transform(y_test2)

# 📈 דו"ח ביצועים שלב שני
print("\n📈 דו\"ח סיווג - שלב שני (3 קטגוריות - light/serious/fatal עם Random Forest):")
print(classification_report(y_test2_labels, y_pred2_labels))

# 📊 מטריצת בלבול שלב שני
plt.figure(figsize=(8,6))
sns.heatmap(confusion_matrix(y_test2_labels, y_pred2_labels, labels=le_y.classes_),
            annot=True, fmt='d', cmap='Blues',
            xticklabels=le_y.classes_, yticklabels=le_y.classes_)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix - Stage 2 (Mapped Injury Type with Random Forest)')
plt.show()

"""**רגרסיה לוגיסטית**
---


"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import classification_report, f1_score
from imblearn.over_sampling import SMOTE

""">**מודל 5 קטגרויות**"""

# ===============================
# 1. הכנת עמודת מטרה ופיצ'רים
# ===============================

target_column = 'Injury Severity'

X = df.drop(columns=[target_column])
y = df[target_column]


# המרת קטגוריות
for col in X.columns:
    if X[col].dtype == 'object':
        X[col] = X[col].astype('category')


# קידוד עמודת המטרה
le = LabelEncoder()
y = le.fit_transform(y)

# ===============================
# 2. פיצול ל-Train/Test
# ===============================

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

# ===============================
# 3. קידוד ל-SMOTE
# ===============================

X_train_encoded = X_train.copy()
for col in X_train_encoded.select_dtypes(['category']).columns:
    X_train_encoded[col] = X_train_encoded[col].cat.codes

# SMOTE
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train_encoded, y_train)

# ===============================
# 4. Standardization
# ===============================

scaler = StandardScaler()
X_train_resampled = scaler.fit_transform(X_train_resampled)

X_test_encoded = X_test.copy()
for col in X_test_encoded.select_dtypes(['category']).columns:
    X_test_encoded[col] = X_test_encoded[col].cat.codes

X_test_encoded = scaler.transform(X_test_encoded)

# ===============================
# 5. בניית מודל Logistic Regression
# ===============================

model = LogisticRegression(
    max_iter=2000,         # לא מוגזם כדי לא לרוץ 100 שנה
    random_state=42,
    class_weight='balanced',
    solver='lbfgs'
)

model.fit(X_train_resampled, y_train_resampled)

# ===============================
# 6. חיזוי ובדיקת ביצועים
# ===============================

y_pred = model.predict(X_test_encoded)

# דוח מפורט לכל הקטגוריות
print("\n🔵 דוח ביצועים על הסט:")
print(classification_report(
    y_test,
    y_pred,
    target_names=le.classes_
))

# Macro F1 כללי
macro_f1 = f1_score(
    y_test,
    y_pred,
    average='macro'
)

print(f"\n🎯 Macro F1 Score כולל: {macro_f1:.4f}")

"""# גרפים להערכת התוצאות"""

# 🎯 Create performance DataFrame
df = pd.DataFrame({
    'Model': ['LightGBM', 'XGBoost', 'CatBoost', 'Random Forest'],
    'Accuracy': [0.88, 0.86, 0.87, 0.87]
})

# 🎨 Plotting
plt.figure(figsize=(8, 5))
sns.set_palette("Blues")
ax = sns.barplot(x='Model', y='Accuracy', data=df)

# ➕ Add values on top of bars
for i, row in df.iterrows():
    ax.text(i, row['Accuracy'] + 0.015, f"{row['Accuracy']:.2f}",
            ha='center', va='bottom', fontsize=10)

# 🖼️ Formatting
plt.title('Accuracy per Model (Injury vs No Injury)', fontsize=14)
plt.ylabel("Accuracy", fontsize=12)
plt.xlabel("Model", fontsize=12)
plt.ylim(0, 1.05)
plt.grid(axis='y')
plt.tight_layout()

# 💾 Optional save
plt.savefig("accuracy_by_model_labeled.png", dpi=300)

# 📊 Show plot
plt.show()

# 🎯 Create DataFrame
df = pd.DataFrame({
    'Model': ['LightGBM', 'XGBoost', 'CatBoost', 'Random Forest'],
    'F1_Injury': [0.74, 0.72, 0.73, 0.71],
    'Recall_Injury': [0.95, 0.97, 0.96, 0.94],
    'Precision_Injury': [0.6, 0.57, 0.59, 0.58],
    'F1_No_Injury': [0.92, 0.91, 0.92, 0.91],
    'Recall_No_Injury': [0.86, 0.84, 0.85, 0.85],
    'Precision_No_Injury': [0.99, 0.99, 0.99, 0.98]
})

# 🟦 Long format for each group
df_injury = df[['Model', 'F1_Injury', 'Recall_Injury', 'Precision_Injury']].melt(id_vars='Model', var_name='Metric', value_name='Score')
df_noinjury = df[['Model', 'F1_No_Injury', 'Recall_No_Injury', 'Precision_No_Injury']].melt(id_vars='Model', var_name='Metric', value_name='Score')

# 📊 Plot Injury Metrics
plt.figure(figsize=(8, 5))
sns.set_palette("Blues")
ax1 = sns.barplot(data=df_injury, x='Model', y='Score', hue='Metric')
plt.title('Injury Class - Model Performance')
plt.ylim(0, 1.05)
plt.ylabel("Score")
plt.xlabel("Model")
plt.legend(title="Metric", bbox_to_anchor=(1.02, 1), loc='upper left')
plt.grid(axis='y')

# ➕ Add text labels
for container in ax1.containers:
    ax1.bar_label(container, fmt='%.2f', label_type='edge', fontsize=9)

plt.tight_layout()
plt.savefig("injury_metrics_by_model_labeled.png", dpi=300)
plt.show()

# 📊 Plot No Injury Metrics
plt.figure(figsize=(8, 5))
sns.set_palette("Blues_d")
ax2 = sns.barplot(data=df_noinjury, x='Model', y='Score', hue='Metric')
plt.title('No Injury Class - Model Performance')
plt.ylim(0, 1.05)
plt.ylabel("Score")
plt.xlabel("Model")
plt.legend(title="Metric", bbox_to_anchor=(1.02, 1), loc='upper left')
plt.grid(axis='y')

# ➕ Add text labels
for container in ax2.containers:
    ax2.bar_label(container, fmt='%.2f', label_type='edge', fontsize=9)

plt.tight_layout()
plt.savefig("no_injury_metrics_by_model_labeled.png", dpi=300)
plt.show()